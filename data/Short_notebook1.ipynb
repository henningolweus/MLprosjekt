{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f9066f-4ed9-4027-bceb-01d0433ec4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3163e94-32aa-4c71-8532-39acafd66a08",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4a2577-d7b2-4cb8-a8a2-c71f5d4996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "x_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "x_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "x_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "x_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "x_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "x_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "x_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "x_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "x_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4281fc-efcb-4228-9ecb-7357104f19c2",
   "metadata": {},
   "source": [
    "## Merge x_train observed and x_train_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe1154c-544b-4578-8823-11ac72258c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a = pd.concat([x_train_observed_a,x_train_estimated_a])\n",
    "x_train_b = pd.concat([x_train_observed_b,x_train_estimated_b])\n",
    "x_train_c = pd.concat([x_train_observed_c,x_train_estimated_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8f57c-dd52-4fdc-9909-6c8441c0ef68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "1. Remove NaN columns\n",
    "2. Aggregate data from 15 minute to hourly intervals:\n",
    "   - Method 1: Take the mean over all four 15minutes recording, resulting in hourly measurements.\n",
    "   - Method 2: Create a separate column for each 15-minute value, effectively quadrupling the number of columns.\n",
    "3. Handle consecutive pv measurments\n",
    "4. Remove rows in train with NaN values in pv measurement\n",
    "5. Remove rows that have timestamps that are not present in both x and y\n",
    "6. Remove rows with NaN values in all columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3318fa-cfb0-4209-950f-511cc839e281",
   "metadata": {},
   "source": [
    "### 1. Remove NaN columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b243223b-3af0-4809-a58c-7837e7d8ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove snow:density column as well as rows with only NaN values\n",
    "def remove_nan_cols(x,remove_cols):\n",
    "    df = x.copy()\n",
    "    df = df.drop(columns = remove_cols ) #Should we include 'cloud_base_agl:m' and ceiling_height_agl:m ['snow_density:kgm3']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8ded65-d60b-48fb-a95b-52bc5e572fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Catboost\n",
    "x_train_a1 = remove_nan_cols(x_train_a,['snow_density:kgm3'])\n",
    "x_train_b1 = remove_nan_cols(x_train_b,['snow_density:kgm3'])\n",
    "x_train_c1 = remove_nan_cols(x_train_c,['snow_density:kgm3'])\n",
    "\n",
    "x_test_a1 = remove_nan_cols(x_test_estimated_a,['snow_density:kgm3'])\n",
    "x_test_b1 = remove_nan_cols(x_test_estimated_b,['snow_density:kgm3'])\n",
    "x_test_c1 = remove_nan_cols(x_test_estimated_c,['snow_density:kgm3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ea17b1-60b1-4b49-8653-cf3572654759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Autogluon\n",
    "x_train_a2 =  remove_nan_cols(x_train_a,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_train_b2 =  remove_nan_cols(x_train_b,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_train_c2 =  remove_nan_cols(x_train_c,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "\n",
    "x_test_a2 =  remove_nan_cols(x_test_estimated_a,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_test_b2 =  remove_nan_cols(x_test_estimated_b,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_test_c2 =  remove_nan_cols(x_test_estimated_c,['snow_density:kgm3', 'cloud_base_agl:m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e541e33-c23b-4804-aa81-e9dae1f402f7",
   "metadata": {},
   "source": [
    "### 2. Transform data to hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428524f-a553-4451-bded-1515313bce8b",
   "metadata": {},
   "source": [
    "**Method 1: Take the mean over all four 15minutes recording, resulting in hourly measurements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b66434-dc7b-4fa2-a875-cbcf6798177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_4942/4253554224.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_4942/4253554224.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_4942/4253554224.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_4942/4253554224.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_4942/4253554224.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_4942/4253554224.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "def resample_to_hourly(x): \n",
    "    df = x.copy()\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    \n",
    "    df_hourly_date_calc = df.resample('H')['date_calc'].first()\n",
    "    # Aggregating by averaging over quartarly measurements\n",
    "    df_hourly = df.resample('H').mean()\n",
    "    df_hourly['date_calc'] = df_hourly_date_calc\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "    \n",
    "    return df_hourly\n",
    "\n",
    "\n",
    "x_train_a_hourly = resample_to_hourly(x_train_a1)\n",
    "x_train_b_hourly = resample_to_hourly(x_train_b1)\n",
    "x_train_c_hourly = resample_to_hourly(x_train_c1)\n",
    "\n",
    "x_test_a_hourly = resample_to_hourly(x_test_a1)\n",
    "x_test_b_hourly = resample_to_hourly(x_test_b1)\n",
    "x_test_c_hourly = resample_to_hourly(x_test_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528c9f9d-1c20-4ff1-aaa3-8ba77d129fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only rows in test that are given in the test csv\n",
    "test = pd.read_csv('test.csv')\n",
    "pred_time_stamps = test['time'].unique()\n",
    "x_test_a1 = x_test_a_hourly[x_test_a_hourly['date_forecast'].isin(pred_time_stamps)]\n",
    "x_test_b1 = x_test_b_hourly[x_test_b_hourly['date_forecast'].isin(pred_time_stamps)]\n",
    "x_test_c1 = x_test_c_hourly[x_test_c_hourly['date_forecast'].isin(pred_time_stamps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3a464-61c5-46a8-9d69-d9800c5bbd1e",
   "metadata": {},
   "source": [
    "**Method 2: Create a separate column for each 15-minute value, quadrupling the number of columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26db8279-7e16-4164-9efd-676f94a7f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_hourly_quarters(x, date_column='date_forecast', exclude_column='date_calc'):\n",
    "    df = x.copy()\n",
    "    # Ensure the date column is in datetime format and set as the index\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df.set_index(date_column, inplace=True)\n",
    "    \n",
    "    # Separate the column to exclude from the resampling\n",
    "    excluded_data = df[[exclude_column]].resample('H').first()  # You can use 'first' or 'last' here\n",
    "    \n",
    "    # Remove the excluded column from df before pivoting\n",
    "    df = df.drop(columns=[exclude_column])\n",
    "\n",
    "    # Add a column for the 15-minute period within the hour\n",
    "    df['quarter'] = df.index.minute // 15  # Use floor division to get the quarter number (0, 1, 2, 3)\n",
    "\n",
    "    # Pivot the table. For each feature, create a new column for each 15-minute period.\n",
    "    df_pivot = df.pivot_table(index=df.index.floor('H'),\n",
    "                              columns='quarter',\n",
    "                              aggfunc='first')  # We use 'first' because each quarter should be unique\n",
    "    \n",
    "    # Flatten the multi-level column index\n",
    "    df_pivot.columns = ['{}_Q{}'.format(feature, quarter) for feature, quarter in df_pivot.columns]\n",
    "\n",
    "    # Reset the index to be able to merge on the date_column\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    excluded_data.reset_index(inplace=True)\n",
    "\n",
    "    # Merge back the excluded column\n",
    "    df_hourly = pd.merge(excluded_data, df_pivot, on=date_column)\n",
    "\n",
    "    return df_hourly\n",
    "\n",
    "# Make sure to pass the column name that contains the datetime information\n",
    "x_train_a_hourly2 = resample_to_hourly_quarters(x_train_a2, date_column='date_forecast')\n",
    "x_train_b_hourly2 = resample_to_hourly_quarters(x_train_b2, date_column='date_forecast')\n",
    "x_train_c_hourly2 = resample_to_hourly_quarters(x_train_c2, date_column='date_forecast')\n",
    "\n",
    "x_test_a_hourly2 = resample_to_hourly_quarters(x_test_a2, date_column='date_forecast')\n",
    "x_test_b_hourly2 = resample_to_hourly_quarters(x_test_b2, date_column='date_forecast')\n",
    "x_test_c_hourly2 = resample_to_hourly_quarters(x_test_b2, date_column='date_forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4ea3d-f5de-48a1-ad94-fa74f576876e",
   "metadata": {},
   "source": [
    "### 3.Remove rows in train with NaN values in pv measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "198cf756-bcf8-443c-8663-e24477233065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify the indices of the rows with NaN values in the 'pv_measurement' column\n",
    "nan_indices_a = train_a[train_a['pv_measurement'].isna()].index\n",
    "nan_indices_b = train_b[train_b['pv_measurement'].isna()].index\n",
    "nan_indices_c = train_c[train_c['pv_measurement'].isna()].index\n",
    "\n",
    "# Drop these indices from y_train\n",
    "train_a = train_a.drop(nan_indices_a).reset_index(drop = True)\n",
    "train_b = train_b.drop(nan_indices_b).reset_index(drop = True)\n",
    "train_c = train_c.drop(nan_indices_c).reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27eb6b7-8929-4662-b7d6-7ba59e7cc6ef",
   "metadata": {},
   "source": [
    "### 4. Handle consecutive pv measurments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59890780-a5c0-4238-8c34-696c8d3d6c96",
   "metadata": {},
   "source": [
    "**Method used catboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4921d339-ce3b-4827-9d4c-b6e117ea901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filters out rows from a DataFrame where the 'pv_measurement' column has consecutive identical values beyond a specified threshold.\n",
    "def remove_constant_intervals(y_train, low_thresh, upp_thresh = 10**6):\n",
    "    \"\"\"\n",
    "    Identify and remove intervals of constant PV readings that exceed a specified duration. \n",
    "    Constant readings may indicate sensor malfunctions or data logging issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = y_train.copy()\n",
    "    \n",
    "    # Calculate the difference in production values\n",
    "    df['diff'] = df['pv_measurement'].diff()\n",
    "\n",
    "    # Identify where the difference is zero\n",
    "    df['zero_diff'] = df['diff'].abs() < 1e-5\n",
    "\n",
    "    # Identify groups of consecutive zero differences\n",
    "    df['group'] = (df['zero_diff'] != df['zero_diff'].shift()).cumsum()\n",
    "\n",
    "    # Filter out only the groups with consecutive zero differences\n",
    "    constant_intervals = df[df['zero_diff']].groupby('group').agg(start=('time', 'min'), \n",
    "                                                                  end=('time', 'max'),\n",
    "                                                                  duration=('time', 'size'))\n",
    "    \n",
    "    # Filter intervals based on the threshold\n",
    "    interval_df_thresh = constant_intervals[(constant_intervals['duration'] > low_thresh) & (constant_intervals['duration'] <upp_thresh)]\n",
    "    \n",
    "    # Remove rows from the main dataframe that fall within these intervals\n",
    "    for _, row in interval_df_thresh.iterrows():\n",
    "        start_time, end_time = row['start'], row['end']\n",
    "        df = df[(df['time'] < start_time) | (df['time'] > end_time)]\n",
    "    \n",
    "    # Drop the added columns used for calculations\n",
    "    df.drop(columns=['diff', 'zero_diff', 'group'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1831d275-58fa-4be3-856e-956225bfc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in groups of constant values, where duration of constant measurements is > 1 day (24 hours)\n",
    "train_a1 = remove_constant_intervals(train_a,24)\n",
    "train_b1 = remove_constant_intervals(train_b,24)\n",
    "train_c1 = remove_constant_intervals(train_c,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57997a03-e985-49b5-9ee2-6e609e5e2979",
   "metadata": {},
   "source": [
    "**Method used for Autogluon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d6894d3-3233-45d7-a217-8f6c1020e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters out rows from a DataFrame where the 'pv_measurement' column has consecutive zero-values beyond a specified threshold. \n",
    "# Also removes all rows, except the first, where 'pv_measurement' is identical for two or more rows. \n",
    "def remove_constant_intervals2(df: pd.DataFrame, threshold: int = 24) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters out rows from a DataFrame where the 'pv_measurement' column has:\n",
    "    1. Consecutive zero values beyond a specified threshold.\n",
    "    2. Consecutive non-zero identical values, keeping only the first in each sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame to be processed.\n",
    "    - threshold: Maximum allowable number of consecutive zero values. Rows in streaks\n",
    "      beyond this threshold will be removed.\n",
    "\n",
    "    Returns:\n",
    "    - Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Create a mask to identify where the current 'pv_measurement'-row is different from the previous one\n",
    "    is_different = df['pv_measurement'] != df['pv_measurement'].shift()\n",
    "    \n",
    "    # Additional mask to identify non-zero values\n",
    "    is_nonzero = df['pv_measurement'] != 0\n",
    "\n",
    "    # Use cumsum to generate unique group identifiers for each streak of identical measurements\n",
    "    groups = is_different.cumsum()\n",
    "\n",
    "    # Count the number of entries in each group\n",
    "    df['group_counts'] = df.groupby(groups)['pv_measurement'].transform('count')\n",
    "\n",
    "    # Identify groups of zeros that exceed the specified threshold\n",
    "    to_remove_zeros = (df['group_counts'] > threshold) & ~is_nonzero\n",
    "\n",
    "    # Identify the first entry in groups of consecutive non-zero identical values\n",
    "    to_keep_nonzero = (df['pv_measurement'].shift() != df['pv_measurement']) | is_different | ~is_nonzero\n",
    "\n",
    "    # Combine conditions\n",
    "    to_remove = to_remove_zeros | ~to_keep_nonzero\n",
    "\n",
    "    # Drop the temporary 'group_counts' column\n",
    "    df = df.drop(columns=['group_counts'])\n",
    "\n",
    "    # Filter out the rows that do not meet the conditions\n",
    "    filtered_df = df[~to_remove]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "054796c6-b372-478c-a615-2b07add0b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a2 = remove_constant_intervals2(train_a,24)\n",
    "train_b2 = remove_constant_intervals2(train_b,24)\n",
    "train_c2 = remove_constant_intervals2(train_c,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0687c-4aac-42c8-848c-bb2d3e5bd3a8",
   "metadata": {},
   "source": [
    "### 5. Remove rows that have timestamps that are not present in both x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e721f6-c93e-4baf-900d-2655ac180c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with date-time values that are not present in both x and y in order to synchronize x and its labels. \n",
    "def remove_non_synchronous_rows(x_train, y_train, x_date_column='date_forecast', y_date_column='time'):\n",
    "    # Convert date columns to datetime format for easier comparison\n",
    "    x_train[x_date_column] = pd.to_datetime(x_train[x_date_column])\n",
    "    y_train[y_date_column] = pd.to_datetime(y_train[y_date_column])\n",
    "    \n",
    "    # Find common dates\n",
    "    common_dates = x_train[x_date_column][x_train[x_date_column].isin(y_train[y_date_column])]\n",
    "    \n",
    "    # Filter both datasets based on common dates\n",
    "    x_train_synced = x_train.loc[x_train[x_date_column].isin(common_dates)]\n",
    "    y_train_synced = y_train.loc[y_train[y_date_column].isin(common_dates)]\n",
    "    \n",
    "    return x_train_synced, y_train_synced\n",
    "\n",
    "# Remove the rows with date and time that only shows up in one of the sets\n",
    "x_train_a1, train_a = remove_non_synchronous_rows(x_train_a_hourly, train_a)\n",
    "x_train_b1, train_b = remove_non_synchronous_rows(x_train_b_hourly, train_b)\n",
    "x_train_c1, train_c = remove_non_synchronous_rows(x_train_c_hourly, train_c)\n",
    "\n",
    "# Remove the rows with date and time that only shows up in one of the sets\n",
    "x_train_a2, train_a = remove_non_synchronous_rows(x_train_a_hourly2, train_a)\n",
    "x_train_b2, train_b = remove_non_synchronous_rows(x_train_b_hourly2, train_b)\n",
    "x_train_c2, train_c = remove_non_synchronous_rows(x_train_c_hourly2, train_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c41e19-4128-4ce9-9ae1-7ef7f928b529",
   "metadata": {},
   "source": [
    "### 6. Remove rows with NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fccf118e-40d4-49de-be03-6aa676d0d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Catboost\n",
    "x_train_a1 = x_train_a1.dropna(subset=['diffuse_rad:W'])\n",
    "x_train_b1 = x_train_b1.dropna(subset=['diffuse_rad:W'])\n",
    "x_train_c1 = x_train_c1.dropna(subset=['diffuse_rad:W'])\n",
    "\n",
    "x_test_a1 = x_test_a1.dropna(subset=['diffuse_rad:W'])\n",
    "x_test_b1 = x_test_b1.dropna(subset=['diffuse_rad:W'])\n",
    "x_test_c1 = x_test_c1.dropna(subset=['diffuse_rad:W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ccb278-97eb-447d-b608-c6efd99f89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Autogluon\n",
    "x_train_a2 = x_train_a2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_train_b2 = x_train_b2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_train_c2 = x_train_c2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "\n",
    "x_test_a2 = x_test_a_hourly2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_test_b2 = x_test_b_hourly2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_test_c2 = x_test_c_hourly2.dropna(subset=['diffuse_rad:W_Q1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c7283-ec5a-404f-884a-81b7fc1d6c04",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering \n",
    "1. Add time features (hour, day, month, year) + binary observed column\n",
    "2. Add cyclical features\n",
    "3. Add direct_rad x sun_elevation feature\n",
    "4. Remove 'date_forecast' from test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3108f82-60c2-4f28-bdcc-1330b23a5231",
   "metadata": {},
   "source": [
    "### 1. Add time features: hour, day, month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe27439-8471-458f-b10c-d5bf04561cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts year, month, day, and hour features from a given datetime column\n",
    "def extract_date_features(X):\n",
    "    df = X.copy()\n",
    "    # Extract features\n",
    "    df['year'] = df['date_forecast'].dt.year\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "    df['day'] = df['date_forecast'].dt.day\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    \n",
    "    df['observed'] = (df['date_calc'].isna()).astype(int)\n",
    "    df['observed'] = df['observed'].astype(str)\n",
    "    \n",
    "    \n",
    "    df = df.drop(columns = ['date_calc'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26f83329-902f-428f-a87b-cbdd7d1d7c6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date_calc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date_calc'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train_a1 \u001b[38;5;241m=\u001b[39m \u001b[43mextract_date_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_a1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m x_train_b1 \u001b[38;5;241m=\u001b[39m extract_date_features(x_train_b1)\n\u001b[1;32m      3\u001b[0m x_train_c1 \u001b[38;5;241m=\u001b[39m extract_date_features(x_train_c1)\n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mextract_date_features\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mday\n\u001b[1;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour\n\u001b[0;32m---> 10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_calc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna())\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_calc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date_calc'"
     ]
    }
   ],
   "source": [
    "x_train_a1 = extract_date_features(x_train_a1)\n",
    "x_train_b1 = extract_date_features(x_train_b1)\n",
    "x_train_c1 = extract_date_features(x_train_c1)\n",
    "\n",
    "x_test_a1 = extract_date_features(x_test_a1)\n",
    "x_test_b1 = extract_date_features(x_test_b1)\n",
    "x_test_c1 = extract_date_features(x_test_c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f77904-e2a2-4ebd-af49-7e885a8110dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a2 = extract_date_features(x_train_a2)\n",
    "x_train_b2 = extract_date_features(x_train_b2)\n",
    "x_train_c2 = extract_date_features(x_train_c2)\n",
    "\n",
    "x_test_a2 = extract_date_features(x_test_a2)\n",
    "x_test_b2 = extract_date_features(x_test_b2)\n",
    "x_test_c2 = extract_date_features(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590f5f0-722e-4f3d-abd4-c78c9daaed52",
   "metadata": {},
   "source": [
    "### 2. Add cyclical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabab776-e7ff-4b58-934f-dba9306397fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cyclical features for hour of the day and month of the year\n",
    "def add_cyclic(x_train):\n",
    "    train_data = x_train.copy()\n",
    "   \n",
    "    train_data['hour_sin'] = np.sin(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['hour_cos'] = np.cos(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['month_sin'] = np.sin(2 * np.pi * (train_data['month']-1) / 12)\n",
    "    train_data['month_cos'] = np.cos(2 * np.pi * (train_data['month']-1) / 12)\n",
    "    \n",
    "    #train_data.drop(columns = ['hour','month'],inplace = True)\n",
    "    return train_data\n",
    "\n",
    "x_train_a1 = add_cyclic(x_train_a1)\n",
    "x_train_b1 = add_cyclic(x_train_b1)\n",
    "x_train_c1 = add_cyclic(x_train_c1)\n",
    "\n",
    "x_test_a1 = add_cyclic(x_test_a1)\n",
    "x_test_b1 = add_cyclic(x_test_b1)\n",
    "x_test_c1 = add_cyclic(x_test_c1)\n",
    "\n",
    "x_train_a2 = add_cyclic(x_train_a2)\n",
    "x_train_b2 = add_cyclic(x_train_b2)\n",
    "x_train_c2 = add_cyclic(x_train_c2)\n",
    "\n",
    "x_test_a2 = add_cyclic(x_test_a2)\n",
    "x_test_b2 = add_cyclic(x_test_b2)\n",
    "x_test_c2 = add_cyclic(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7bcf2a-7ff1-494d-9c2c-08ef792370bd",
   "metadata": {},
   "source": [
    "### 3. Add direct_rad x sun_elevation feature(?) Skulle denne med i Autogluon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8be47-63e2-4c31-a402-21cbc7ef507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_and_interaction_features(x):\n",
    "    data = x.copy()\n",
    "    # Calculate rolling averages for 'direct_rad:W' and 'diffuse_rad:W\n",
    "\n",
    "    # Create interaction term between 'direct_rad:W' and 'sun_elevation:d'\n",
    "    data['direct_rad_x_sun_elevation_Q0'] = data['direct_rad:W_Q0'] * data['sun_elevation:d_Q0']\n",
    "    data['direct_rad_x_sun_elevation_Q1'] = data['direct_rad:W_Q1'] * data['sun_elevation:d_Q1']\n",
    "    data['direct_rad_x_sun_elevation_Q2'] = data['direct_rad:W_Q2'] * data['sun_elevation:d_Q2']\n",
    "    data['direct_rad_x_sun_elevation_Q3'] = data['direct_rad:W_Q3'] * data['sun_elevation:d_Q3']\n",
    "    return data\n",
    "\n",
    "\n",
    "x_train_a2 = add_rolling_and_interaction_features(x_train_a2)\n",
    "x_train_b2 = add_rolling_and_interaction_features(x_train_b2)\n",
    "x_train_c2 = add_rolling_and_interaction_features(x_train_c2)\n",
    "\n",
    "x_test_a2 = add_rolling_and_interaction_features(x_test_a2)\n",
    "x_test_b2 = add_rolling_and_interaction_features(x_test_b2)\n",
    "x_test_c2 = add_rolling_and_interaction_features(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7317c6-dd5e-443b-93b3-36da2870be15",
   "metadata": {},
   "source": [
    "### 4. Remove 'date_forecast' from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661cfc7-ac4e-4390-a053-f2c6ce440658",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_a1=x_test_a1.drop(columns = ['date_forecast'])\n",
    "x_test_b1=x_test_b1.drop(columns = ['date_forecast'])\n",
    "x_test_c1=x_test_c1.drop(columns = ['date_forecast'])\n",
    "\n",
    "x_test_a2=x_test_a2.drop(columns = ['date_forecast'])\n",
    "x_test_b2=x_test_b2.drop(columns = ['date_forecast'])\n",
    "x_test_c2=x_test_c2.drop(columns = ['date_forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73965368-3f16-4af7-a145-7d63602e2b7c",
   "metadata": {},
   "source": [
    "## Part 2: Model Building \n",
    "1. Catboost Model\n",
    "2. Autogluon Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d36734-3c71-4d5b-97b7-4771bb5a3810",
   "metadata": {},
   "source": [
    "### 1. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddf100-953f-4369-a174-86d059699b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge x_train and train for training models\n",
    "merged_a1 = pd.merge(x_train_a1, train_a1, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_b1 = pd.merge(x_train_b1, train_b1, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_c1 = pd.merge(x_train_c1, train_c1, left_on='date_forecast', right_on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102f9e3-ac1d-4122-a2a0-f254819cfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_catboost_multiple_seed(merged_df,x_test,number_of_models):\n",
    "    merged_df = merged_df.drop(columns=['date_forecast', 'time'])\n",
    "    X = merged_df.drop(columns=['pv_measurement'])\n",
    "    y = merged_df['pv_measurement']\n",
    "    \n",
    "    predictions = []\n",
    "    models = []\n",
    "    scores = []\n",
    "    seeds = range(number_of_models)\n",
    "    \n",
    "    for seed in seeds:\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "            X, y, train_size=0.8, random_state=seed)\n",
    "        \n",
    "        catboost_model = CatBoostRegressor(\n",
    "            cat_features=['observed'],\n",
    "            iterations=10000,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE',\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        catboost_model.fit(X_train, y_train, eval_set=(X_validation, y_validation),\n",
    "                           use_best_model=True, early_stopping_rounds=200)\n",
    "        \n",
    "        score = catboost_model.get_best_score()['validation']['MAE']\n",
    "        scores.append(score)\n",
    "        # Print the best validation MAE for the current seed\n",
    "        print(f\"Best validation MAE for seed {seed}: {score}\")\n",
    "        \n",
    "        \n",
    "        # Predict using the current model\n",
    "        preds = catboost_model.predict(x_test)\n",
    "        predictions.append(preds)\n",
    "        models.append(catboost_model)\n",
    "    \n",
    "    # Average the predictions from all models\n",
    "    averaged_predictions = np.mean(predictions, axis=0)\n",
    "    average_score = np.mean(scores, axis = 0)\n",
    "    \n",
    "    return averaged_predictions,models, average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eae8bc-293c-4f15-9bcd-f443141c2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a1, models_a, avg_a = build_catboost_multiple_seed(merged_a1,x_test_a1,20)\n",
    "pred_b1, models_b, avg_b = build_catboost_multiple_seed(merged_b1,x_test_b1,20)\n",
    "pred_c1, models_c, avg_c= build_catboost_multiple_seed(merged_c1,x_test_c1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f427e7b-4085-4fd3-b474-e150cdc6d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_a, avg_b, avg_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c0563-5eaf-4baf-9df4-bd748102217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub(pred_a,pred_b,pred_c):\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "    submission['prediction'] = np.concatenate([pred_a,pred_b,pred_c])\n",
    "    submission.loc[submission['prediction'] < 0, 'prediction'] = 0\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc90db7-e142-49a7-bc65-8dd1de8bac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sub = create_sub(pred_a1,pred_b1,pred_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c02cb-1afe-44e8-9f55-5d00d0dea8ca",
   "metadata": {},
   "source": [
    "### 2. Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77346e8c-49cc-4f57-9ce4-1080dbe8e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge x_train and train for training models\n",
    "merged_a2 = pd.merge(x_train_a2, train_a2, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_b2 = pd.merge(x_train_b2, train_b2, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_c2 = pd.merge(x_train_c2, train_c2, left_on='date_forecast', right_on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22379979-1262-4abb-bcef-c2788a085ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42  # Replace with your desired seed value\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d4bdd-3e15-432e-a953-2279d1b9c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autogluon(merged_data, time_limit,location):\n",
    "    merged_df = merged_data.drop(columns=['date_forecast', 'time'])\n",
    "    \n",
    "    predictor = TabularPredictor(\n",
    "        label ='pv_measurement',\n",
    "        eval_metric= 'mean_absolute_error',\n",
    "        path = f'AutgluonModels/{location}'\n",
    "    )\n",
    "\n",
    "    predictor.fit(\n",
    "        train_data = merged_df, \n",
    "        verbosity = 0,\n",
    "        presets='best_quality', \n",
    "        time_limit= time_limit,\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40ae0a-32fb-4af5-8bc4-2f5261ed748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a2 = build_autogluon(merged_a2,3600,'A')\n",
    "model_b2 = build_autogluon(merged_b2,3600,'B')\n",
    "model_c2 = build_autogluon(merged_c2,3600,'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0c06f-57b3-49a4-aadc-92cee3421b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a2 = model_a2.predict(x_test_a2)\n",
    "pred_b2 = model_b2.predict(x_test_b2)\n",
    "pred_c2 = model_c2.predict(x_test_c2)\n",
    "\n",
    "gluon_sub = create_sub(pred_a2,pred_b2,pred_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6def0-1aeb-4438-8145-b5bf53866a39",
   "metadata": {},
   "source": [
    "## Part 3: Blend predictions and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd9440-abff-44bb-a6be-730b70f6296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted average of catboost and autogluon model\n",
    "def weighted_avg(sub1,sub2, w1, w2):\n",
    "    merged_df = pd.merge(sub1, sub2, on=['id'])\n",
    "    merged_df['prediction'] = merged_df['prediction_x']*w1 + merged_df['prediction_y']*w2\n",
    "    final_df = merged_df.drop(columns=['prediction_x', 'prediction_y'])\n",
    "    final_df.loc[final_df['prediction'] < 8, 'prediction'] = 0\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237c751-43f8-4d85-9320-2f9d853b81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1 = weighted_avg(cat_sub,gluon_sub,0.5,0.5)\n",
    "final_df2 = weighted_avg(cat_sub,gluon_sub,0.7,0.3)\n",
    "final_df3 = weighted_avg(cat_sub,gluon_sub,0.6,0.4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9411f-0b7a-48b5-b9d0-4f6a60342758",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1.to_csv('Final_subs/20Cat60gluonShortNotebookmodified.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fef1b-661c-4289-9b59-c4e51971f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df2.to_csv('Final_subs/TrainedOverNightShortNotebook70_30.csv', index=False)\n",
    "#final_df3.to_csv('Final_subs/TrainedOverNightShortNotebook5050.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
