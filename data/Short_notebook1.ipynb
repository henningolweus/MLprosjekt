{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f9066f-4ed9-4027-bceb-01d0433ec4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3163e94-32aa-4c71-8532-39acafd66a08",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4a2577-d7b2-4cb8-a8a2-c71f5d4996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "x_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "x_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "x_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "x_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "x_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "x_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "x_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "x_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "x_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4281fc-efcb-4228-9ecb-7357104f19c2",
   "metadata": {},
   "source": [
    "## Merge x_train observed and x_train_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe1154c-544b-4578-8823-11ac72258c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a = pd.concat([x_train_observed_a,x_train_estimated_a])\n",
    "x_train_b = pd.concat([x_train_observed_b,x_train_estimated_b])\n",
    "x_train_c = pd.concat([x_train_observed_c,x_train_estimated_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8f57c-dd52-4fdc-9909-6c8441c0ef68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "1. Remove NaN columns\n",
    "2. Aggregate data from 15 minute to hourly intervals:\n",
    "   - Method 1: Take the mean over all four 15minutes recording, resulting in hourly measurements.\n",
    "   - Method 2: Create a separate column for each 15-minute value, effectively quadrupling the number of columns.\n",
    "3. Handle consecutive pv measurments\n",
    "4. Remove rows in train with NaN values in pv measurement\n",
    "5. Remove rows that have timestamps that are not present in both x and y\n",
    "6. Remove rows with NaN values in all columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3318fa-cfb0-4209-950f-511cc839e281",
   "metadata": {},
   "source": [
    "### 1. Remove NaN columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b243223b-3af0-4809-a58c-7837e7d8ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove snow:density column as well as rows with only NaN values\n",
    "def remove_nan_cols(x,remove_cols):\n",
    "    df = x.copy()\n",
    "    df = df.drop(columns = remove_cols ) #Should we include 'cloud_base_agl:m' and ceiling_height_agl:m ['snow_density:kgm3']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8ded65-d60b-48fb-a95b-52bc5e572fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Catboost\n",
    "x_train_a1 = remove_nan_cols(x_train_a,['snow_density:kgm3'])\n",
    "x_train_b1 = remove_nan_cols(x_train_b,['snow_density:kgm3'])\n",
    "x_train_c1 = remove_nan_cols(x_train_c,['snow_density:kgm3'])\n",
    "\n",
    "x_test_a1 = remove_nan_cols(x_test_estimated_a,['snow_density:kgm3'])\n",
    "x_test_b1 = remove_nan_cols(x_test_estimated_b,['snow_density:kgm3'])\n",
    "x_test_c1 = remove_nan_cols(x_test_estimated_c,['snow_density:kgm3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ea17b1-60b1-4b49-8653-cf3572654759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Autogluon\n",
    "x_train_a2 =  remove_nan_cols(x_train_a,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_train_b2 =  remove_nan_cols(x_train_b,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_train_c2 =  remove_nan_cols(x_train_c,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "\n",
    "x_test_a2 =  remove_nan_cols(x_test_estimated_a,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_test_b2 =  remove_nan_cols(x_test_estimated_b,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_test_c2 =  remove_nan_cols(x_test_estimated_c,['snow_density:kgm3', 'cloud_base_agl:m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e541e33-c23b-4804-aa81-e9dae1f402f7",
   "metadata": {},
   "source": [
    "### 2. Transform data to hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428524f-a553-4451-bded-1515313bce8b",
   "metadata": {},
   "source": [
    "**Method 1: Take the mean over all four 15minutes recording, resulting in hourly measurements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b66434-dc7b-4fa2-a875-cbcf6798177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_hourly(x): \n",
    "    df = x.copy()\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    \n",
    "    # Aggregating by averaging over quartarly measurements\n",
    "    df_hourly = df.resample('H').mean(numeric_only=False)\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "    return df_hourly\n",
    "\n",
    "x_train_a_hourly = resample_to_hourly(x_train_a1)\n",
    "x_train_b_hourly = resample_to_hourly(x_train_b1)\n",
    "x_train_c_hourly = resample_to_hourly(x_train_c1)\n",
    "\n",
    "x_test_a_hourly = resample_to_hourly(x_test_a1)\n",
    "x_test_b_hourly = resample_to_hourly(x_test_b1)\n",
    "x_test_c_hourly = resample_to_hourly(x_test_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528c9f9d-1c20-4ff1-aaa3-8ba77d129fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only rows in test that are given in the test csv\n",
    "test = pd.read_csv('test.csv')\n",
    "pred_time_stamps = test['time'].unique()\n",
    "x_test_a1 = x_test_a_hourly[x_test_a_hourly['date_forecast'].isin(pred_time_stamps)]\n",
    "x_test_b1 = x_test_b_hourly[x_test_b_hourly['date_forecast'].isin(pred_time_stamps)]\n",
    "x_test_c1 = x_test_c_hourly[x_test_c_hourly['date_forecast'].isin(pred_time_stamps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3a464-61c5-46a8-9d69-d9800c5bbd1e",
   "metadata": {},
   "source": [
    "**Method 2: Create a separate column for each 15-minute value, quadrupling the number of columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26db8279-7e16-4164-9efd-676f94a7f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_hourly_quarters(x, date_column='date_forecast', exclude_column='date_calc'):\n",
    "    df = x.copy()\n",
    "    # Ensure the date column is in datetime format and set as the index\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df.set_index(date_column, inplace=True)\n",
    "    \n",
    "    # Separate the column to exclude from the resampling\n",
    "    excluded_data = df[[exclude_column]].resample('H').first()  # You can use 'first' or 'last' here\n",
    "    \n",
    "    # Remove the excluded column from df before pivoting\n",
    "    df = df.drop(columns=[exclude_column])\n",
    "\n",
    "    # Add a column for the 15-minute period within the hour\n",
    "    df['quarter'] = df.index.minute // 15  # Use floor division to get the quarter number (0, 1, 2, 3)\n",
    "\n",
    "    # Pivot the table. For each feature, create a new column for each 15-minute period.\n",
    "    df_pivot = df.pivot_table(index=df.index.floor('H'),\n",
    "                              columns='quarter',\n",
    "                              aggfunc='first')  # We use 'first' because each quarter should be unique\n",
    "    \n",
    "    # Flatten the multi-level column index\n",
    "    df_pivot.columns = ['{}_Q{}'.format(feature, quarter) for feature, quarter in df_pivot.columns]\n",
    "\n",
    "    # Reset the index to be able to merge on the date_column\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    excluded_data.reset_index(inplace=True)\n",
    "\n",
    "    # Merge back the excluded column\n",
    "    df_hourly = pd.merge(excluded_data, df_pivot, on=date_column)\n",
    "\n",
    "    return df_hourly\n",
    "\n",
    "# Make sure to pass the column name that contains the datetime information\n",
    "x_train_a_hourly2 = resample_to_hourly_quarters(x_train_a2, date_column='date_forecast')\n",
    "x_train_b_hourly2 = resample_to_hourly_quarters(x_train_b2, date_column='date_forecast')\n",
    "x_train_c_hourly2 = resample_to_hourly_quarters(x_train_c2, date_column='date_forecast')\n",
    "\n",
    "x_test_a_hourly2 = resample_to_hourly_quarters(x_test_a2, date_column='date_forecast')\n",
    "x_test_b_hourly2 = resample_to_hourly_quarters(x_test_b2, date_column='date_forecast')\n",
    "x_test_c_hourly2 = resample_to_hourly_quarters(x_test_b2, date_column='date_forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27eb6b7-8929-4662-b7d6-7ba59e7cc6ef",
   "metadata": {},
   "source": [
    "### 3. Handle consecutive pv measurments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4921d339-ce3b-4827-9d4c-b6e117ea901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filters out rows from a DataFrame where the 'pv_measurement' column has consecutive identical values beyond a specified threshold.\n",
    "def remove_constant_intervals(y_train, low_thresh, upp_thresh = 10**6):\n",
    "    \"\"\"\n",
    "    Identify and remove intervals of constant PV readings that exceed a specified duration. \n",
    "    Constant readings may indicate sensor malfunctions or data logging issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = y_train.copy()\n",
    "    \n",
    "    # Calculate the difference in production values\n",
    "    df['diff'] = df['pv_measurement'].diff()\n",
    "\n",
    "    # Identify where the difference is zero\n",
    "    df['zero_diff'] = df['diff'].abs() < 1e-5\n",
    "\n",
    "    # Identify groups of consecutive zero differences\n",
    "    df['group'] = (df['zero_diff'] != df['zero_diff'].shift()).cumsum()\n",
    "\n",
    "    # Filter out only the groups with consecutive zero differences\n",
    "    constant_intervals = df[df['zero_diff']].groupby('group').agg(start=('time', 'min'), \n",
    "                                                                  end=('time', 'max'),\n",
    "                                                                  duration=('time', 'size'))\n",
    "    \n",
    "    # Filter intervals based on the threshold\n",
    "    interval_df_thresh = constant_intervals[(constant_intervals['duration'] > low_thresh) & (constant_intervals['duration'] <upp_thresh)]\n",
    "    \n",
    "    # Remove rows from the main dataframe that fall within these intervals\n",
    "    for _, row in interval_df_thresh.iterrows():\n",
    "        start_time, end_time = row['start'], row['end']\n",
    "        df = df[(df['time'] < start_time) | (df['time'] > end_time)]\n",
    "    \n",
    "    # Drop the added columns used for calculations\n",
    "    df.drop(columns=['diff', 'zero_diff', 'group'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1831d275-58fa-4be3-856e-956225bfc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in groups of constant values, where duration of constant measurements is > 1 day (24 hours)\n",
    "train_a = remove_constant_intervals(train_a,24)\n",
    "train_b = remove_constant_intervals(train_b,24)\n",
    "train_c = remove_constant_intervals(train_c,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c51db-d90e-484e-8dbd-66d769725b8f",
   "metadata": {},
   "source": [
    "### 4.Remove rows in train with NaN values in pv measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17d4ad34-079e-41d1-bda7-d0c0aed65e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify the indices of the rows with NaN values in the 'pv_measurement' column\n",
    "nan_indices_a = train_a[train_a['pv_measurement'].isna()].index\n",
    "nan_indices_b = train_b[train_b['pv_measurement'].isna()].index\n",
    "nan_indices_c = train_c[train_c['pv_measurement'].isna()].index\n",
    "\n",
    "# Drop these indices from y_train\n",
    "train_a = train_a.drop(nan_indices_a).reset_index(drop = True)\n",
    "train_b = train_b.drop(nan_indices_b).reset_index(drop = True)\n",
    "train_c = train_c.drop(nan_indices_c).reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0687c-4aac-42c8-848c-bb2d3e5bd3a8",
   "metadata": {},
   "source": [
    "### 5. Remove rows that have timestamps that are not present in both x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e721f6-c93e-4baf-900d-2655ac180c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with date-time values that are not present in both x and y in order to synchronize x and its labels. \n",
    "def remove_non_synchronous_rows(x_train, y_train, x_date_column='date_forecast', y_date_column='time'):\n",
    "    # Convert date columns to datetime format for easier comparison\n",
    "    x_train[x_date_column] = pd.to_datetime(x_train[x_date_column])\n",
    "    y_train[y_date_column] = pd.to_datetime(y_train[y_date_column])\n",
    "    \n",
    "    # Find common dates\n",
    "    common_dates = x_train[x_date_column][x_train[x_date_column].isin(y_train[y_date_column])]\n",
    "    \n",
    "    # Filter both datasets based on common dates\n",
    "    x_train_synced = x_train.loc[x_train[x_date_column].isin(common_dates)]\n",
    "    y_train_synced = y_train.loc[y_train[y_date_column].isin(common_dates)]\n",
    "    \n",
    "    return x_train_synced, y_train_synced\n",
    "\n",
    "# Remove the rows with date and time that only shows up in one of the sets\n",
    "x_train_a1, train_a = remove_non_synchronous_rows(x_train_a_hourly, train_a)\n",
    "x_train_b1, train_b = remove_non_synchronous_rows(x_train_b_hourly, train_b)\n",
    "x_train_c1, train_c = remove_non_synchronous_rows(x_train_c_hourly, train_c)\n",
    "\n",
    "# Remove the rows with date and time that only shows up in one of the sets\n",
    "x_train_a2, train_a = remove_non_synchronous_rows(x_train_a_hourly2, train_a)\n",
    "x_train_b2, train_b = remove_non_synchronous_rows(x_train_b_hourly2, train_b)\n",
    "x_train_c2, train_c = remove_non_synchronous_rows(x_train_c_hourly2, train_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c41e19-4128-4ce9-9ae1-7ef7f928b529",
   "metadata": {},
   "source": [
    "### 6. Remove rows with NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fccf118e-40d4-49de-be03-6aa676d0d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Catboost\n",
    "x_train_a1 = x_train_a1.dropna(subset=['diffuse_rad:W'])\n",
    "x_train_b1 = x_train_b1.dropna(subset=['diffuse_rad:W'])\n",
    "x_train_c1 = x_train_c1.dropna(subset=['diffuse_rad:W'])\n",
    "\n",
    "x_test_a1 = x_test_a1.dropna(subset=['diffuse_rad:W'])\n",
    "x_test_b1 = x_test_b1.dropna(subset=['diffuse_rad:W'])\n",
    "x_test_c1 = x_test_c1.dropna(subset=['diffuse_rad:W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8ccb278-97eb-447d-b608-c6efd99f89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Autogluon\n",
    "x_train_a2 = x_train_a2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_train_b2 = x_train_b2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_train_c2 = x_train_c2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "\n",
    "x_test_a2 = x_test_a_hourly2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_test_b2 = x_test_b_hourly2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_test_c2 = x_test_c_hourly2.dropna(subset=['diffuse_rad:W_Q1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c7283-ec5a-404f-884a-81b7fc1d6c04",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering \n",
    "1. Add time features (hour, day, month, year) + binary observed column\n",
    "2. Add cyclical features\n",
    "3. Add direct_rad x sun_elevation feature\n",
    "4. Remove 'date_forecast' from test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3108f82-60c2-4f28-bdcc-1330b23a5231",
   "metadata": {},
   "source": [
    "### 1. Add time features: hour, day, month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe27439-8471-458f-b10c-d5bf04561cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts year, month, day, and hour features from a given datetime column\n",
    "def extract_date_features(X):\n",
    "    df = X.copy()\n",
    "    # Extract features\n",
    "    df['year'] = df['date_forecast'].dt.year\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "    df['day'] = df['date_forecast'].dt.day\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    \n",
    "    df['observed'] = (df['date_calc'].isna()).astype(int)\n",
    "    df['observed'] = df['observed'].astype(str)\n",
    "    \n",
    "    \n",
    "    df = df.drop(columns = ['date_calc'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26f83329-902f-428f-a87b-cbdd7d1d7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a1 = extract_date_features(x_train_a1)\n",
    "x_train_b1 = extract_date_features(x_train_b1)\n",
    "x_train_c1 = extract_date_features(x_train_c1)\n",
    "\n",
    "x_test_a1 = extract_date_features(x_test_a1)\n",
    "x_test_b1 = extract_date_features(x_test_b1)\n",
    "x_test_c1 = extract_date_features(x_test_c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f77904-e2a2-4ebd-af49-7e885a8110dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a2 = extract_date_features(x_train_a2)\n",
    "x_train_b2 = extract_date_features(x_train_b2)\n",
    "x_train_c2 = extract_date_features(x_train_c2)\n",
    "\n",
    "x_test_a2 = extract_date_features(x_test_a2)\n",
    "x_test_b2 = extract_date_features(x_test_b2)\n",
    "x_test_c2 = extract_date_features(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590f5f0-722e-4f3d-abd4-c78c9daaed52",
   "metadata": {},
   "source": [
    "### 2. Add cyclical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dabab776-e7ff-4b58-934f-dba9306397fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cyclical features for hour of the day and month of the year\n",
    "def add_cyclic(x_train):\n",
    "    train_data = x_train.copy()\n",
    "   \n",
    "    train_data['hour_sin'] = np.sin(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['hour_cos'] = np.cos(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['month_sin'] = np.sin(2 * np.pi * (train_data['month']-1) / 12)\n",
    "    train_data['month_cos'] = np.cos(2 * np.pi * (train_data['month']-1) / 12)\n",
    "    \n",
    "    #train_data.drop(columns = ['hour','month'],inplace = True)\n",
    "    return train_data\n",
    "\n",
    "x_train_a1 = add_cyclic(x_train_a1)\n",
    "x_train_b1 = add_cyclic(x_train_b1)\n",
    "x_train_c1 = add_cyclic(x_train_c1)\n",
    "\n",
    "x_test_a1 = add_cyclic(x_test_a1)\n",
    "x_test_b1 = add_cyclic(x_test_b1)\n",
    "x_test_c1 = add_cyclic(x_test_c1)\n",
    "\n",
    "x_train_a2 = add_cyclic(x_train_a2)\n",
    "x_train_b2 = add_cyclic(x_train_b2)\n",
    "x_train_c2 = add_cyclic(x_train_c2)\n",
    "\n",
    "x_test_a2 = add_cyclic(x_test_a2)\n",
    "x_test_b2 = add_cyclic(x_test_b2)\n",
    "x_test_c2 = add_cyclic(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7bcf2a-7ff1-494d-9c2c-08ef792370bd",
   "metadata": {},
   "source": [
    "### 3. Add direct_rad x sun_elevation feature(?) Skulle denne med i Autogluon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69e8be47-63e2-4c31-a402-21cbc7ef507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_and_interaction_features(x):\n",
    "    data = x.copy()\n",
    "    # Calculate rolling averages for 'direct_rad:W' and 'diffuse_rad:W\n",
    "\n",
    "    # Create interaction term between 'direct_rad:W' and 'sun_elevation:d'\n",
    "    data['direct_rad_x_sun_elevation_Q0'] = data['direct_rad:W_Q0'] * data['sun_elevation:d_Q0']\n",
    "    data['direct_rad_x_sun_elevation_Q1'] = data['direct_rad:W_Q1'] * data['sun_elevation:d_Q1']\n",
    "    data['direct_rad_x_sun_elevation_Q2'] = data['direct_rad:W_Q2'] * data['sun_elevation:d_Q2']\n",
    "    data['direct_rad_x_sun_elevation_Q3'] = data['direct_rad:W_Q3'] * data['sun_elevation:d_Q3']\n",
    "    return data\n",
    "\n",
    "\n",
    "x_train_a2 = add_rolling_and_interaction_features(x_train_a2)\n",
    "x_train_b2 = add_rolling_and_interaction_features(x_train_b2)\n",
    "x_train_c2 = add_rolling_and_interaction_features(x_train_c2)\n",
    "\n",
    "x_test_a2 = add_rolling_and_interaction_features(x_test_a2)\n",
    "x_test_b2 = add_rolling_and_interaction_features(x_test_b2)\n",
    "x_test_c2 = add_rolling_and_interaction_features(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7317c6-dd5e-443b-93b3-36da2870be15",
   "metadata": {},
   "source": [
    "### 4. Remove 'date_forecast' from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3661cfc7-ac4e-4390-a053-f2c6ce440658",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_a1=x_test_a1.drop(columns = ['date_forecast'])\n",
    "x_test_b1=x_test_b1.drop(columns = ['date_forecast'])\n",
    "x_test_c1=x_test_c1.drop(columns = ['date_forecast'])\n",
    "\n",
    "x_test_a2=x_test_a2.drop(columns = ['date_forecast'])\n",
    "x_test_b2=x_test_b2.drop(columns = ['date_forecast'])\n",
    "x_test_c2=x_test_c2.drop(columns = ['date_forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73965368-3f16-4af7-a145-7d63602e2b7c",
   "metadata": {},
   "source": [
    "## Part 2: Model Building \n",
    "1. Catboost Model\n",
    "2. Autogluon Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d36734-3c71-4d5b-97b7-4771bb5a3810",
   "metadata": {},
   "source": [
    "### 1. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58ddf100-953f-4369-a174-86d059699b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge x_train and train for training models\n",
    "merged_a1 = pd.merge(x_train_a1, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_b1 = pd.merge(x_train_b1, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_c1 = pd.merge(x_train_c1, train_c, left_on='date_forecast', right_on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1102f9e3-ac1d-4122-a2a0-f254819cfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_catboost_multiple_seed(merged_df,x_test,number_of_models):\n",
    "    merged_df = merged_df.drop(columns=['date_forecast', 'time'])\n",
    "    X = merged_df.drop(columns=['pv_measurement'])\n",
    "    y = merged_df['pv_measurement']\n",
    "    \n",
    "    predictions = []\n",
    "    models = []\n",
    "    scores = []\n",
    "    seeds = range(number_of_models)\n",
    "    \n",
    "    for seed in seeds:\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "            X, y, train_size=0.8, random_state=seed)\n",
    "        \n",
    "        catboost_model = CatBoostRegressor(\n",
    "            cat_features=['observed'],\n",
    "            iterations=10000,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE',\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        catboost_model.fit(X_train, y_train, eval_set=(X_validation, y_validation),\n",
    "                           use_best_model=True, early_stopping_rounds=200)\n",
    "        \n",
    "        score = catboost_model.get_best_score()['validation']['MAE']\n",
    "        scores.append(score)\n",
    "        # Print the best validation MAE for the current seed\n",
    "        print(f\"Best validation MAE for seed {seed}: {score}\")\n",
    "        \n",
    "        \n",
    "        # Predict using the current model\n",
    "        preds = catboost_model.predict(x_test)\n",
    "        predictions.append(preds)\n",
    "        models.append(catboost_model)\n",
    "    \n",
    "    # Average the predictions from all models\n",
    "    averaged_predictions = np.mean(predictions, axis=0)\n",
    "    average_score = np.mean(scores, axis = 0)\n",
    "    \n",
    "    return averaged_predictions,models, average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1eae8bc-293c-4f15-9bcd-f443141c2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation MAE for seed 0: 194.93429152346036\n",
      "Best validation MAE for seed 1: 193.4782138055255\n",
      "Best validation MAE for seed 2: 195.97581682902893\n",
      "Best validation MAE for seed 3: 190.23967204190663\n",
      "Best validation MAE for seed 4: 182.71361864869448\n"
     ]
    }
   ],
   "source": [
    "pred_a1, models_a, avg_a = build_catboost_multiple_seed(merged_a1,x_test_a1,30)\n",
    "pred_b1, models_b, avg_b = build_catboost_multiple_seed(merged_b1,x_test_b1,30)\n",
    "pred_c1, models_c, avg_c= build_catboost_multiple_seed(merged_c1,x_test_c1,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f427e7b-4085-4fd3-b474-e150cdc6d25c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(avg_a, \u001b[43mavg_b\u001b[49m, avg_c)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_b' is not defined"
     ]
    }
   ],
   "source": [
    "print(avg_a, avg_b, avg_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c0563-5eaf-4baf-9df4-bd748102217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub(pred_a,pred_b,pred_c):\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "    submission['prediction'] = np.concatenate([pred_a,pred_b,pred_c])\n",
    "    submission.loc[submission['prediction'] < 0, 'prediction'] = 0\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc90db7-e142-49a7-bc65-8dd1de8bac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sub = create_sub(pred_a1,pred_b1,pred_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c02cb-1afe-44e8-9f55-5d00d0dea8ca",
   "metadata": {},
   "source": [
    "### 2. Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77346e8c-49cc-4f57-9ce4-1080dbe8e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge x_train and train for training models\n",
    "merged_a2 = pd.merge(x_train_a2, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_b2 = pd.merge(x_train_b2, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_c2 = pd.merge(x_train_c2, train_c, left_on='date_forecast', right_on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22379979-1262-4abb-bcef-c2788a085ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42  # Replace with your desired seed value\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e30d4bdd-3e15-432e-a953-2279d1b9c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autogluon(merged_data, time_limit,location):\n",
    "    merged_df = merged_data.drop(columns=['date_forecast', 'time'])\n",
    "    \n",
    "    predictor = TabularPredictor(\n",
    "        label ='pv_measurement',\n",
    "        eval_metric= 'mean_absolute_error',\n",
    "        path = f'AutgluonModels/{location}'\n",
    "    )\n",
    "\n",
    "    predictor.fit(\n",
    "        train_data = merged_df, \n",
    "        verbosity = 0,\n",
    "        presets='best_quality', \n",
    "        time_limit= time_limit,\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e40ae0a-32fb-4af5-8bc4-2f5261ed748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutgluonModels/A\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutgluonModels/B\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_a2 \u001b[38;5;241m=\u001b[39m build_autogluon(merged_a2,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model_b2 \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_autogluon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_b2\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_c2 \u001b[38;5;241m=\u001b[39m build_autogluon(merged_c2,\u001b[38;5;241m3600\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m, in \u001b[0;36mbuild_autogluon\u001b[0;34m(merged_data, time_limit, location)\u001b[0m\n\u001b[1;32m      2\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m merged_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m predictor \u001b[38;5;241m=\u001b[39m TabularPredictor(\n\u001b[1;32m      5\u001b[0m     label \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpv_measurement\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutgluonModels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_quality\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictor\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:986\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m     aux_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_weighted_ensemble\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuning_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlabeled_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_bag_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bag_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_bag_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bag_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bag_holdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bag_holdout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\n\u001b[1;32m   1006\u001b[0m     keep_only_best\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_only_best\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1007\u001b[0m     refit_full\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefit_full\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[1;32m   1013\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/tabular/learner/abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:157\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval_metric\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 157\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m    171\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/tabular/trainer/auto_trainer.py:114\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m log_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m20\u001b[39m, log_str)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2371\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rows_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_val)\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_cols_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m-> 2371\u001b[0m model_names_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multi_levels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoGluon did not successfully train any models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:395\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    393\u001b[0m         core_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_core)\n\u001b[1;32m    394\u001b[0m         aux_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_aux)\n\u001b[0;32m--> 395\u001b[0m     base_model_names, aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     model_names_fit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base_model_names \u001b[38;5;241m+\u001b[39m aux_models\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:539\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    537\u001b[0m     core_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[1;32m    538\u001b[0m     aux_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[0;32m--> 539\u001b[0m core_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_new_level_aux(\n\u001b[1;32m    555\u001b[0m         X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, base_model_names\u001b[38;5;241m=\u001b[39mcore_models, level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, infer_limit\u001b[38;5;241m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maux_kwargs\n\u001b[1;32m    556\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:673\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2321\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2320\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 2321\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_initial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2332\u001b[0m     n_repeat_start \u001b[38;5;241m=\u001b[39m n_repeats_initial\n\u001b[1;32m   2333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2170\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2169\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2170\u001b[0m     models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2183\u001b[0m multi_fold_time_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m multi_fold_time_start\n\u001b[1;32m   2184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2278\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         time_start_model \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2277\u001b[0m         time_left \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time_start_model \u001b[38;5;241m-\u001b[39m time_start)\n\u001b[0;32m-> 2278\u001b[0m model_name_trained_lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single_full\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   2047\u001b[0m         bagged_model_fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[1;32m   2048\u001b[0m             k_fold\u001b[38;5;241m=\u001b[39mk_fold, k_fold_start\u001b[38;5;241m=\u001b[39mk_fold_start, k_fold_end\u001b[38;5;241m=\u001b[39mk_fold_end, n_repeats\u001b[38;5;241m=\u001b[39mn_repeats, n_repeat_start\u001b[38;5;241m=\u001b[39mn_repeat_start\n\u001b[1;32m   2049\u001b[0m         )\n\u001b[1;32m   2050\u001b[0m         model_fit_kwargs\u001b[38;5;241m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[0;32m-> 2051\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1733\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1731\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1733\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1735\u001b[0m fit_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_evaluation:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1684\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[0;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, total_resources\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AbstractModel:\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;124;03m    Returns trained model object.\u001b[39;00m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1684\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py:829\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_fit_resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_memory_usage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 829\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py:169\u001b[0m, in \u001b[0;36mStackerEnsembleModel._fit\u001b[0;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     time_limit \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:250\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit\u001b[0;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m save_bag_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_bag_folds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k_fold \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_child_oof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_child_oof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_oof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_skip_oof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:416\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit_single\u001b[0;34m(self, X, y, model_base, use_child_oof, time_limit, skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m time_start_predict \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_base\u001b[38;5;241m.\u001b[39m_get_tags()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_oof\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oof_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_oof_pred_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mWARNING: `use_child_oof` was specified but child model does not have a dedicated `get_oof_pred_proba` method. This model may have heavily overfit validation scores.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py:115\u001b[0m, in \u001b[0;36mKNNModel.get_oof_pred_proba\u001b[0;34m(self, X, normalize, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_oof_pred_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"X should be the same X passed to `.fit`\"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     y_oof_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_oof_pred_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_pred_probas\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/knn_model.py:129\u001b[0m, in \u001b[0;36mKNNModel._get_oof_pred_proba\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     y_oof_pred_proba \u001b[38;5;241m=\u001b[39m KNeighborsClassifierLOOMixin\u001b[38;5;241m.\u001b[39mpredict_proba_loo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     y_oof_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[43mKNeighborsRegressorLOOMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_loo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m y_oof_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_proba_to_unified_form(y_oof_pred_proba)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X_unused_index:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/knn/_knn_loo_variants.py:115\u001b[0m, in \u001b[0;36mKNeighborsRegressorLOOMixin.predict_loo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_loo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict the target for the training data via leave-one-out.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        Target values.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     neigh_dist, neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     weights \u001b[38;5;241m=\u001b[39m _get_weights(neigh_dist, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[1;32m    119\u001b[0m     _y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_base.py:824\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    817\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    820\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[1;32m    821\u001b[0m     )\n\u001b[1;32m    822\u001b[0m )\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[0;32m--> 824\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[1;32m    836\u001b[0m ):\n\u001b[1;32m    837\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[1;32m    838\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[1;32m    839\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:289\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin64\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    278\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    279\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly float64 or float32 datasets pairs are supported at this time, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot: X.dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and Y.dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m )\n",
      "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:584\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin32.compute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/threadpoolctl.py:440\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_a2 = build_autogluon(merged_a2,3600,'A')\n",
    "model_b2 = build_autogluon(merged_b2,3600,'B')\n",
    "model_c2 = build_autogluon(merged_c2,3600,'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0c06f-57b3-49a4-aadc-92cee3421b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a2 = model_a2.predict(x_test_a2)\n",
    "pred_b2 = model_b2.predict(x_test_b2)\n",
    "pred_c2 = model_c2.predict(x_test_c2)\n",
    "\n",
    "gluon_sub = create_sub(pred_a2,pred_b2,pred_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6def0-1aeb-4438-8145-b5bf53866a39",
   "metadata": {},
   "source": [
    "## Part 3: Blend predictions and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd9440-abff-44bb-a6be-730b70f6296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted average of catboost and autogluon model\n",
    "def weighted_avg(sub1,sub2, w1, w2):\n",
    "    merged_df = pd.merge(sub1, sub2, on=['id'])\n",
    "    merged_df['prediction'] = merged_df['prediction_x']*w1 + merged_df['prediction_y']*w2\n",
    "    final_df = merged_df.drop(columns=['prediction_x', 'prediction_y'])\n",
    "    final_df.loc[final_df['prediction'] < 8, 'prediction'] = 0\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237c751-43f8-4d85-9320-2f9d853b81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1 = weighted_avg(cat_sub,gluon_sub,0.7,0.3)\n",
    "final_df2 = weighted_avg(cat_sub,gluon_sub,0.6,0.4)\n",
    "final_df3 = weighted_avg(cat_sub,gluon_sub,0.5,0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9411f-0b7a-48b5-b9d0-4f6a60342758",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2.to_csv('Final_subs/TrainedOverNightShortNotebook.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
