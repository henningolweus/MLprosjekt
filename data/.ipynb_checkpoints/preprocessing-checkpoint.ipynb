{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method of preprocessing\n",
    "1. Import the datasets\n",
    "2. Transform the datasets into hourly format\n",
    "3. Handle missing values and rows\n",
    "4. Final feature engineering and storing\n",
    "5. Create additional combined super-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "x_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "x_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "x_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "x_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "x_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "x_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_merged_a = pd.concat([x_train_observed_a,x_train_estimated_a])\n",
    "x_train_merged_b = pd.concat([x_train_observed_b,x_train_estimated_b])\n",
    "x_train_merged_c = pd.concat([x_train_observed_c,x_train_estimated_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transform into hourly\n",
    "\n",
    "- Observed and estimated measurements are taken every 15 minutes, while energy is measured every hour\n",
    "- We need to transform the measurements into hourly aggregations to match the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating from 15-minute intervals to hourly intervals based on different aggregation methods\n",
    "def resample_to_hourly(df, aggregation_methods,mean = False):\n",
    "    if mean:\n",
    "        df.set_index('date_forecast', inplace=True)\n",
    "        df_hourly = df.resample('H').mean()\n",
    "        df_hourly.reset_index(inplace=True)\n",
    "    else:\n",
    "        df.set_index('date_forecast', inplace=True)\n",
    "        df_hourly = df.resample('H').agg(aggregation_methods)\n",
    "        df_hourly.reset_index(inplace=True)\n",
    "    \n",
    "    return df_hourly\n",
    "\n",
    "# Aggregation methods based on features' names\n",
    "aggregation_methods = {\n",
    "    'date_calc' : 'max',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'mean',\n",
    "    'clear_sky_energy_1h:J': 'sum',\n",
    "    'clear_sky_rad:W': 'mean',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'max',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'diffuse_rad:W': 'mean',\n",
    "    'diffuse_rad_1h:J': 'sum',\n",
    "    'direct_rad:W': 'mean',\n",
    "    'direct_rad_1h:J': 'sum',\n",
    "    'effective_cloud_cover:p': 'mean',\n",
    "    'elevation:m': 'mean',\n",
    "    'fresh_snow_12h:cm': 'sum',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'sum',\n",
    "    'fresh_snow_3h:cm': 'sum',\n",
    "    'fresh_snow_6h:cm': 'sum',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'max',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'mean',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'mean',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'mean',\n",
    "    'sun_elevation:d': 'mean',\n",
    "    'super_cooled_liquid_water:kgm2': 'mean',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating using specific transformations for each feature \n",
    "\"\"\"\n",
    "x_train_a_hourly = resample_to_hourly(x_train_merged_a, aggregation_methods)\n",
    "x_train_b_hourly = resample_to_hourly(x_train_merged_b, aggregation_methods)\n",
    "x_train_c_hourly = resample_to_hourly(x_train_merged_c, aggregation_methods)\n",
    "\n",
    "x_test_a_hourly = resample_to_hourly(x_test_estimated_a, aggregation_methods)\n",
    "x_test_b_hourly = resample_to_hourly(x_test_estimated_b, aggregation_methods)\n",
    "x_test_c_hourly = resample_to_hourly(x_test_estimated_c, aggregation_methods)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating by averaging over quartarly measurements\n",
    "x_train_a_hourly = resample_to_hourly(x_train_merged_a, aggregation_methods, True)\n",
    "x_train_b_hourly = resample_to_hourly(x_train_merged_b, aggregation_methods, True)\n",
    "x_train_c_hourly = resample_to_hourly(x_train_merged_c, aggregation_methods, True)\n",
    "\n",
    "x_test_a_hourly = resample_to_hourly(x_test_estimated_a, aggregation_methods, True)\n",
    "x_test_b_hourly = resample_to_hourly(x_test_estimated_b, aggregation_methods, True)\n",
    "x_test_c_hourly = resample_to_hourly(x_test_estimated_c, aggregation_methods, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only rows in test that are given in the test csv\n",
    "test = pd.read_csv('test.csv')\n",
    "pred_time_stamps = test['time'].unique()\n",
    "x_test_a = x_test_a_hourly[x_test_a_hourly['date_forecast'].isin(pred_time_stamps)]\n",
    "x_test_b = x_test_b_hourly[x_test_b_hourly['date_forecast'].isin(pred_time_stamps)]\n",
    "x_test_c = x_test_c_hourly[x_test_c_hourly['date_forecast'].isin(pred_time_stamps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Handle missing values and rows\n",
    "- Remove NaN pv measurement values from y\n",
    "- Remove rows that are not present in both x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indices of the rows with NaN values in the 'pv_measurement' column\n",
    "nan_indices_a = train_a[train_a['pv_measurement'].isna()].index\n",
    "nan_indices_b = train_b[train_b['pv_measurement'].isna()].index\n",
    "nan_indices_c = train_c[train_c['pv_measurement'].isna()].index\n",
    "\n",
    "# Drop these indices from y_train\n",
    "train_a = train_a.drop(nan_indices_a).reset_index(drop = True)\n",
    "train_b = train_b.drop(nan_indices_b).reset_index(drop = True)\n",
    "train_c = train_c.drop(nan_indices_c).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with date-time values that are not present in both x and y in order to synchronize x and its labels. \n",
    "def remove_non_synchronous_rows(x_train, y_train, x_date_column='date_forecast', y_date_column='time'):\n",
    "    # Convert date columns to datetime format for easier comparison\n",
    "    x_train[x_date_column] = pd.to_datetime(x_train[x_date_column])\n",
    "    y_train[y_date_column] = pd.to_datetime(y_train[y_date_column])\n",
    "    \n",
    "    # Find common dates\n",
    "    common_dates = x_train[x_date_column][x_train[x_date_column].isin(y_train[y_date_column])]\n",
    "    \n",
    "    # Filter both datasets based on common dates\n",
    "    x_train_synced = x_train[x_train[x_date_column].isin(common_dates)]\n",
    "    y_train_synced = y_train[y_train[y_date_column].isin(common_dates)]\n",
    "    \n",
    "    return x_train_synced, y_train_synced\n",
    "\n",
    "# Remove the rows with date and time that only shows up in one of the sets\n",
    "x_train_a_hourly, train_a = remove_non_synchronous_rows(x_train_a_hourly, train_a)\n",
    "x_train_b_hourly, train_b = remove_non_synchronous_rows(x_train_b_hourly, train_b)\n",
    "x_train_c_hourly, train_c = remove_non_synchronous_rows(x_train_c_hourly, train_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final feature engineering and storing\n",
    "- Extract year, month, day and hour features from each datetime column\n",
    "- Store the cleaned data for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts year, month, day, and hour features from a given datetime column\n",
    "def extract_date_features(X):\n",
    "    df = X.copy()\n",
    "    # Extract features\n",
    "    df['year'] = df['date_forecast'].dt.year\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "    #df['day'] = df['date_forecast'].dt.day\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    \n",
    "    df['estimated'] = (~df['date_calc'].isna()).astype(int)\n",
    "    \n",
    "    df = df.drop(columns = ['date_calc'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a = extract_date_features(x_train_a_hourly)\n",
    "x_train_b = extract_date_features(x_train_b_hourly)\n",
    "x_train_c = extract_date_features(x_train_c_hourly)\n",
    "\n",
    "x_test_a = extract_date_features(x_test_a)\n",
    "x_test_b = extract_date_features(x_test_b)\n",
    "x_test_c = extract_date_features(x_test_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a['time'] = pd.to_datetime(train_a['time'])\n",
    "train_b['time'] = pd.to_datetime(train_b['time'])\n",
    "train_c['time'] = pd.to_datetime(train_c['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cleaned datasets\n",
    "output_dir = 'cleaned_data'\n",
    "# Ensure directory exists, if not create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Directories for each location\n",
    "dir_a = os.path.join(output_dir, 'A')\n",
    "dir_b = os.path.join(output_dir, 'B')\n",
    "dir_c = os.path.join(output_dir, 'C')\n",
    "\n",
    "# Ensure subdirectories exist\n",
    "for dir_path in [dir_a, dir_b, dir_c]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "\n",
    "# Saving datasets for location A\n",
    "x_train_a.to_csv(os.path.join(dir_a, 'x_train_a.csv'), index = False)\n",
    "x_test_a.to_csv(os.path.join(dir_a, 'x_test_a.csv'), index = False)\n",
    "train_a.to_csv(os.path.join(dir_a, 'train_a.csv'), index = False)\n",
    "\n",
    "# Saving datasets for location B\n",
    "x_train_b.to_csv(os.path.join(dir_b, 'x_train_b.csv'), index = False)\n",
    "x_test_b.to_csv(os.path.join(dir_b, 'x_test_b.csv'), index = False)\n",
    "train_b.to_csv(os.path.join(dir_b, 'train_b.csv'), index = False)\n",
    "\n",
    "# Saving datasets for location C\n",
    "x_train_c.to_csv(os.path.join(dir_c, 'x_train_c.csv'), index = False)\n",
    "x_test_c.to_csv(os.path.join(dir_c, 'x_test_c.csv'), index = False)\n",
    "train_c.to_csv(os.path.join(dir_c, 'train_c.csv'),index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
