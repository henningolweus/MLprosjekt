{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle team name: AI papi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Team Member                | StudentID  |\n",
    "|----------------------------|------------|\n",
    "| Per-Christian Færøvik Grieg |  544635 |\n",
    "| Syver Verlo Nes            | 539086  |\n",
    "| Henning Olweus             | 527221  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n",
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f9066f-4ed9-4027-bceb-01d0433ec4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3163e94-32aa-4c71-8532-39acafd66a08",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4a2577-d7b2-4cb8-a8a2-c71f5d4996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "x_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "x_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "x_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "x_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "x_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "x_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "x_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "x_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "x_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4281fc-efcb-4228-9ecb-7357104f19c2",
   "metadata": {},
   "source": [
    "## Merge x_train observed and x_train_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe1154c-544b-4578-8823-11ac72258c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a = pd.concat([x_train_observed_a,x_train_estimated_a])\n",
    "x_train_b = pd.concat([x_train_observed_b,x_train_estimated_b])\n",
    "x_train_c = pd.concat([x_train_observed_c,x_train_estimated_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8f57c-dd52-4fdc-9909-6c8441c0ef68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "1. Remove NaN columns\n",
    "2. Aggregate data from 15 minute to hourly intervals:\n",
    "   - Method 1: Take the mean over all four 15minutes recording, resulting in hourly measurements.\n",
    "   - Method 2: Create a separate column for each 15-minute value, effectively quadrupling the number of columns.\n",
    "3. Handle consecutive pv measurments\n",
    "4. Remove rows in train with NaN values in pv measurement\n",
    "5. Remove rows that have timestamps that are not present in both x and y\n",
    "6. Remove rows with NaN values in all columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3318fa-cfb0-4209-950f-511cc839e281",
   "metadata": {},
   "source": [
    "### 1. Remove NaN columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b243223b-3af0-4809-a58c-7837e7d8ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove snow:density column as well as rows with only NaN values\n",
    "def remove_nan_cols(x,remove_cols):\n",
    "    df = x.copy()\n",
    "    df = df.drop(columns = remove_cols ) #Should we include 'cloud_base_agl:m' and ceiling_height_agl:m ['snow_density:kgm3']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8ded65-d60b-48fb-a95b-52bc5e572fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Catboost\n",
    "x_train_a1 = remove_nan_cols(x_train_a,['snow_density:kgm3'])\n",
    "x_train_b1 = remove_nan_cols(x_train_b,['snow_density:kgm3'])\n",
    "x_train_c1 = remove_nan_cols(x_train_c,['snow_density:kgm3'])\n",
    "\n",
    "x_test_a1 = remove_nan_cols(x_test_estimated_a,['snow_density:kgm3'])\n",
    "x_test_b1 = remove_nan_cols(x_test_estimated_b,['snow_density:kgm3'])\n",
    "x_test_c1 = remove_nan_cols(x_test_estimated_c,['snow_density:kgm3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ea17b1-60b1-4b49-8653-cf3572654759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Autogluon\n",
    "x_train_a2 =  remove_nan_cols(x_train_a,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_train_b2 =  remove_nan_cols(x_train_b,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_train_c2 =  remove_nan_cols(x_train_c,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "\n",
    "x_test_a2 =  remove_nan_cols(x_test_estimated_a,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_test_b2 =  remove_nan_cols(x_test_estimated_b,['snow_density:kgm3', 'cloud_base_agl:m'])\n",
    "x_test_c2 =  remove_nan_cols(x_test_estimated_c,['snow_density:kgm3', 'cloud_base_agl:m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e541e33-c23b-4804-aa81-e9dae1f402f7",
   "metadata": {},
   "source": [
    "### 2. Transform data to hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428524f-a553-4451-bded-1515313bce8b",
   "metadata": {},
   "source": [
    "**Method 1: Take the mean over all four 15minutes recording, resulting in hourly measurements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b66434-dc7b-4fa2-a875-cbcf6798177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_80360/19032443.py:7: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_80360/19032443.py:7: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_80360/19032443.py:7: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_80360/19032443.py:7: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_80360/19032443.py:7: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "/var/tmp/ipykernel_80360/19032443.py:7: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "def resample_to_hourly(x): \n",
    "    df = x.copy()\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    \n",
    "    df_hourly_date_calc = df.resample('H')['date_calc'].first()\n",
    "    # Aggregating by averaging over quartarly measurements\n",
    "    df_hourly = df.resample('H').mean()\n",
    "    df_hourly['date_calc'] = df_hourly_date_calc\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "    \n",
    "    return df_hourly\n",
    "\n",
    "\n",
    "x_train_a_hourly = resample_to_hourly(x_train_a1)\n",
    "x_train_b_hourly = resample_to_hourly(x_train_b1)\n",
    "x_train_c_hourly = resample_to_hourly(x_train_c1)\n",
    "\n",
    "x_test_a_hourly = resample_to_hourly(x_test_a1)\n",
    "x_test_b_hourly = resample_to_hourly(x_test_b1)\n",
    "x_test_c_hourly = resample_to_hourly(x_test_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528c9f9d-1c20-4ff1-aaa3-8ba77d129fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only rows in test that are given in the test csv\n",
    "test = pd.read_csv('test.csv')\n",
    "pred_time_stamps = test['time'].unique()\n",
    "x_test_a1 = x_test_a_hourly[x_test_a_hourly['date_forecast'].isin(pred_time_stamps)]\n",
    "x_test_b1 = x_test_b_hourly[x_test_b_hourly['date_forecast'].isin(pred_time_stamps)]\n",
    "x_test_c1 = x_test_c_hourly[x_test_c_hourly['date_forecast'].isin(pred_time_stamps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3a464-61c5-46a8-9d69-d9800c5bbd1e",
   "metadata": {},
   "source": [
    "**Method 2: Create a separate column for each 15-minute value, quadrupling the number of columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26db8279-7e16-4164-9efd-676f94a7f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_hourly_quarters(x, date_column='date_forecast', exclude_column='date_calc'):\n",
    "    df = x.copy()\n",
    "    # Ensure the date column is in datetime format and set as the index\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df.set_index(date_column, inplace=True)\n",
    "    \n",
    "    # Separate the column to exclude from the resampling\n",
    "    excluded_data = df[[exclude_column]].resample('H').first()  # You can use 'first' or 'last' here\n",
    "    \n",
    "    # Remove the excluded column from df before pivoting\n",
    "    df = df.drop(columns=[exclude_column])\n",
    "\n",
    "    # Add a column for the 15-minute period within the hour\n",
    "    df['quarter'] = df.index.minute // 15  # Use floor division to get the quarter number (0, 1, 2, 3)\n",
    "\n",
    "    # Pivot the table. For each feature, create a new column for each 15-minute period.\n",
    "    df_pivot = df.pivot_table(index=df.index.floor('H'),\n",
    "                              columns='quarter',\n",
    "                              aggfunc='first')  # We use 'first' because each quarter should be unique\n",
    "    \n",
    "    # Flatten the multi-level column index\n",
    "    df_pivot.columns = ['{}_Q{}'.format(feature, quarter) for feature, quarter in df_pivot.columns]\n",
    "\n",
    "    # Reset the index to be able to merge on the date_column\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    excluded_data.reset_index(inplace=True)\n",
    "\n",
    "    # Merge back the excluded column\n",
    "    df_hourly = pd.merge(excluded_data, df_pivot, on=date_column)\n",
    "\n",
    "    return df_hourly\n",
    "\n",
    "# Make sure to pass the column name that contains the datetime information\n",
    "x_train_a_hourly2 = resample_to_hourly_quarters(x_train_a2, date_column='date_forecast')\n",
    "x_train_b_hourly2 = resample_to_hourly_quarters(x_train_b2, date_column='date_forecast')\n",
    "x_train_c_hourly2 = resample_to_hourly_quarters(x_train_c2, date_column='date_forecast')\n",
    "\n",
    "x_test_a_hourly2 = resample_to_hourly_quarters(x_test_a2, date_column='date_forecast')\n",
    "x_test_b_hourly2 = resample_to_hourly_quarters(x_test_b2, date_column='date_forecast')\n",
    "x_test_c_hourly2 = resample_to_hourly_quarters(x_test_b2, date_column='date_forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4ea3d-f5de-48a1-ad94-fa74f576876e",
   "metadata": {},
   "source": [
    "### 3.Remove rows in train with NaN values in pv measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "198cf756-bcf8-443c-8663-e24477233065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify the indices of the rows with NaN values in the 'pv_measurement' column\n",
    "nan_indices_a = train_a[train_a['pv_measurement'].isna()].index\n",
    "nan_indices_b = train_b[train_b['pv_measurement'].isna()].index\n",
    "nan_indices_c = train_c[train_c['pv_measurement'].isna()].index\n",
    "\n",
    "# Drop these indices from y_train\n",
    "train_a = train_a.drop(nan_indices_a).reset_index(drop = True)\n",
    "train_b = train_b.drop(nan_indices_b).reset_index(drop = True)\n",
    "train_c = train_c.drop(nan_indices_c).reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27eb6b7-8929-4662-b7d6-7ba59e7cc6ef",
   "metadata": {},
   "source": [
    "### 4. Handle consecutive pv measurments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59890780-a5c0-4238-8c34-696c8d3d6c96",
   "metadata": {},
   "source": [
    "**Method used catboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4921d339-ce3b-4827-9d4c-b6e117ea901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filters out rows from a DataFrame where the 'pv_measurement' column has consecutive identical values beyond a specified threshold.\n",
    "def remove_constant_intervals(y_train, low_thresh, upp_thresh = 10**6):\n",
    "    \"\"\"\n",
    "    Identify and remove intervals of constant PV readings that exceed a specified duration. \n",
    "    Constant readings may indicate sensor malfunctions or data logging issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = y_train.copy()\n",
    "    \n",
    "    # Calculate the difference in production values\n",
    "    df['diff'] = df['pv_measurement'].diff()\n",
    "\n",
    "    # Identify where the difference is zero\n",
    "    df['zero_diff'] = df['diff'].abs() < 1e-5\n",
    "\n",
    "    # Identify groups of consecutive zero differences\n",
    "    df['group'] = (df['zero_diff'] != df['zero_diff'].shift()).cumsum()\n",
    "\n",
    "    # Filter out only the groups with consecutive zero differences\n",
    "    constant_intervals = df[df['zero_diff']].groupby('group').agg(start=('time', 'min'), \n",
    "                                                                  end=('time', 'max'),\n",
    "                                                                  duration=('time', 'size'))\n",
    "    \n",
    "    # Filter intervals based on the threshold\n",
    "    interval_df_thresh = constant_intervals[(constant_intervals['duration'] > low_thresh) & (constant_intervals['duration'] <upp_thresh)]\n",
    "    \n",
    "    # Remove rows from the main dataframe that fall within these intervals\n",
    "    for _, row in interval_df_thresh.iterrows():\n",
    "        start_time, end_time = row['start'], row['end']\n",
    "        df = df[(df['time'] < start_time) | (df['time'] > end_time)]\n",
    "    \n",
    "    # Drop the added columns used for calculations\n",
    "    df.drop(columns=['diff', 'zero_diff', 'group'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1831d275-58fa-4be3-856e-956225bfc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in groups of constant values, where duration of constant measurements is > 1 day (24 hours)\n",
    "train_a1 = remove_constant_intervals(train_a,24)\n",
    "train_b1 = remove_constant_intervals(train_b,24)\n",
    "train_c1 = remove_constant_intervals(train_c,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57997a03-e985-49b5-9ee2-6e609e5e2979",
   "metadata": {},
   "source": [
    "**Method used for Autogluon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d6894d3-3233-45d7-a217-8f6c1020e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters out rows from a DataFrame where the 'pv_measurement' column has consecutive zero-values beyond a specified threshold. \n",
    "# Also removes all rows, except the first, where 'pv_measurement' is identical for two or more rows. \n",
    "def remove_constant_intervals2(df: pd.DataFrame, threshold: int = 24) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters out rows from a DataFrame where the 'pv_measurement' column has:\n",
    "    1. Consecutive zero values beyond a specified threshold.\n",
    "    2. Consecutive non-zero identical values, keeping only the first in each sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame to be processed.\n",
    "    - threshold: Maximum allowable number of consecutive zero values. Rows in streaks\n",
    "      beyond this threshold will be removed.\n",
    "\n",
    "    Returns:\n",
    "    - Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Create a mask to identify where the current 'pv_measurement'-row is different from the previous one\n",
    "    is_different = df['pv_measurement'] != df['pv_measurement'].shift()\n",
    "    \n",
    "    # Additional mask to identify non-zero values\n",
    "    is_nonzero = df['pv_measurement'] != 0\n",
    "\n",
    "    # Use cumsum to generate unique group identifiers for each streak of identical measurements\n",
    "    groups = is_different.cumsum()\n",
    "\n",
    "    # Count the number of entries in each group\n",
    "    df['group_counts'] = df.groupby(groups)['pv_measurement'].transform('count')\n",
    "\n",
    "    # Identify groups of zeros that exceed the specified threshold\n",
    "    to_remove_zeros = (df['group_counts'] > threshold) & ~is_nonzero\n",
    "\n",
    "    # Identify the first entry in groups of consecutive non-zero identical values\n",
    "    to_keep_nonzero = (df['pv_measurement'].shift() != df['pv_measurement']) | is_different | ~is_nonzero\n",
    "\n",
    "    # Combine conditions\n",
    "    to_remove = to_remove_zeros | ~to_keep_nonzero\n",
    "\n",
    "    # Drop the temporary 'group_counts' column\n",
    "    df = df.drop(columns=['group_counts'])\n",
    "\n",
    "    # Filter out the rows that do not meet the conditions\n",
    "    filtered_df = df[~to_remove]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "054796c6-b372-478c-a615-2b07add0b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a2 = remove_constant_intervals2(train_a,24)\n",
    "train_b2 = remove_constant_intervals2(train_b,24)\n",
    "train_c2 = remove_constant_intervals2(train_c,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0687c-4aac-42c8-848c-bb2d3e5bd3a8",
   "metadata": {},
   "source": [
    "### 5. Remove rows that have timestamps that are not present in both x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e721f6-c93e-4baf-900d-2655ac180c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with date-time values that are not present in both x and y in order to synchronize x and its labels. \n",
    "def remove_non_synchronous_rows(x_train, y_train, x_date_column='date_forecast', y_date_column='time'):\n",
    "    # Convert date columns to datetime format for easier comparison\n",
    "    x_train[x_date_column] = pd.to_datetime(x_train[x_date_column])\n",
    "    y_train[y_date_column] = pd.to_datetime(y_train[y_date_column])\n",
    "    \n",
    "    # Find common dates\n",
    "    common_dates = x_train[x_date_column][x_train[x_date_column].isin(y_train[y_date_column])]\n",
    "    \n",
    "    # Filter both datasets based on common dates\n",
    "    x_train_synced = x_train.loc[x_train[x_date_column].isin(common_dates)]\n",
    "    y_train_synced = y_train.loc[y_train[y_date_column].isin(common_dates)]\n",
    "    \n",
    "    return x_train_synced, y_train_synced\n",
    "\n",
    "# Remove the rows with date and time that only shows up in one of the sets\n",
    "x_train_a1, train_a = remove_non_synchronous_rows(x_train_a_hourly, train_a)\n",
    "x_train_b1, train_b = remove_non_synchronous_rows(x_train_b_hourly, train_b)\n",
    "x_train_c1, train_c = remove_non_synchronous_rows(x_train_c_hourly, train_c)\n",
    "\n",
    "# Remove the rows with date and time that only shows up in one of the sets\n",
    "x_train_a2, train_a = remove_non_synchronous_rows(x_train_a_hourly2, train_a)\n",
    "x_train_b2, train_b = remove_non_synchronous_rows(x_train_b_hourly2, train_b)\n",
    "x_train_c2, train_c = remove_non_synchronous_rows(x_train_c_hourly2, train_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c41e19-4128-4ce9-9ae1-7ef7f928b529",
   "metadata": {},
   "source": [
    "### 6. Remove rows with NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fccf118e-40d4-49de-be03-6aa676d0d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Catboost\n",
    "x_train_a1 = x_train_a1.dropna(subset=['diffuse_rad:W'])\n",
    "x_train_b1 = x_train_b1.dropna(subset=['diffuse_rad:W'])\n",
    "x_train_c1 = x_train_c1.dropna(subset=['diffuse_rad:W'])\n",
    "\n",
    "x_test_a1 = x_test_a1.dropna(subset=['diffuse_rad:W'])\n",
    "x_test_b1 = x_test_b1.dropna(subset=['diffuse_rad:W'])\n",
    "x_test_c1 = x_test_c1.dropna(subset=['diffuse_rad:W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ccb278-97eb-447d-b608-c6efd99f89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Autogluon\n",
    "x_train_a2 = x_train_a2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_train_b2 = x_train_b2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_train_c2 = x_train_c2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "\n",
    "x_test_a2 = x_test_a_hourly2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_test_b2 = x_test_b_hourly2.dropna(subset=['diffuse_rad:W_Q1'])\n",
    "x_test_c2 = x_test_c_hourly2.dropna(subset=['diffuse_rad:W_Q1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c7283-ec5a-404f-884a-81b7fc1d6c04",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering \n",
    "1. Add time features (hour, day, month, year) + binary observed column\n",
    "2. Add cyclical features\n",
    "3. Add direct_rad x sun_elevation feature\n",
    "4. Remove 'date_forecast' from test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3108f82-60c2-4f28-bdcc-1330b23a5231",
   "metadata": {},
   "source": [
    "### 1. Add time features: hour, day, month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe27439-8471-458f-b10c-d5bf04561cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts year, month, day, and hour features from a given datetime column\n",
    "def extract_date_features(X):\n",
    "    df = X.copy()\n",
    "    # Extract features\n",
    "    df['year'] = df['date_forecast'].dt.year\n",
    "    df['month'] = df['date_forecast'].dt.month\n",
    "    df['day'] = df['date_forecast'].dt.day\n",
    "    df['hour'] = df['date_forecast'].dt.hour\n",
    "    \n",
    "    df['observed'] = (df['date_calc'].isna()).astype(int)\n",
    "    df['observed'] = df['observed'].astype(str)\n",
    "    \n",
    "    \n",
    "    df = df.drop(columns = ['date_calc'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26f83329-902f-428f-a87b-cbdd7d1d7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a1 = extract_date_features(x_train_a1)\n",
    "x_train_b1 = extract_date_features(x_train_b1)\n",
    "x_train_c1 = extract_date_features(x_train_c1)\n",
    "\n",
    "x_test_a1 = extract_date_features(x_test_a1)\n",
    "x_test_b1 = extract_date_features(x_test_b1)\n",
    "x_test_c1 = extract_date_features(x_test_c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00f77904-e2a2-4ebd-af49-7e885a8110dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a2 = extract_date_features(x_train_a2)\n",
    "x_train_b2 = extract_date_features(x_train_b2)\n",
    "x_train_c2 = extract_date_features(x_train_c2)\n",
    "\n",
    "x_test_a2 = extract_date_features(x_test_a2)\n",
    "x_test_b2 = extract_date_features(x_test_b2)\n",
    "x_test_c2 = extract_date_features(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590f5f0-722e-4f3d-abd4-c78c9daaed52",
   "metadata": {},
   "source": [
    "### 2. Add cyclical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dabab776-e7ff-4b58-934f-dba9306397fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cyclical features for hour of the day and month of the year\n",
    "def add_cyclic(x_train):\n",
    "    train_data = x_train.copy()\n",
    "   \n",
    "    train_data['hour_sin'] = np.sin(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['hour_cos'] = np.cos(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['month_sin'] = np.sin(2 * np.pi * (train_data['month']-1) / 12)\n",
    "    train_data['month_cos'] = np.cos(2 * np.pi * (train_data['month']-1) / 12)\n",
    "    \n",
    "    #train_data.drop(columns = ['hour','month'],inplace = True)\n",
    "    return train_data\n",
    "\n",
    "x_train_a1 = add_cyclic(x_train_a1)\n",
    "x_train_b1 = add_cyclic(x_train_b1)\n",
    "x_train_c1 = add_cyclic(x_train_c1)\n",
    "\n",
    "x_test_a1 = add_cyclic(x_test_a1)\n",
    "x_test_b1 = add_cyclic(x_test_b1)\n",
    "x_test_c1 = add_cyclic(x_test_c1)\n",
    "\n",
    "x_train_a2 = add_cyclic(x_train_a2)\n",
    "x_train_b2 = add_cyclic(x_train_b2)\n",
    "x_train_c2 = add_cyclic(x_train_c2)\n",
    "\n",
    "x_test_a2 = add_cyclic(x_test_a2)\n",
    "x_test_b2 = add_cyclic(x_test_b2)\n",
    "x_test_c2 = add_cyclic(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7bcf2a-7ff1-494d-9c2c-08ef792370bd",
   "metadata": {},
   "source": [
    "### 3. Add direct_rad x sun_elevation feature for Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69e8be47-63e2-4c31-a402-21cbc7ef507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_and_interaction_features(x):\n",
    "    data = x.copy()\n",
    "    # Calculate rolling averages for 'direct_rad:W' and 'diffuse_rad:W\n",
    "\n",
    "    # Create interaction term between 'direct_rad:W' and 'sun_elevation:d'\n",
    "    data['direct_rad_x_sun_elevation_Q0'] = data['direct_rad:W_Q0'] * data['sun_elevation:d_Q0']\n",
    "    data['direct_rad_x_sun_elevation_Q1'] = data['direct_rad:W_Q1'] * data['sun_elevation:d_Q1']\n",
    "    data['direct_rad_x_sun_elevation_Q2'] = data['direct_rad:W_Q2'] * data['sun_elevation:d_Q2']\n",
    "    data['direct_rad_x_sun_elevation_Q3'] = data['direct_rad:W_Q3'] * data['sun_elevation:d_Q3']\n",
    "    return data\n",
    "\n",
    "\n",
    "x_train_a2 = add_rolling_and_interaction_features(x_train_a2)\n",
    "x_train_b2 = add_rolling_and_interaction_features(x_train_b2)\n",
    "x_train_c2 = add_rolling_and_interaction_features(x_train_c2)\n",
    "\n",
    "x_test_a2 = add_rolling_and_interaction_features(x_test_a2)\n",
    "x_test_b2 = add_rolling_and_interaction_features(x_test_b2)\n",
    "x_test_c2 = add_rolling_and_interaction_features(x_test_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7317c6-dd5e-443b-93b3-36da2870be15",
   "metadata": {},
   "source": [
    "### 4. Remove 'date_forecast' from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3661cfc7-ac4e-4390-a053-f2c6ce440658",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_a1=x_test_a1.drop(columns = ['date_forecast'])\n",
    "x_test_b1=x_test_b1.drop(columns = ['date_forecast'])\n",
    "x_test_c1=x_test_c1.drop(columns = ['date_forecast'])\n",
    "\n",
    "x_test_a2=x_test_a2.drop(columns = ['date_forecast'])\n",
    "x_test_b2=x_test_b2.drop(columns = ['date_forecast'])\n",
    "x_test_c2=x_test_c2.drop(columns = ['date_forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73965368-3f16-4af7-a145-7d63602e2b7c",
   "metadata": {},
   "source": [
    "## Part 2: Model Building \n",
    "1. Catboost Model\n",
    "2. Autogluon Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d36734-3c71-4d5b-97b7-4771bb5a3810",
   "metadata": {},
   "source": [
    "### 1. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58ddf100-953f-4369-a174-86d059699b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge x_train and train for training models\n",
    "merged_a1 = pd.merge(x_train_a1, train_a1, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_b1 = pd.merge(x_train_b1, train_b1, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_c1 = pd.merge(x_train_c1, train_c1, left_on='date_forecast', right_on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1102f9e3-ac1d-4122-a2a0-f254819cfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_catboost_multiple_seed(merged_df,x_test,number_of_models):\n",
    "    merged_df = merged_df.drop(columns=['date_forecast', 'time'])\n",
    "    X = merged_df.drop(columns=['pv_measurement'])\n",
    "    y = merged_df['pv_measurement']\n",
    "    \n",
    "    predictions = []\n",
    "    models = []\n",
    "    scores = []\n",
    "    seeds = range(number_of_models)\n",
    "    \n",
    "    for seed in seeds:\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "            X, y, train_size=0.8, random_state=seed)\n",
    "        \n",
    "        catboost_model = CatBoostRegressor(\n",
    "            cat_features=['observed'],\n",
    "            iterations=10000,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE',\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        catboost_model.fit(X_train, y_train, eval_set=(X_validation, y_validation),\n",
    "                           use_best_model=True, early_stopping_rounds=200)\n",
    "        \n",
    "        score = catboost_model.get_best_score()['validation']['MAE']\n",
    "        scores.append(score)\n",
    "        # Print the best validation MAE for the current seed\n",
    "        print(f\"Best validation MAE for seed {seed}: {score}\")\n",
    "        \n",
    "        \n",
    "        # Predict using the current model\n",
    "        preds = catboost_model.predict(x_test)\n",
    "        predictions.append(preds)\n",
    "        models.append(catboost_model)\n",
    "    \n",
    "    # Average the predictions from all models\n",
    "    averaged_predictions = np.mean(predictions, axis=0)\n",
    "    average_score = np.mean(scores, axis = 0)\n",
    "    \n",
    "    return averaged_predictions,models, average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1eae8bc-293c-4f15-9bcd-f443141c2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation MAE for seed 0: 175.5860124250549\n",
      "Best validation MAE for seed 1: 171.94322061991087\n",
      "Best validation MAE for seed 2: 177.937207824725\n",
      "Best validation MAE for seed 3: 170.87770905898097\n",
      "Best validation MAE for seed 4: 162.99381722687087\n",
      "Best validation MAE for seed 5: 170.49027048727132\n",
      "Best validation MAE for seed 6: 174.40631614515695\n",
      "Best validation MAE for seed 7: 168.36628891942235\n",
      "Best validation MAE for seed 8: 176.6247454598639\n",
      "Best validation MAE for seed 9: 165.68120882709215\n",
      "Best validation MAE for seed 10: 168.5717933542984\n",
      "Best validation MAE for seed 11: 170.2498134685807\n",
      "Best validation MAE for seed 12: 174.44546264489995\n",
      "Best validation MAE for seed 13: 189.32913622484543\n",
      "Best validation MAE for seed 14: 169.43484618662347\n",
      "Best validation MAE for seed 15: 170.26622073481286\n",
      "Best validation MAE for seed 1: 24.712659297137368\n",
      "Best validation MAE for seed 2: 24.59432850863548\n",
      "Best validation MAE for seed 3: 25.00872410618475\n",
      "Best validation MAE for seed 4: 24.909624299139754\n",
      "Best validation MAE for seed 5: 24.344655782824177\n",
      "Best validation MAE for seed 6: 24.97502149881866\n",
      "Best validation MAE for seed 7: 25.21880938795846\n",
      "Best validation MAE for seed 8: 24.785771919235195\n",
      "Best validation MAE for seed 9: 25.465761546804227\n",
      "Best validation MAE for seed 10: 24.41010762286868\n",
      "Best validation MAE for seed 11: 25.263826711591246\n",
      "Best validation MAE for seed 12: 26.576825783228806\n",
      "Best validation MAE for seed 13: 24.529592653978664\n",
      "Best validation MAE for seed 14: 24.707123640839495\n",
      "Best validation MAE for seed 1: 21.083289554392714\n",
      "Best validation MAE for seed 2: 22.617298812843696\n",
      "Best validation MAE for seed 3: 22.428139807142706\n",
      "Best validation MAE for seed 4: 22.32849940647166\n",
      "Best validation MAE for seed 5: 21.374305923299264\n",
      "Best validation MAE for seed 6: 22.290806820224265\n",
      "Best validation MAE for seed 7: 21.489571192222304\n",
      "Best validation MAE for seed 8: 21.05927328498742\n",
      "Best validation MAE for seed 9: 21.795057739133007\n",
      "Best validation MAE for seed 10: 21.986284516677873\n",
      "Best validation MAE for seed 11: 22.105606139942847\n",
      "Best validation MAE for seed 12: 22.07313127797586\n",
      "Best validation MAE for seed 13: 21.19942257776429\n",
      "Best validation MAE for seed 14: 22.063328064177732\n",
      "Best validation MAE for seed 15: 22.047739028255528\n",
      "Best validation MAE for seed 16: 22.09410963749573\n",
      "Best validation MAE for seed 17: 21.30259225446659\n",
      "Best validation MAE for seed 18: 22.003667998519813\n",
      "Best validation MAE for seed 19: 21.823413202730865\n"
     ]
    }
   ],
   "source": [
    "pred_a1, models_a, avg_a = build_catboost_multiple_seed(merged_a1,x_test_a1,20)\n",
    "pred_b1, models_b, avg_b = build_catboost_multiple_seed(merged_b1,x_test_b1,20)\n",
    "pred_c1, models_c, avg_c= build_catboost_multiple_seed(merged_c1,x_test_c1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f427e7b-4085-4fd3-b474-e150cdc6d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.27243255508276 24.90319192395137 21.857663560045417\n"
     ]
    }
   ],
   "source": [
    "print(avg_a, avg_b, avg_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "596c0563-5eaf-4baf-9df4-bd748102217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub(pred_a,pred_b,pred_c):\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "    submission['prediction'] = np.concatenate([pred_a,pred_b,pred_c])\n",
    "    submission.loc[submission['prediction'] < 0, 'prediction'] = 0\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fc90db7-e142-49a7-bc65-8dd1de8bac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sub = create_sub(pred_a1,pred_b1,pred_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c02cb-1afe-44e8-9f55-5d00d0dea8ca",
   "metadata": {},
   "source": [
    "### 2. Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77346e8c-49cc-4f57-9ce4-1080dbe8e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge x_train and train for training models\n",
    "merged_a2 = pd.merge(x_train_a2, train_a2, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_b2 = pd.merge(x_train_b2, train_b2, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_c2 = pd.merge(x_train_c2, train_c2, left_on='date_forecast', right_on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22379979-1262-4abb-bcef-c2788a085ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42  # Replace with your desired seed value\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e30d4bdd-3e15-432e-a953-2279d1b9c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autogluon(merged_data, time_limit,location):\n",
    "    merged_df = merged_data.drop(columns=['date_forecast', 'time'])\n",
    "    \n",
    "    predictor = TabularPredictor(\n",
    "        label ='pv_measurement',\n",
    "        eval_metric= 'mean_absolute_error',\n",
    "        path = f'AutgluonModels/{location}'\n",
    "    )\n",
    "\n",
    "    predictor.fit(\n",
    "        train_data = merged_df, \n",
    "        verbosity = 2,\n",
    "        presets='best_quality', \n",
    "        time_limit= time_limit,\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e40ae0a-32fb-4af5-8bc4-2f5261ed748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutgluonModels/A\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 12000s\n",
      "AutoGluon will save models to \"AutgluonModels/A/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.197-1 (2023-09-29)\n",
      "Disk Space Avail:   77.41 GB / 105.09 GB (73.7%)\n",
      "Train Data Rows:    34016\n",
      "Train Data Columns: 185\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 631.83219, 1166.75725)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13582.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 28.1 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 13 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 8): ['elevation:m_Q0', 'elevation:m_Q1', 'elevation:m_Q2', 'elevation:m_Q3', 'snow_drift:idx_Q0', 'snow_drift:idx_Q1', 'snow_drift:idx_Q2', 'snow_drift:idx_Q3']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['wind_speed_w_1000hPa:ms_Q1', 'wind_speed_w_1000hPa:ms_Q2', 'wind_speed_w_1000hPa:ms_Q3']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 3 | ['wind_speed_w_1000hPa:ms_Q1', 'wind_speed_w_1000hPa:ms_Q2', 'wind_speed_w_1000hPa:ms_Q3']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 169 | ['absolute_humidity_2m:gm3_Q0', 'absolute_humidity_2m:gm3_Q1', 'absolute_humidity_2m:gm3_Q2', 'absolute_humidity_2m:gm3_Q3', 'air_density_2m:kgm3_Q0', ...]\n",
      "\t\t('int', [])    :   4 | ['year', 'month', 'day', 'hour']\n",
      "\t\t('object', []) :   1 | ['observed']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 160 | ['absolute_humidity_2m:gm3_Q0', 'absolute_humidity_2m:gm3_Q1', 'absolute_humidity_2m:gm3_Q2', 'absolute_humidity_2m:gm3_Q3', 'air_density_2m:kgm3_Q0', ...]\n",
      "\t\t('int', [])       :   4 | ['year', 'month', 'day', 'hour']\n",
      "\t\t('int', ['bool']) :  10 | ['is_day:idx_Q0', 'is_day:idx_Q1', 'is_day:idx_Q2', 'is_day:idx_Q3', 'is_in_shadow:idx_Q0', ...]\n",
      "\t1.3s = Fit runtime\n",
      "\t174 features in original data used to generate 174 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 23.74 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.48s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 7997.01s of the 11998.51s of remaining time.\n",
      "\t-222.7446\t = Validation score   (-mean_absolute_error)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t7.95s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 7985.29s of the 11986.79s of remaining time.\n",
      "\t-222.3934\t = Validation score   (-mean_absolute_error)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t6.83s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 7977.9s of the 11979.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-151.4483\t = Validation score   (-mean_absolute_error)\n",
      "\t536.76s\t = Training   runtime\n",
      "\t105.77s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 7416.36s of the 11417.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-163.3708\t = Validation score   (-mean_absolute_error)\n",
      "\t703.63s\t = Training   runtime\n",
      "\t107.34s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 6698.66s of the 10700.16s of remaining time.\n",
      "\t-180.7969\t = Validation score   (-mean_absolute_error)\n",
      "\t368.87s\t = Training   runtime\n",
      "\t3.54s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6321.63s of the 10323.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-169.2848\t = Validation score   (-mean_absolute_error)\n",
      "\t2319.56s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3998.64s of the 8000.14s of remaining time.\n",
      "\t-178.5061\t = Validation score   (-mean_absolute_error)\n",
      "\t96.32s\t = Training   runtime\n",
      "\t3.42s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 3893.92s of the 7895.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-178.326\t = Validation score   (-mean_absolute_error)\n",
      "\t197.59s\t = Training   runtime\n",
      "\t1.25s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3692.88s of the 7694.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-173.5065\t = Validation score   (-mean_absolute_error)\n",
      "\t1277.78s\t = Training   runtime\n",
      "\t83.3s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2398.93s of the 6400.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-168.48\t = Validation score   (-mean_absolute_error)\n",
      "\t332.58s\t = Training   runtime\n",
      "\t2.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2062.68s of the 6064.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-159.9143\t = Validation score   (-mean_absolute_error)\n",
      "\t1724.5s\t = Training   runtime\n",
      "\t186.43s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 799.7s of the 4298.74s of remaining time.\n",
      "\t-149.822\t = Validation score   (-mean_absolute_error)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4297.93s of the 4297.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-152.8931\t = Validation score   (-mean_absolute_error)\n",
      "\t39.8s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4254.76s of the 4254.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-149.3967\t = Validation score   (-mean_absolute_error)\n",
      "\t432.25s\t = Training   runtime\n",
      "\t3.64s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 3780.47s of the 3780.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-150.8828\t = Validation score   (-mean_absolute_error)\n",
      "\t103.45s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 3673.91s of the 3673.88s of remaining time.\n",
      "\t-149.0832\t = Validation score   (-mean_absolute_error)\n",
      "\t113.68s\t = Training   runtime\n",
      "\t3.63s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 3551.93s of the 3551.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-150.3407\t = Validation score   (-mean_absolute_error)\n",
      "\t201.98s\t = Training   runtime\n",
      "\t1.74s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 3346.35s of the 3346.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-149.5204\t = Validation score   (-mean_absolute_error)\n",
      "\t43.2s\t = Training   runtime\n",
      "\t0.68s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 3298.97s of the 3298.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-149.917\t = Validation score   (-mean_absolute_error)\n",
      "\t58.65s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 2895.83s of the 2895.8s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-150.4205\t = Validation score   (-mean_absolute_error)\n",
      "\t211.03s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2784.65s of the 2784.62s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-148.7691\t = Validation score   (-mean_absolute_error)\n",
      "\t377.99s\t = Training   runtime\n",
      "\t2.93s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2605.12s of the 2605.09s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-148.7224\t = Validation score   (-mean_absolute_error)\n",
      "\t91.98s\t = Training   runtime\n",
      "\t1.29s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 2552.11s of the 2552.08s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-151.1248\t = Validation score   (-mean_absolute_error)\n",
      "\t414.43s\t = Training   runtime\n",
      "\t3.9s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2360.23s of the 2360.2s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-149.8064\t = Validation score   (-mean_absolute_error)\n",
      "\t189.31s\t = Training   runtime\n",
      "\t2.02s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2264.85s of the 2264.82s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-152.228\t = Validation score   (-mean_absolute_error)\n",
      "\t116.72s\t = Training   runtime\n",
      "\t2.88s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2220.19s of the 2220.16s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-149.682\t = Validation score   (-mean_absolute_error)\n",
      "\t88.87s\t = Training   runtime\n",
      "\t1.29s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 2186.72s of the 2186.69s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-150.2738\t = Validation score   (-mean_absolute_error)\n",
      "\t320.59s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2073.89s of the 2073.86s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-148.1469\t = Validation score   (-mean_absolute_error)\n",
      "\t575.89s\t = Training   runtime\n",
      "\t4.55s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1872.33s of the 1872.31s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-148.5433\t = Validation score   (-mean_absolute_error)\n",
      "\t138.53s\t = Training   runtime\n",
      "\t1.96s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1822.29s of the 1822.26s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-149.4679\t = Validation score   (-mean_absolute_error)\n",
      "\t281.43s\t = Training   runtime\n",
      "\t3.16s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1498.22s of the 1498.19s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-152.2217\t = Validation score   (-mean_absolute_error)\n",
      "\t157.7s\t = Training   runtime\n",
      "\t3.74s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1453.37s of the 1453.34s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-149.5745\t = Validation score   (-mean_absolute_error)\n",
      "\t118.6s\t = Training   runtime\n",
      "\t1.82s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1420.2s of the 1420.17s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-150.2116\t = Validation score   (-mean_absolute_error)\n",
      "\t413.24s\t = Training   runtime\n",
      "\t0.82s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1324.0s of the 1323.97s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-148.0259\t = Validation score   (-mean_absolute_error)\n",
      "\t754.73s\t = Training   runtime\n",
      "\t5.93s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1141.5s of the 1141.47s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-148.408\t = Validation score   (-mean_absolute_error)\n",
      "\t182.84s\t = Training   runtime\n",
      "\t2.6s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1093.39s of the 1093.36s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-150.294\t = Validation score   (-mean_absolute_error)\n",
      "\t871.66s\t = Training   runtime\n",
      "\t7.89s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 856.34s of the 856.31s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-149.3987\t = Validation score   (-mean_absolute_error)\n",
      "\t382.76s\t = Training   runtime\n",
      "\t4.23s\t = Validation runtime\n",
      "Completed 4/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 429.79s of the 750.52s of remaining time.\n",
      "\t-146.0994\t = Validation score   (-mean_absolute_error)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 11250.26s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutgluonModels/A/\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutgluonModels/B\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 12000s\n",
      "AutoGluon will save models to \"AutgluonModels/B/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.197-1 (2023-09-29)\n",
      "Disk Space Avail:   76.42 GB / 105.09 GB (72.7%)\n",
      "Train Data Rows:    25759\n",
      "Train Data Columns: 185\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 107.48217, 212.77383)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12611.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.28 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 17 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 4): ['elevation:m_Q0', 'elevation:m_Q1', 'elevation:m_Q2', 'elevation:m_Q3']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 6): ['snow_drift:idx_Q1', 'snow_drift:idx_Q2', 'snow_drift:idx_Q3', 'wind_speed_w_1000hPa:ms_Q1', 'wind_speed_w_1000hPa:ms_Q2', 'wind_speed_w_1000hPa:ms_Q3']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 6 | ['snow_drift:idx_Q1', 'snow_drift:idx_Q2', 'snow_drift:idx_Q3', 'wind_speed_w_1000hPa:ms_Q1', 'wind_speed_w_1000hPa:ms_Q2', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 170 | ['absolute_humidity_2m:gm3_Q0', 'absolute_humidity_2m:gm3_Q1', 'absolute_humidity_2m:gm3_Q2', 'absolute_humidity_2m:gm3_Q3', 'air_density_2m:kgm3_Q0', ...]\n",
      "\t\t('int', [])    :   4 | ['year', 'month', 'day', 'hour']\n",
      "\t\t('object', []) :   1 | ['observed']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 160 | ['absolute_humidity_2m:gm3_Q0', 'absolute_humidity_2m:gm3_Q1', 'absolute_humidity_2m:gm3_Q2', 'absolute_humidity_2m:gm3_Q3', 'air_density_2m:kgm3_Q0', ...]\n",
      "\t\t('int', [])       :   4 | ['year', 'month', 'day', 'hour']\n",
      "\t\t('int', ['bool']) :  11 | ['is_day:idx_Q0', 'is_day:idx_Q1', 'is_day:idx_Q2', 'is_day:idx_Q3', 'is_in_shadow:idx_Q0', ...]\n",
      "\t1.5s = Fit runtime\n",
      "\t175 features in original data used to generate 175 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 18.01 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 7996.95s of the 11998.42s of remaining time.\n",
      "\t-33.1047\t = Validation score   (-mean_absolute_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t4.2s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 7992.23s of the 11993.7s of remaining time.\n",
      "\t-33.0149\t = Validation score   (-mean_absolute_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t4.2s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 7987.6s of the 11989.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.8962\t = Validation score   (-mean_absolute_error)\n",
      "\t460.98s\t = Training   runtime\n",
      "\t88.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 7511.33s of the 11512.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-23.6498\t = Validation score   (-mean_absolute_error)\n",
      "\t473.84s\t = Training   runtime\n",
      "\t37.17s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 7030.1s of the 11031.57s of remaining time.\n",
      "\t-26.2117\t = Validation score   (-mean_absolute_error)\n",
      "\t296.72s\t = Training   runtime\n",
      "\t2.9s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6727.58s of the 10729.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.2452\t = Validation score   (-mean_absolute_error)\n",
      "\t2086.82s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 4637.33s of the 8638.8s of remaining time.\n",
      "\t-24.6413\t = Validation score   (-mean_absolute_error)\n",
      "\t277.94s\t = Training   runtime\n",
      "\t1.7s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2318.85s of the 6320.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-23.321\t = Validation score   (-mean_absolute_error)\n",
      "\t1526.91s\t = Training   runtime\n",
      "\t82.84s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 799.69s of the 4776.63s of remaining time.\n",
      "\t-21.616\t = Validation score   (-mean_absolute_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4775.9s of the 4775.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.3738\t = Validation score   (-mean_absolute_error)\n",
      "\t35.08s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4737.26s of the 4737.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.0698\t = Validation score   (-mean_absolute_error)\n",
      "\t26.29s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 4707.49s of the 4707.47s of remaining time.\n",
      "\t-21.6327\t = Validation score   (-mean_absolute_error)\n",
      "\t74.9s\t = Training   runtime\n",
      "\t3.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 4164.62s of the 4164.6s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.8296\t = Validation score   (-mean_absolute_error)\n",
      "\t137.02s\t = Training   runtime\n",
      "\t1.34s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 4024.13s of the 4024.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.8445\t = Validation score   (-mean_absolute_error)\n",
      "\t46.14s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 3973.79s of the 3973.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.5619\t = Validation score   (-mean_absolute_error)\n",
      "\t185.94s\t = Training   runtime\n",
      "\t1.84s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3784.26s of the 3784.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.0518\t = Validation score   (-mean_absolute_error)\n",
      "\t89.85s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 3690.83s of the 3690.81s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.1473\t = Validation score   (-mean_absolute_error)\n",
      "\t90.8s\t = Training   runtime\n",
      "\t2.49s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3630.89s of the 3630.86s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.8623\t = Validation score   (-mean_absolute_error)\n",
      "\t55.04s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 3598.83s of the 3598.8s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.0671\t = Validation score   (-mean_absolute_error)\n",
      "\t220.51s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 3482.71s of the 3482.69s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.6644\t = Validation score   (-mean_absolute_error)\n",
      "\t273.05s\t = Training   runtime\n",
      "\t2.46s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 3343.0s of the 3342.98s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.6137\t = Validation score   (-mean_absolute_error)\n",
      "\t88.47s\t = Training   runtime\n",
      "\t1.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 3296.95s of the 3296.93s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.2458\t = Validation score   (-mean_absolute_error)\n",
      "\t367.1s\t = Training   runtime\n",
      "\t3.54s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3112.47s of the 3112.44s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.0195\t = Validation score   (-mean_absolute_error)\n",
      "\t331.25s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2817.36s of the 2817.34s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.5323\t = Validation score   (-mean_absolute_error)\n",
      "\t408.25s\t = Training   runtime\n",
      "\t3.51s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2678.55s of the 2678.53s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.5658\t = Validation score   (-mean_absolute_error)\n",
      "\t130.17s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 2632.76s of the 2632.74s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.1636\t = Validation score   (-mean_absolute_error)\n",
      "\t542.83s\t = Training   runtime\n",
      "\t5.48s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2453.11s of the 2453.09s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.788\t = Validation score   (-mean_absolute_error)\n",
      "\t272.45s\t = Training   runtime\n",
      "\t2.62s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2359.86s of the 2359.83s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.9995\t = Validation score   (-mean_absolute_error)\n",
      "\t463.17s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2036.37s of the 2036.35s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.4878\t = Validation score   (-mean_absolute_error)\n",
      "\t546.85s\t = Training   runtime\n",
      "\t4.4s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1893.94s of the 1893.92s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.5504\t = Validation score   (-mean_absolute_error)\n",
      "\t171.91s\t = Training   runtime\n",
      "\t2.23s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1848.67s of the 1848.65s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.0812\t = Validation score   (-mean_absolute_error)\n",
      "\t749.34s\t = Training   runtime\n",
      "\t7.63s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1638.85s of the 1638.83s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.7213\t = Validation score   (-mean_absolute_error)\n",
      "\t362.36s\t = Training   runtime\n",
      "\t3.53s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1545.28s of the 1545.25s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.071\t = Validation score   (-mean_absolute_error)\n",
      "\t332.19s\t = Training   runtime\n",
      "\t10.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1497.35s of the 1497.33s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.8102\t = Validation score   (-mean_absolute_error)\n",
      "\t133.93s\t = Training   runtime\n",
      "\t1.51s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1468.85s of the 1468.83s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.9993\t = Validation score   (-mean_absolute_error)\n",
      "\t583.46s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1344.95s of the 1344.92s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.0598\t = Validation score   (-mean_absolute_error)\n",
      "\t920.16s\t = Training   runtime\n",
      "\t9.39s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 981.17s of the 981.15s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.7255\t = Validation score   (-mean_absolute_error)\n",
      "\t456.64s\t = Training   runtime\n",
      "\t4.4s\t = Validation runtime\n",
      "Completed 5/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 477.59s of the 882.55s of remaining time.\n",
      "\t-21.2008\t = Validation score   (-mean_absolute_error)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 11118.19s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutgluonModels/B/\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutgluonModels/C\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 12000s\n",
      "AutoGluon will save models to \"AutgluonModels/C/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.197-1 (2023-09-29)\n",
      "Disk Space Avail:   75.78 GB / 105.09 GB (72.1%)\n",
      "Train Data Rows:    20651\n",
      "Train Data Columns: 185\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, -0.0, 95.61191, 179.53607)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12074.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 17.06 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 9 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 8): ['elevation:m_Q0', 'elevation:m_Q1', 'elevation:m_Q2', 'elevation:m_Q3', 'snow_drift:idx_Q0', 'snow_drift:idx_Q1', 'snow_drift:idx_Q2', 'snow_drift:idx_Q3']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['snow_depth:cm_Q1', 'snow_depth:cm_Q2']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['snow_depth:cm_Q1', 'snow_depth:cm_Q2']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 170 | ['absolute_humidity_2m:gm3_Q0', 'absolute_humidity_2m:gm3_Q1', 'absolute_humidity_2m:gm3_Q2', 'absolute_humidity_2m:gm3_Q3', 'air_density_2m:kgm3_Q0', ...]\n",
      "\t\t('int', [])    :   4 | ['year', 'month', 'day', 'hour']\n",
      "\t\t('object', []) :   1 | ['observed']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 162 | ['absolute_humidity_2m:gm3_Q0', 'absolute_humidity_2m:gm3_Q1', 'absolute_humidity_2m:gm3_Q2', 'absolute_humidity_2m:gm3_Q3', 'air_density_2m:kgm3_Q0', ...]\n",
      "\t\t('int', [])       :   4 | ['year', 'month', 'day', 'hour']\n",
      "\t\t('int', ['bool']) :   9 | ['is_day:idx_Q0', 'is_day:idx_Q1', 'is_day:idx_Q2', 'is_day:idx_Q3', 'is_in_shadow:idx_Q0', ...]\n",
      "\t1.3s = Fit runtime\n",
      "\t175 features in original data used to generate 175 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 14.56 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 7997.04s of the 11998.56s of remaining time.\n",
      "\t-29.3641\t = Validation score   (-mean_absolute_error)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t2.77s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 7993.9s of the 11995.42s of remaining time.\n",
      "\t-29.2574\t = Validation score   (-mean_absolute_error)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t2.68s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 7990.86s of the 11992.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.7213\t = Validation score   (-mean_absolute_error)\n",
      "\t434.38s\t = Training   runtime\n",
      "\t74.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 7544.01s of the 11545.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.1376\t = Validation score   (-mean_absolute_error)\n",
      "\t525.41s\t = Training   runtime\n",
      "\t38.29s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 7008.8s of the 11010.31s of remaining time.\n",
      "\t-23.7713\t = Validation score   (-mean_absolute_error)\n",
      "\t185.97s\t = Training   runtime\n",
      "\t2.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6818.85s of the 10820.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.2513\t = Validation score   (-mean_absolute_error)\n",
      "\t2009.25s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 4806.35s of the 8807.86s of remaining time.\n",
      "\t-23.1637\t = Validation score   (-mean_absolute_error)\n",
      "\t44.0s\t = Training   runtime\n",
      "\t2.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 4758.29s of the 8759.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.9768\t = Validation score   (-mean_absolute_error)\n",
      "\t111.76s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 4643.3s of the 8644.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.1378\t = Validation score   (-mean_absolute_error)\n",
      "\t1503.47s\t = Training   runtime\n",
      "\t55.63s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3130.74s of the 7132.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.7762\t = Validation score   (-mean_absolute_error)\n",
      "\t214.16s\t = Training   runtime\n",
      "\t1.51s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2912.84s of the 6914.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.2286\t = Validation score   (-mean_absolute_error)\n",
      "\t33.87s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 5089.24s of the 5089.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.8812\t = Validation score   (-mean_absolute_error)\n",
      "\t28.03s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 5057.44s of the 5057.41s of remaining time.\n",
      "\t-19.5787\t = Validation score   (-mean_absolute_error)\n",
      "\t214.14s\t = Training   runtime\n",
      "\t2.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 4839.26s of the 4839.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.2503\t = Validation score   (-mean_absolute_error)\n",
      "\t113.28s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 4722.2s of the 4722.18s of remaining time.\n",
      "\t-19.5003\t = Validation score   (-mean_absolute_error)\n",
      "\t48.48s\t = Training   runtime\n",
      "\t2.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 4669.48s of the 4669.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.4435\t = Validation score   (-mean_absolute_error)\n",
      "\t148.11s\t = Training   runtime\n",
      "\t1.55s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 4351.83s of the 4351.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.983\t = Validation score   (-mean_absolute_error)\n",
      "\t92.28s\t = Training   runtime\n",
      "\t0.71s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4255.81s of the 4255.79s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.088\t = Validation score   (-mean_absolute_error)\n",
      "\t65.85s\t = Training   runtime\n",
      "\t1.31s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4220.52s of the 4220.5s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.7712\t = Validation score   (-mean_absolute_error)\n",
      "\t55.04s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 4190.38s of the 4190.35s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.1391\t = Validation score   (-mean_absolute_error)\n",
      "\t235.82s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 4064.55s of the 4064.52s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.7568\t = Validation score   (-mean_absolute_error)\n",
      "\t222.15s\t = Training   runtime\n",
      "\t2.55s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 3950.25s of the 3950.23s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.6302\t = Validation score   (-mean_absolute_error)\n",
      "\t95.51s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 3898.68s of the 3898.66s of remaining time.\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.7678\t = Validation score   (-mean_absolute_error)\n",
      "\t81.28s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 3606.72s of the 3606.69s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.0781\t = Validation score   (-mean_absolute_error)\n",
      "\t337.09s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 3501.98s of the 3501.96s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.6659\t = Validation score   (-mean_absolute_error)\n",
      "\t333.94s\t = Training   runtime\n",
      "\t3.56s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 3386.79s of the 3386.77s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.6052\t = Validation score   (-mean_absolute_error)\n",
      "\t143.27s\t = Training   runtime\n",
      "\t1.33s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 3335.15s of the 3335.13s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.0408\t = Validation score   (-mean_absolute_error)\n",
      "\t410.04s\t = Training   runtime\n",
      "\t4.88s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3197.9s of the 3197.88s of remaining time.\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.7563\t = Validation score   (-mean_absolute_error)\n",
      "\t282.86s\t = Training   runtime\n",
      "\t2.06s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 3099.02s of the 3099.0s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.002\t = Validation score   (-mean_absolute_error)\n",
      "\t124.38s\t = Training   runtime\n",
      "\t2.41s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3065.18s of the 3065.16s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.7791\t = Validation score   (-mean_absolute_error)\n",
      "\t110.27s\t = Training   runtime\n",
      "\t1.31s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 3032.96s of the 3032.94s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.0729\t = Validation score   (-mean_absolute_error)\n",
      "\t443.39s\t = Training   runtime\n",
      "\t0.65s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2923.23s of the 2923.2s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.6487\t = Validation score   (-mean_absolute_error)\n",
      "\t446.67s\t = Training   runtime\n",
      "\t4.57s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2806.35s of the 2806.33s of remaining time.\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.0478\t = Validation score   (-mean_absolute_error)\n",
      "\t545.84s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2321.51s of the 2321.49s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.615\t = Validation score   (-mean_absolute_error)\n",
      "\t558.34s\t = Training   runtime\n",
      "\t5.38s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2205.14s of the 2205.11s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.5495\t = Validation score   (-mean_absolute_error)\n",
      "\t261.53s\t = Training   runtime\n",
      "\t2.75s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 2151.96s of the 2151.91s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.9663\t = Validation score   (-mean_absolute_error)\n",
      "\t695.82s\t = Training   runtime\n",
      "\t8.48s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1984.34s of the 1984.32s of remaining time.\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.7002\t = Validation score   (-mean_absolute_error)\n",
      "\t475.43s\t = Training   runtime\n",
      "\t3.73s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1880.87s of the 1880.85s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.9381\t = Validation score   (-mean_absolute_error)\n",
      "\t198.71s\t = Training   runtime\n",
      "\t4.49s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1844.0s of the 1843.98s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.754\t = Validation score   (-mean_absolute_error)\n",
      "\t165.75s\t = Training   runtime\n",
      "\t1.9s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1812.62s of the 1812.6s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.0371\t = Validation score   (-mean_absolute_error)\n",
      "\t665.51s\t = Training   runtime\n",
      "\t0.97s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1689.42s of the 1689.4s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.5999\t = Validation score   (-mean_absolute_error)\n",
      "\t671.49s\t = Training   runtime\n",
      "\t6.72s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1572.97s of the 1572.95s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.5304\t = Validation score   (-mean_absolute_error)\n",
      "\t312.0s\t = Training   runtime\n",
      "\t3.3s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1518.27s of the 1518.25s of remaining time.\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.677\t = Validation score   (-mean_absolute_error)\n",
      "\t570.98s\t = Training   runtime\n",
      "\t4.42s\t = Validation runtime\n",
      "Repeating k-fold bagging: 7/20\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1241.3s of the 1241.28s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.9463\t = Validation score   (-mean_absolute_error)\n",
      "\t240.86s\t = Training   runtime\n",
      "\t5.45s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1193.29s of the 1193.27s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.7539\t = Validation score   (-mean_absolute_error)\n",
      "\t193.82s\t = Training   runtime\n",
      "\t2.26s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1161.87s of the 1161.85s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-20.0288\t = Validation score   (-mean_absolute_error)\n",
      "\t769.65s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1054.34s of the 1054.32s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.5899\t = Validation score   (-mean_absolute_error)\n",
      "\t786.44s\t = Training   runtime\n",
      "\t7.83s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 936.0s of the 935.97s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.5157\t = Validation score   (-mean_absolute_error)\n",
      "\t371.32s\t = Training   runtime\n",
      "\t4.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 872.22s of the 872.19s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.9487\t = Validation score   (-mean_absolute_error)\n",
      "\t987.73s\t = Training   runtime\n",
      "\t11.62s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 737.88s of the 737.86s of remaining time.\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-19.6662\t = Validation score   (-mean_absolute_error)\n",
      "\t665.67s\t = Training   runtime\n",
      "\t5.09s\t = Validation runtime\n",
      "Completed 7/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 512.68s of the 639.0s of remaining time.\n",
      "\t-19.2364\t = Validation score   (-mean_absolute_error)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 11361.88s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutgluonModels/C/\")\n"
     ]
    }
   ],
   "source": [
    "model_a2 = build_autogluon(merged_a2,12000,'A')\n",
    "model_b2 = build_autogluon(merged_b2,12000,'B')\n",
    "model_c2 = build_autogluon(merged_c2,12000,'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99b0c06f-57b3-49a4-aadc-92cee3421b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a2 = model_a2.predict(x_test_a2)\n",
    "pred_b2 = model_b2.predict(x_test_b2)\n",
    "pred_c2 = model_c2.predict(x_test_c2)\n",
    "\n",
    "gluon_sub = create_sub(pred_a2,pred_b2,pred_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6def0-1aeb-4438-8145-b5bf53866a39",
   "metadata": {},
   "source": [
    "## Part 3: Blend predictions and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efcd9440-abff-44bb-a6be-730b70f6296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted average of catboost and autogluon model\n",
    "def weighted_avg(sub1,sub2, w1, w2):\n",
    "    merged_df = pd.merge(sub1, sub2, on=['id'])\n",
    "    merged_df['prediction'] = merged_df['prediction_x']*w1 + merged_df['prediction_y']*w2\n",
    "    final_df = merged_df.drop(columns=['prediction_x', 'prediction_y'])\n",
    "    final_df.loc[final_df['prediction'] < 8, 'prediction'] = 0\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f237c751-43f8-4d85-9320-2f9d853b81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1 = weighted_avg(cat_sub,gluon_sub,0.5,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eeac67a-8b4a-4004-b89e-bf68c82bb07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>64.725498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>365.558548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2155</td>\n",
       "      <td>70.646169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2156</td>\n",
       "      <td>40.805214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2157</td>\n",
       "      <td>16.436187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2159</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0        0    0.000000\n",
       "1        1    0.000000\n",
       "2        2    0.000000\n",
       "3        3   64.725498\n",
       "4        4  365.558548\n",
       "...    ...         ...\n",
       "2155  2155   70.646169\n",
       "2156  2156   40.805214\n",
       "2157  2157   16.436187\n",
       "2158  2158    0.000000\n",
       "2159  2159    0.000000\n",
       "\n",
       "[2160 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fc9411f-0b7a-48b5-b9d0-4f6a60342758",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1.to_csv('finalsubmission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
