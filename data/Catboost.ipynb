{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c788fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6686ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a = pd.read_csv('cleaned_data/A/x_train_a.csv')\n",
    "x_train_b = pd.read_csv('cleaned_data/B/x_train_b.csv')\n",
    "x_train_c = pd.read_csv('cleaned_data/C/x_train_c.csv')\n",
    "\n",
    "x_test_a = pd.read_csv('cleaned_data/A/x_test_a.csv')\n",
    "x_test_b = pd.read_csv('cleaned_data/B/x_test_b.csv')\n",
    "x_test_c = pd.read_csv('cleaned_data/C/x_test_c.csv')\n",
    "\n",
    "train_a = pd.read_csv('cleaned_data/A/train_a.csv')\n",
    "train_b = pd.read_csv('cleaned_data/B/train_b.csv')\n",
    "train_c = pd.read_csv('cleaned_data/C/train_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78d43bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a['time'] = pd.to_datetime(train_a['time'])\n",
    "train_b['time'] = pd.to_datetime(train_b['time'])\n",
    "train_c['time'] = pd.to_datetime(train_c['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfd9be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_a = x_test_a.drop(columns = ['date_forecast'])\n",
    "x_test_b = x_test_b.drop(columns = ['date_forecast'])\n",
    "x_test_c = x_test_c.drop(columns = ['date_forecast'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e2e8f2f-66c0-4531-9037-92edb12e193c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "absolute_humidity_2m:gm3          float64\n",
       "air_density_2m:kgm3               float64\n",
       "ceiling_height_agl:m              float64\n",
       "clear_sky_energy_1h:J             float64\n",
       "clear_sky_rad:W                   float64\n",
       "cloud_base_agl:m                  float64\n",
       "dew_or_rime:idx                   float64\n",
       "dew_point_2m:K                    float64\n",
       "diffuse_rad:W                     float64\n",
       "diffuse_rad_1h:J                  float64\n",
       "direct_rad:W                      float64\n",
       "direct_rad_1h:J                   float64\n",
       "effective_cloud_cover:p           float64\n",
       "elevation:m                       float64\n",
       "fresh_snow_12h:cm                 float64\n",
       "fresh_snow_1h:cm                  float64\n",
       "fresh_snow_24h:cm                 float64\n",
       "fresh_snow_3h:cm                  float64\n",
       "fresh_snow_6h:cm                  float64\n",
       "is_day:idx                        float64\n",
       "is_in_shadow:idx                  float64\n",
       "msl_pressure:hPa                  float64\n",
       "precip_5min:mm                    float64\n",
       "precip_type_5min:idx              float64\n",
       "pressure_100m:hPa                 float64\n",
       "pressure_50m:hPa                  float64\n",
       "prob_rime:p                       float64\n",
       "rain_water:kgm2                   float64\n",
       "relative_humidity_1000hPa:p       float64\n",
       "sfc_pressure:hPa                  float64\n",
       "snow_density:kgm3                 float64\n",
       "snow_depth:cm                     float64\n",
       "snow_drift:idx                    float64\n",
       "snow_melt_10min:mm                float64\n",
       "snow_water:kgm2                   float64\n",
       "sun_azimuth:d                     float64\n",
       "sun_elevation:d                   float64\n",
       "super_cooled_liquid_water:kgm2    float64\n",
       "t_1000hPa:K                       float64\n",
       "total_cloud_cover:p               float64\n",
       "visibility:m                      float64\n",
       "wind_speed_10m:ms                 float64\n",
       "wind_speed_u_10m:ms               float64\n",
       "wind_speed_v_10m:ms               float64\n",
       "wind_speed_w_1000hPa:ms           float64\n",
       "year                                int64\n",
       "month                               int64\n",
       "day                                 int64\n",
       "hour                                int64\n",
       "estimated                           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_a.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8d7f5d6-8d69-4852-973d-b44ffe010c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_forecast                      object\n",
       "absolute_humidity_2m:gm3          float64\n",
       "air_density_2m:kgm3               float64\n",
       "ceiling_height_agl:m              float64\n",
       "clear_sky_energy_1h:J             float64\n",
       "clear_sky_rad:W                   float64\n",
       "cloud_base_agl:m                  float64\n",
       "dew_or_rime:idx                   float64\n",
       "dew_point_2m:K                    float64\n",
       "diffuse_rad:W                     float64\n",
       "diffuse_rad_1h:J                  float64\n",
       "direct_rad:W                      float64\n",
       "direct_rad_1h:J                   float64\n",
       "effective_cloud_cover:p           float64\n",
       "elevation:m                       float64\n",
       "fresh_snow_12h:cm                 float64\n",
       "fresh_snow_1h:cm                  float64\n",
       "fresh_snow_24h:cm                 float64\n",
       "fresh_snow_3h:cm                  float64\n",
       "fresh_snow_6h:cm                  float64\n",
       "is_day:idx                        float64\n",
       "is_in_shadow:idx                  float64\n",
       "msl_pressure:hPa                  float64\n",
       "precip_5min:mm                    float64\n",
       "precip_type_5min:idx              float64\n",
       "pressure_100m:hPa                 float64\n",
       "pressure_50m:hPa                  float64\n",
       "prob_rime:p                       float64\n",
       "rain_water:kgm2                   float64\n",
       "relative_humidity_1000hPa:p       float64\n",
       "sfc_pressure:hPa                  float64\n",
       "snow_density:kgm3                 float64\n",
       "snow_depth:cm                     float64\n",
       "snow_drift:idx                    float64\n",
       "snow_melt_10min:mm                float64\n",
       "snow_water:kgm2                   float64\n",
       "sun_azimuth:d                     float64\n",
       "sun_elevation:d                   float64\n",
       "super_cooled_liquid_water:kgm2    float64\n",
       "t_1000hPa:K                       float64\n",
       "total_cloud_cover:p               float64\n",
       "visibility:m                      float64\n",
       "wind_speed_10m:ms                 float64\n",
       "wind_speed_u_10m:ms               float64\n",
       "wind_speed_v_10m:ms               float64\n",
       "wind_speed_w_1000hPa:ms           float64\n",
       "year                                int64\n",
       "month                               int64\n",
       "day                                 int64\n",
       "hour                                int64\n",
       "estimated                           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_a.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1a15d",
   "metadata": {},
   "source": [
    "# Analysis of Target variable  - Looking at PV_measurement\n",
    "1. Handle constant measurments over longer periods of time. Likely caused by sensor malfunction, data logging issues, or other external factors.\n",
    "    - Handeled by removing all constant values lasting more than 24 hours \n",
    "2. Add cyclical features \n",
    "2. Handle longer periods of missing data:\n",
    "    - Remove (currently tested)\n",
    "    - Interpolate \n",
    "    - Copy from previous year\n",
    "    - Copy solar production at missing time from another location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89446f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Handle constant PV measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5db23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series plot of PV_measurement \n",
    "\n",
    "def solar_prod_plot(y_train, resolution='year', chunks=5):\n",
    "    df = y_train.copy()\n",
    "    \n",
    "    # Determine the plotting resolution based on the 'resolution' argument\n",
    "    # Chunks = number of year/months/days in each plot\n",
    "    if resolution == 'year':\n",
    "        unique_values = df['time'].dt.year.unique()\n",
    "        label = 'Year'\n",
    "    elif resolution == 'month':\n",
    "        df['year_month'] = df['time'].dt.to_period('M')\n",
    "        unique_values = df['year_month'].unique()\n",
    "        label = 'Month'\n",
    "    elif resolution == 'week':\n",
    "        df['year_week'] = df['time'].dt.to_period('W')\n",
    "        unique_values = df['year_week'].unique()\n",
    "        label = 'Week'\n",
    "    elif resolution == 'day':\n",
    "        df['date'] = df['time'].dt.date\n",
    "        unique_values = df['date'].unique()\n",
    "        label = 'Day'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid resolution. Choose from 'year', 'month', 'week', or 'day'.\")\n",
    "    \n",
    "    # Loop over the unique values in chunks\n",
    "    for i in range(0, len(unique_values), chunks):\n",
    "        subset_values = unique_values[i:i+chunks]\n",
    "        \n",
    "        if resolution == 'year':\n",
    "            subset_df = df[df['time'].dt.year.isin(subset_values)]\n",
    "        elif resolution == 'month':\n",
    "            subset_df = df[df['year_month'].isin(subset_values)]\n",
    "        elif resolution == 'week':\n",
    "            subset_df = df[df['year_week'].isin(subset_values)]\n",
    "        elif resolution == 'day':\n",
    "            subset_df = df[df['date'].isin(subset_values)]\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(subset_df['time'], subset_df['pv_measurement'])\n",
    "\n",
    "        title = f\"Solar Power Production for {label}: {subset_values[0]}\"\n",
    "        if len(subset_values) > 1:\n",
    "            title += f\" to {subset_values[-1]}\"\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"PV Measurement\")\n",
    "        plt.show()\n",
    "\n",
    "def remove_constant_intervals(y_train, low_thresh, upp_thresh):\n",
    "    \"\"\"\n",
    "    Identify and remove intervals of constant PV readings that exceed a specified duration. \n",
    "    Constant readings may indicate sensor malfunctions or data logging issues.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    y_train : pd.DataFrame\n",
    "        Dataframe containing the time-series data of solar power production.\n",
    "    threshold : int\n",
    "        The minimum duration required for an interval to be considered for removal.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The input dataframe with intervals of constant readings (exceeding the duration threshold) removed.\n",
    "    \"\"\"\n",
    "    df = y_train.copy()\n",
    "    \n",
    "    # Calculate the difference in production values\n",
    "    df['diff'] = df['pv_measurement'].diff()\n",
    "\n",
    "    # Identify where the difference is zero\n",
    "    df['zero_diff'] = df['diff'].abs() < 1e-5\n",
    "\n",
    "    # Identify groups of consecutive zero differences\n",
    "    df['group'] = (df['zero_diff'] != df['zero_diff'].shift()).cumsum()\n",
    "\n",
    "    # Filter out only the groups with consecutive zero differences\n",
    "    constant_intervals = df[df['zero_diff']].groupby('group').agg(start=('time', 'min'), \n",
    "                                                                  end=('time', 'max'),\n",
    "                                                                  duration=('time', 'size'))\n",
    "    \n",
    "    # Filter intervals based on the threshold\n",
    "    interval_df_thresh = constant_intervals[(constant_intervals['duration'] > low_thresh) & (constant_intervals['duration'] <upp_thresh)]\n",
    "    \n",
    "    # Remove rows from the main dataframe that fall within these intervals\n",
    "    for _, row in interval_df_thresh.iterrows():\n",
    "        start_time, end_time = row['start'], row['end']\n",
    "        df = df[(df['time'] < start_time) | (df['time'] > end_time)]\n",
    "    \n",
    "    # Drop the added columns used for calculations\n",
    "    df.drop(columns=['diff', 'zero_diff', 'group'], inplace=True)\n",
    "    \n",
    "    return df, constant_intervals\n",
    "\n",
    "\n",
    "def get_time_interval(df, start_time = '2020-08-01 00:00:00', end_time = '2021-01-01 00:00:00'):\n",
    "    # Filter rows based on the time period\n",
    "    filtered_df = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e1af31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removed all constant values with duration > 24 hours\n",
    "\n",
    "train_a, const_interval_a = remove_constant_intervals(train_a,24,10**6)\n",
    "\n",
    "#update X_train_a by removing coresponding rows that have been filtered here\n",
    "x_train_a, train_a = align_X_y(x_train_a, train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77a90e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows removed 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>2020-01-04 15:00:00</td>\n",
       "      <td>2020-01-06 08:00:00</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    start                 end  duration\n",
       "group                                                  \n",
       "434   2020-01-04 15:00:00 2020-01-06 08:00:00        42"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_removed_a = np.sum(const_interval_a[const_interval_a['duration']>24]['duration'])\n",
    "print(f'total number of rows removed {rows_removed_a}')\n",
    "const_interval_a[const_interval_a['duration']>24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2d4bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in groups of constant values, where duration of constant measurements is > 1 day (24 hours)\n",
    "train_b, const_interval_b = remove_constant_intervals(train_b,24,10**6)\n",
    "\n",
    "#update X_train_a by removing coresponding rows that have been filtered here\n",
    "x_train_b, train_b = align_X_y(x_train_b, train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dd0cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows removed 6865\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019-01-14 15:00:00</td>\n",
       "      <td>2019-01-18 11:00:00</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019-01-19 13:00:00</td>\n",
       "      <td>2019-01-26 08:00:00</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2019-01-27 11:00:00</td>\n",
       "      <td>2019-01-28 13:00:00</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2019-02-10 16:00:00</td>\n",
       "      <td>2019-02-13 07:00:00</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2019-03-23 18:00:00</td>\n",
       "      <td>2019-03-26 06:00:00</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>2019-05-31 08:00:00</td>\n",
       "      <td>2019-06-03 12:00:00</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2019-10-28 14:00:00</td>\n",
       "      <td>2019-10-30 22:00:00</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>2019-12-01 13:00:00</td>\n",
       "      <td>2019-12-04 08:00:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>2019-12-07 14:00:00</td>\n",
       "      <td>2019-12-11 08:00:00</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2019-12-18 14:00:00</td>\n",
       "      <td>2019-12-20 09:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>2019-12-25 14:00:00</td>\n",
       "      <td>2019-12-30 09:00:00</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>2020-01-02 14:00:00</td>\n",
       "      <td>2020-01-04 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>2020-01-04 14:00:00</td>\n",
       "      <td>2020-01-06 10:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>2020-01-24 12:00:00</td>\n",
       "      <td>2020-01-26 08:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>2020-02-05 14:00:00</td>\n",
       "      <td>2020-02-07 09:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>2020-02-23 17:00:00</td>\n",
       "      <td>2020-02-25 09:00:00</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>2020-03-26 14:00:00</td>\n",
       "      <td>2020-03-27 21:00:00</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>2020-04-02 02:00:00</td>\n",
       "      <td>2020-04-16 06:00:00</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2020-07-12 21:00:00</td>\n",
       "      <td>2020-08-25 21:00:00</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>2020-09-24 13:00:00</td>\n",
       "      <td>2020-09-25 21:00:00</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>2020-12-16 14:00:00</td>\n",
       "      <td>2020-12-18 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>2020-12-26 14:00:00</td>\n",
       "      <td>2020-12-28 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>2021-01-09 14:00:00</td>\n",
       "      <td>2021-01-13 09:00:00</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>2021-01-19 13:00:00</td>\n",
       "      <td>2021-01-21 09:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>2021-01-22 16:00:00</td>\n",
       "      <td>2021-01-24 08:00:00</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>2021-01-28 16:00:00</td>\n",
       "      <td>2021-01-30 08:00:00</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>2021-01-30 14:00:00</td>\n",
       "      <td>2021-02-01 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>2021-02-01 11:00:00</td>\n",
       "      <td>2021-02-03 08:00:00</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2021-02-18 00:00:00</td>\n",
       "      <td>2021-03-08 14:00:00</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2021-03-08 16:00:00</td>\n",
       "      <td>2021-04-19 11:00:00</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>2021-04-28 23:00:00</td>\n",
       "      <td>2021-05-01 21:00:00</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>2021-06-05 02:00:00</td>\n",
       "      <td>2021-06-07 07:00:00</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2021-06-13 02:00:00</td>\n",
       "      <td>2021-06-14 09:00:00</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>2021-06-22 02:00:00</td>\n",
       "      <td>2021-06-24 08:00:00</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>2021-07-03 13:00:00</td>\n",
       "      <td>2021-07-06 05:00:00</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>2021-08-25 23:00:00</td>\n",
       "      <td>2021-09-03 21:00:00</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>2021-09-08 13:00:00</td>\n",
       "      <td>2021-09-14 13:00:00</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>2021-09-19 00:00:00</td>\n",
       "      <td>2021-09-27 08:00:00</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>2021-11-22 15:00:00</td>\n",
       "      <td>2021-11-24 08:00:00</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>2021-11-26 12:00:00</td>\n",
       "      <td>2021-12-04 08:00:00</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>2021-12-16 14:00:00</td>\n",
       "      <td>2021-12-18 09:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>2021-12-21 14:00:00</td>\n",
       "      <td>2021-12-24 09:00:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>2021-12-24 12:00:00</td>\n",
       "      <td>2022-01-03 09:00:00</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>2022-01-03 13:00:00</td>\n",
       "      <td>2022-01-11 09:00:00</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>2022-01-12 14:00:00</td>\n",
       "      <td>2022-01-14 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>2022-01-30 16:00:00</td>\n",
       "      <td>2022-02-04 09:00:00</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2022-02-10 15:00:00</td>\n",
       "      <td>2022-02-12 11:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>2022-02-14 16:00:00</td>\n",
       "      <td>2022-02-16 09:00:00</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>2022-02-16 14:00:00</td>\n",
       "      <td>2022-02-18 10:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>2022-02-19 10:00:00</td>\n",
       "      <td>2022-02-24 06:00:00</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2022-03-06 11:00:00</td>\n",
       "      <td>2022-03-07 11:00:00</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2022-03-19 14:00:00</td>\n",
       "      <td>2022-03-28 07:00:00</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>2022-03-28 12:00:00</td>\n",
       "      <td>2022-04-05 06:00:00</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>2023-01-15 15:00:00</td>\n",
       "      <td>2023-01-17 09:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    start                 end  duration\n",
       "group                                                  \n",
       "32    2019-01-14 15:00:00 2019-01-18 11:00:00        93\n",
       "36    2019-01-19 13:00:00 2019-01-26 08:00:00       164\n",
       "40    2019-01-27 11:00:00 2019-01-28 13:00:00        27\n",
       "74    2019-02-10 16:00:00 2019-02-13 07:00:00        64\n",
       "160   2019-03-23 18:00:00 2019-03-26 06:00:00        61\n",
       "302   2019-05-31 08:00:00 2019-06-03 12:00:00        77\n",
       "606   2019-10-28 14:00:00 2019-10-30 22:00:00        57\n",
       "674   2019-12-01 13:00:00 2019-12-04 08:00:00        68\n",
       "682   2019-12-07 14:00:00 2019-12-11 08:00:00        91\n",
       "700   2019-12-18 14:00:00 2019-12-20 09:00:00        44\n",
       "712   2019-12-25 14:00:00 2019-12-30 09:00:00       116\n",
       "724   2020-01-02 14:00:00 2020-01-04 08:00:00        43\n",
       "726   2020-01-04 14:00:00 2020-01-06 10:00:00        45\n",
       "768   2020-01-24 12:00:00 2020-01-26 08:00:00        45\n",
       "790   2020-02-05 14:00:00 2020-02-07 09:00:00        44\n",
       "824   2020-02-23 17:00:00 2020-02-25 09:00:00        41\n",
       "890   2020-03-26 14:00:00 2020-03-27 21:00:00        32\n",
       "906   2020-04-02 02:00:00 2020-04-16 06:00:00       341\n",
       "1090  2020-07-12 21:00:00 2020-08-25 21:00:00      1057\n",
       "1154  2020-09-24 13:00:00 2020-09-25 21:00:00        33\n",
       "1332  2020-12-16 14:00:00 2020-12-18 08:00:00        43\n",
       "1352  2020-12-26 14:00:00 2020-12-28 08:00:00        43\n",
       "1380  2021-01-09 14:00:00 2021-01-13 09:00:00        92\n",
       "1396  2021-01-19 13:00:00 2021-01-21 09:00:00        45\n",
       "1400  2021-01-22 16:00:00 2021-01-24 08:00:00        41\n",
       "1410  2021-01-28 16:00:00 2021-01-30 08:00:00        41\n",
       "1414  2021-01-30 14:00:00 2021-02-01 08:00:00        43\n",
       "1416  2021-02-01 11:00:00 2021-02-03 08:00:00        46\n",
       "1454  2021-02-18 00:00:00 2021-03-08 14:00:00       447\n",
       "1456  2021-03-08 16:00:00 2021-04-19 11:00:00      1003\n",
       "1478  2021-04-28 23:00:00 2021-05-01 21:00:00        71\n",
       "1550  2021-06-05 02:00:00 2021-06-07 07:00:00        54\n",
       "1564  2021-06-13 02:00:00 2021-06-14 09:00:00        32\n",
       "1582  2021-06-22 02:00:00 2021-06-24 08:00:00        55\n",
       "1602  2021-07-03 13:00:00 2021-07-06 05:00:00        65\n",
       "1710  2021-08-25 23:00:00 2021-09-03 21:00:00       215\n",
       "1722  2021-09-08 13:00:00 2021-09-14 13:00:00       145\n",
       "1734  2021-09-19 00:00:00 2021-09-27 08:00:00       201\n",
       "1858  2021-11-22 15:00:00 2021-11-24 08:00:00        42\n",
       "1864  2021-11-26 12:00:00 2021-12-04 08:00:00       189\n",
       "1894  2021-12-16 14:00:00 2021-12-18 09:00:00        44\n",
       "1902  2021-12-21 14:00:00 2021-12-24 09:00:00        68\n",
       "1904  2021-12-24 12:00:00 2022-01-03 09:00:00       238\n",
       "1906  2022-01-03 13:00:00 2022-01-11 09:00:00       189\n",
       "1910  2022-01-12 14:00:00 2022-01-14 08:00:00        43\n",
       "1948  2022-01-30 16:00:00 2022-02-04 09:00:00       114\n",
       "1966  2022-02-10 15:00:00 2022-02-12 11:00:00        45\n",
       "1972  2022-02-14 16:00:00 2022-02-16 09:00:00        42\n",
       "1974  2022-02-16 14:00:00 2022-02-18 10:00:00        45\n",
       "1978  2022-02-19 10:00:00 2022-02-24 06:00:00       117\n",
       "2004  2022-03-06 11:00:00 2022-03-07 11:00:00        25\n",
       "2032  2022-03-19 14:00:00 2022-03-28 07:00:00       209\n",
       "2034  2022-03-28 12:00:00 2022-04-05 06:00:00       187\n",
       "2196  2023-01-15 15:00:00 2023-01-17 09:00:00        43"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_removed = np.sum(const_interval_b[const_interval_b['duration']>24]['duration'])\n",
    "print(f'total number of rows removed {rows_removed}')\n",
    "const_interval_b[const_interval_b['duration']>24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d09483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in groups of constant values, where duration of constant measurements is > 1 day (24 hours)\n",
    "train_c, const_interval_c = remove_constant_intervals(train_c,24,10**6)\n",
    "\n",
    "#update X_train_a by removing coresponding rows that have been filtered here\n",
    "x_train_c, train_c = align_X_y(x_train_c, train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c619c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows removed 4926\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-09-04 10:00:00</td>\n",
       "      <td>2019-09-05 12:00:00</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2019-11-11 12:00:00</td>\n",
       "      <td>2019-11-13 08:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2019-11-28 15:00:00</td>\n",
       "      <td>2019-12-05 09:00:00</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2019-12-07 14:00:00</td>\n",
       "      <td>2019-12-13 09:00:00</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2019-12-16 14:00:00</td>\n",
       "      <td>2019-12-21 09:00:00</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2019-12-25 13:00:00</td>\n",
       "      <td>2019-12-30 09:00:00</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2020-01-02 14:00:00</td>\n",
       "      <td>2020-01-07 09:00:00</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2020-01-23 15:00:00</td>\n",
       "      <td>2020-01-26 08:00:00</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>2020-02-05 14:00:00</td>\n",
       "      <td>2020-02-10 07:00:00</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>2020-02-23 17:00:00</td>\n",
       "      <td>2020-03-08 08:00:00</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2020-03-28 18:00:00</td>\n",
       "      <td>2020-03-31 09:00:00</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>2020-11-18 13:00:00</td>\n",
       "      <td>2020-11-22 08:00:00</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>2020-12-16 14:00:00</td>\n",
       "      <td>2020-12-18 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>2020-12-21 14:00:00</td>\n",
       "      <td>2020-12-23 09:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>2020-12-25 14:00:00</td>\n",
       "      <td>2020-12-28 09:00:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>2021-01-09 14:00:00</td>\n",
       "      <td>2021-01-22 10:00:00</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>2021-01-22 15:00:00</td>\n",
       "      <td>2021-01-24 10:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>2021-01-24 13:00:00</td>\n",
       "      <td>2021-02-19 10:00:00</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>2021-03-03 17:00:00</td>\n",
       "      <td>2021-03-06 07:00:00</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>2021-03-08 14:00:00</td>\n",
       "      <td>2021-03-10 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>2021-03-20 18:00:00</td>\n",
       "      <td>2021-03-22 05:00:00</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2021-04-09 19:00:00</td>\n",
       "      <td>2021-04-11 08:00:00</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>2021-11-24 14:00:00</td>\n",
       "      <td>2021-12-14 09:00:00</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>2021-12-21 14:00:00</td>\n",
       "      <td>2022-01-16 10:00:00</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>2022-01-16 13:00:00</td>\n",
       "      <td>2022-01-18 10:00:00</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2022-01-19 14:00:00</td>\n",
       "      <td>2022-01-22 09:00:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>2022-01-24 16:00:00</td>\n",
       "      <td>2022-01-26 10:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>2022-01-27 16:00:00</td>\n",
       "      <td>2022-01-30 08:00:00</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>2022-01-30 15:00:00</td>\n",
       "      <td>2022-02-07 11:00:00</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>2022-02-08 14:00:00</td>\n",
       "      <td>2022-03-02 09:00:00</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>2022-04-02 18:00:00</td>\n",
       "      <td>2022-04-04 09:00:00</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>2022-04-05 13:00:00</td>\n",
       "      <td>2022-04-08 09:00:00</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>2023-02-19 14:00:00</td>\n",
       "      <td>2023-02-21 10:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>2023-02-21 16:00:00</td>\n",
       "      <td>2023-02-23 09:00:00</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    start                 end  duration\n",
       "group                                                  \n",
       "2     2019-09-04 10:00:00 2019-09-05 12:00:00        27\n",
       "180   2019-11-11 12:00:00 2019-11-13 08:00:00        45\n",
       "230   2019-11-28 15:00:00 2019-12-05 09:00:00       163\n",
       "240   2019-12-07 14:00:00 2019-12-13 09:00:00       140\n",
       "256   2019-12-16 14:00:00 2019-12-21 09:00:00       116\n",
       "276   2019-12-25 13:00:00 2019-12-30 09:00:00       117\n",
       "290   2020-01-02 14:00:00 2020-01-07 09:00:00       116\n",
       "340   2020-01-23 15:00:00 2020-01-26 08:00:00        66\n",
       "376   2020-02-05 14:00:00 2020-02-10 07:00:00       114\n",
       "414   2020-02-23 17:00:00 2020-03-08 08:00:00       328\n",
       "484   2020-03-28 18:00:00 2020-03-31 09:00:00        64\n",
       "1150  2020-11-18 13:00:00 2020-11-22 08:00:00        92\n",
       "1238  2020-12-16 14:00:00 2020-12-18 08:00:00        43\n",
       "1252  2020-12-21 14:00:00 2020-12-23 09:00:00        44\n",
       "1264  2020-12-25 14:00:00 2020-12-28 09:00:00        68\n",
       "1312  2021-01-09 14:00:00 2021-01-22 10:00:00       309\n",
       "1316  2021-01-22 15:00:00 2021-01-24 10:00:00        44\n",
       "1318  2021-01-24 13:00:00 2021-02-19 10:00:00       622\n",
       "1358  2021-03-03 17:00:00 2021-03-06 07:00:00        63\n",
       "1374  2021-03-08 14:00:00 2021-03-10 08:00:00        43\n",
       "1408  2021-03-20 18:00:00 2021-03-22 05:00:00        36\n",
       "1458  2021-04-09 19:00:00 2021-04-11 08:00:00        38\n",
       "2122  2021-11-24 14:00:00 2021-12-14 09:00:00       476\n",
       "2146  2021-12-21 14:00:00 2022-01-16 10:00:00       621\n",
       "2148  2022-01-16 13:00:00 2022-01-18 10:00:00        46\n",
       "2152  2022-01-19 14:00:00 2022-01-22 09:00:00        68\n",
       "2164  2022-01-24 16:00:00 2022-01-26 10:00:00        43\n",
       "2168  2022-01-27 16:00:00 2022-01-30 08:00:00        65\n",
       "2172  2022-01-30 15:00:00 2022-02-07 11:00:00       189\n",
       "2178  2022-02-08 14:00:00 2022-03-02 09:00:00       524\n",
       "2284  2022-04-02 18:00:00 2022-04-04 09:00:00        40\n",
       "2290  2022-04-05 13:00:00 2022-04-08 09:00:00        69\n",
       "2486  2023-02-19 14:00:00 2023-02-21 10:00:00        45\n",
       "2490  2023-02-21 16:00:00 2023-02-23 09:00:00        42"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_removed = np.sum(const_interval_c[const_interval_c['duration']>24]['duration'])\n",
    "print(f'total number of rows removed {rows_removed}')\n",
    "const_interval_c[const_interval_c['duration']>24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295598a-c357-4143-898f-f57bc361c20f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merge x_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7084bebc-37bb-4a70-afd6-5d22049f2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_a = pd.merge(x_train_a, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_b = pd.merge(x_train_b, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_c = pd.merge(x_train_c, train_c, left_on='date_forecast', right_on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df46b11c-5cb0-445c-aa2c-b16a28f93dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are plotting on the modified dataset\n",
    "def time_series_plot(feature,merged_data):\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Solar Power Production', color='tab:blue')\n",
    "    ax1.plot(merged_data['time'], merged_data['pv_measurement'], color='tab:blue', label='Solar Power Production')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    ax2.set_ylabel(feature, color='tab:red')  \n",
    "    ax2.plot(merged_data['date_forecast'], merged_data[feature], color='tab:red', label=feature)\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Time Series Plot of Solar Power Production and {feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c9108-c9fe-423c-a2d3-249b198af636",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Add avg pv_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25519486-5b0a-40f0-9522-9c8bd35de95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_average_pv_feature(merged_df, test_df,time_group):\n",
    "    df = merged_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    # Group by year, month, date, and hour and calculate the mean PV measurement\n",
    "    average_pv = df.groupby(time_group)['pv_measurement'].mean().reset_index()\n",
    "    average_pv = average_pv.rename(columns={'pv_measurement': 'average_pv_measurement'})\n",
    "\n",
    "    # Print for debugging\n",
    "\n",
    "\n",
    "    # Merge the average PV measurements back into the original dataframe\n",
    "    df = pd.merge(df, average_pv, on=time_group, how='left')\n",
    "    test_df = pd.merge(test_df, average_pv, on= time_group, how='left')\n",
    "    \n",
    "    # Print for debugging\n",
    "    return df,test_df\n",
    "\n",
    "\"\"\"\n",
    "time_group_1 = ['month', 'day', 'hour']\n",
    "time_group_2 = 'week'\n",
    "time_group_3 = 'month'\n",
    "\n",
    "merged_a_avg, x_test_a_avg = add_average_pv_feature(merged_a,x_test_a,time_group_1)\n",
    "merged_b_avg, x_test_b_avg = add_average_pv_feature(merged_b,x_test_b, time_group_1)\n",
    "merged_c_avg, x_test_c_avg = add_average_pv_feature(merged_c,x_test_c,time_group_1)\n",
    "\n",
    "merged_a_avg[(merged_a_avg['month']==6) & (merged_a_avg['day']==4) & (merged_a_avg['hour']==16)][['year','month','week','day','hour','pv_measurement','average_pv_measurement']]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcdb7f-cabb-4695-90ca-ab8a5a283e42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Add lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0aae5e-3b81-426d-a165-06f346b3e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_feature(data, lag_hours, column_name='pv_measurement'):\n",
    "    \"\"\"\n",
    "    Add lag features to the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The original dataset.\n",
    "    lag_hours (int): The number of hours to lag.\n",
    "    column_name (str): The name of the column to create the lag feature for.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataset with the new lag feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the lag feature\n",
    "    df = data.copy()\n",
    "    lag_feature_name = f\"{column_name}_lag_{lag_hours}h\"\n",
    "    df[lag_feature_name] = df[column_name].shift(lag_hours)\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "laged_a = add_lag_feature(merged_a,24)\n",
    "laged_b = add_lag_feature(merged_b,24)\n",
    "laged_c = add_lag_feature(merged_c,24)\n",
    "\n",
    "x_test_a_laged = x_test_a.copy()\n",
    "x_test_b_laged = x_test_b.copy()\n",
    "x_test_c_laged = x_test_c.copy()\n",
    "\n",
    "\n",
    "# You can add an empty column for the lag feature in your test set:\n",
    "x_test_a_laged[f'pv_measurement_lag_{1}h'] = None\n",
    "x_test_b_laged[f'pv_measurement_lag_{1}h'] = None\n",
    "x_test_c_laged[f'pv_measurement_lag_{1}h'] = None\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b9338-8fa4-41ed-bd50-584aafeb6421",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Handle NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a772e60a-8d9d-46cb-abb2-3ad372d63760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "snow_density:kgm3                 21110\n",
       "ceiling_height_agl:m               4421\n",
       "cloud_base_agl:m                   1886\n",
       "pressure_50m:hPa                     24\n",
       "sun_azimuth:d                        24\n",
       "prob_rime:p                          24\n",
       "rain_water:kgm2                      24\n",
       "relative_humidity_1000hPa:p          24\n",
       "sfc_pressure:hPa                     24\n",
       "snow_depth:cm                        24\n",
       "snow_drift:idx                       24\n",
       "snow_melt_10min:mm                   24\n",
       "snow_water:kgm2                      24\n",
       "sun_elevation:d                      24\n",
       "pressure_100m:hPa                    24\n",
       "super_cooled_liquid_water:kgm2       24\n",
       "t_1000hPa:K                          24\n",
       "total_cloud_cover:p                  24\n",
       "visibility:m                         24\n",
       "wind_speed_10m:ms                    24\n",
       "wind_speed_u_10m:ms                  24\n",
       "wind_speed_v_10m:ms                  24\n",
       "wind_speed_w_1000hPa:ms              24\n",
       "absolute_humidity_2m:gm3             24\n",
       "precip_type_5min:idx                 24\n",
       "effective_cloud_cover:p              24\n",
       "precip_5min:mm                       24\n",
       "clear_sky_energy_1h:J                24\n",
       "clear_sky_rad:W                      24\n",
       "dew_or_rime:idx                      24\n",
       "dew_point_2m:K                       24\n",
       "diffuse_rad:W                        24\n",
       "diffuse_rad_1h:J                     24\n",
       "direct_rad:W                         24\n",
       "direct_rad_1h:J                      24\n",
       "elevation:m                          24\n",
       "fresh_snow_12h:cm                    24\n",
       "fresh_snow_1h:cm                     24\n",
       "fresh_snow_24h:cm                    24\n",
       "fresh_snow_3h:cm                     24\n",
       "fresh_snow_6h:cm                     24\n",
       "is_day:idx                           24\n",
       "is_in_shadow:idx                     24\n",
       "msl_pressure:hPa                     24\n",
       "air_density_2m:kgm3                  24\n",
       "time                                  0\n",
       "hour                                  0\n",
       "estimated                             0\n",
       "date_forecast                         0\n",
       "day                                   0\n",
       "month                                 0\n",
       "year                                  0\n",
       "pv_measurement                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_a.isna().sum().sort_values(ascending = False)\n",
    "merged_b.isna().sum().sort_values(ascending = False)\n",
    "merged_c.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "168ddcd0-b2a8-4871-a769-b49b8430e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(merged_data):\n",
    "    df = merged_data.copy()\n",
    "    \n",
    "    nan_cols = ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3',\n",
    "       'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W',\n",
    "       'cloud_base_agl:m', 'dew_or_rime:idx', 'dew_point_2m:K',\n",
    "       'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J',\n",
    "       'effective_cloud_cover:p', 'elevation:m', 'fresh_snow_12h:cm',\n",
    "       'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm',\n",
    "       'fresh_snow_6h:cm', 'is_day:idx', 'is_in_shadow:idx',\n",
    "       'msl_pressure:hPa', 'precip_5min:mm', 'precip_type_5min:idx',\n",
    "       'pressure_100m:hPa', 'pressure_50m:hPa', 'prob_rime:p',\n",
    "       'rain_water:kgm2', 'relative_humidity_1000hPa:p', 'sfc_pressure:hPa',\n",
    "       'snow_density:kgm3', 'snow_depth:cm', 'snow_drift:idx',\n",
    "       'snow_melt_10min:mm', 'snow_water:kgm2', 'sun_azimuth:d',\n",
    "       'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K',\n",
    "       'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms',\n",
    "       'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms']\n",
    "    \n",
    "    nan_rows_mask = merged_data.loc[:,nan_cols].isna().all(axis=1)\n",
    "    df = df.drop(df[nan_rows_mask].index, inplace=False)\n",
    "    \n",
    "    df = df.drop(columns = ['snow_density:kgm3']) #Tried also to remove 'cloud_base_agl:m' and ceiling_height_agl:m \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d40687cf-baef-4e0b-87ba-a131726a4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_a = remove_nan(merged_a)\n",
    "merged_b = remove_nan(merged_b)\n",
    "merged_c = remove_nan(merged_c)\n",
    "\n",
    "x_test_a = remove_nan(x_test_a)\n",
    "x_test_b = remove_nan(x_test_b)\n",
    "x_test_c = remove_nan(x_test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d8647",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add Cyclical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aea1031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cyclical features for hour of the day\n",
    "def add_cyclic(merged_df):\n",
    "    train_data = merged_df.copy()\n",
    "   \n",
    "    train_data['hour_sin'] = np.sin(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['hour_cos'] = np.cos(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['month_sin'] = np.sin(2 * np.pi * (train_data['month']-1) / 12)\n",
    "    train_data['month_cos'] = np.cos(2 * np.pi * (train_data['month']-1) / 12)\n",
    "    \n",
    "    #train_data.drop(columns = ['hour','month'],inplace = True)\n",
    "    return train_data\n",
    "\n",
    "merged_a = add_cyclic(merged_a)\n",
    "merged_b = add_cyclic(merged_b)\n",
    "merged_c = add_cyclic(merged_c)\n",
    "\n",
    "x_test_a = add_cyclic(x_test_a)\n",
    "x_test_b = add_cyclic(x_test_b)\n",
    "x_test_c = add_cyclic(x_test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab20ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove outliers during night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_avg(y_train):\n",
    "    # Grouping by hour and calculating the average PV measurement for each hour\n",
    "    train_data = y_train.copy()\n",
    "    train_data['hour'] = y_train['time'].dt.hour\n",
    "    hourly_avg = train_data.groupby('hour')['pv_measurement'].mean()\n",
    "\n",
    "    # Plotting the average PV production for each hour\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    hourly_avg.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Average PV Production by Hour')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Average PV Production')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_dist_hour(y_train, hour):\n",
    "    train_data = y_train.copy()\n",
    "    train_data['hour'] = y_train['time'].dt.hour\n",
    "    \n",
    "    # Filtering the data for the given hour\n",
    "    hour_data = train_data[train_data['hour'] == hour]\n",
    "    \n",
    "    # Plotting the distribution of PV measurements for 1 am\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(hour_data['pv_measurement'], bins=50, color='teal', alpha=0.7)\n",
    "    plt.title(f'Distribution of PV Measurements at {hour}')\n",
    "    plt.xlabel('PV Measurement')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(hour_data['pv_measurement'].value_counts())\n",
    "#train_c[(train_c['time'].dt.hour == 2) &(train_c['pv_measurement'] == 9.8)]\n",
    "\n",
    "def get_nighttime_stats(y_train,night_start,night_end):\n",
    "    train_data = y_train.copy()\n",
    "    train_data['hour'] = y_train['time'].dt.hour\n",
    "\n",
    "    # Filtering the data for nighttime hours (8 pm to 4 am)\n",
    "    nighttime_data = train_data[(train_data['hour'] >= night_start) | (train_data['hour'] <= night_end)]\n",
    "\n",
    "    # Descriptive statistics for nighttime PV measurements\n",
    "    nighttime_stats = nighttime_data['pv_measurement'].describe()\n",
    "\n",
    "    # Plotting the distribution of nighttime PV measurements\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(nighttime_data['pv_measurement'], bins=50, color='purple', alpha=0.7)\n",
    "    plt.axvline(nighttime_stats['75%'], color='red', linestyle='dashed', label='75th Percentile')\n",
    "    plt.axvline(nighttime_stats['max'], color='green', linestyle='dashed', label='Max Value')\n",
    "    plt.title('Distribution of Nighttime PV Measurements')\n",
    "    plt.xlabel('PV Measurement')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(nighttime_stats)\n",
    "    \n",
    "def set_nighttime_to_zero(y_train, night_start,night_end, thresh):\n",
    "    df = y_train.copy()\n",
    "    df['hour'] = y_train['time'].dt.hour\n",
    "    mask = (df['hour'] >= 23) | (df['hour'] <= 3) & (df['pv_measurement'] > thresh)\n",
    "    df.loc[mask, 'pv_measurement'] = 0\n",
    "    df = df.drop(columns = ['hour'])\n",
    "    return df\n",
    "\n",
    "#train_a[(train_a['time'].dt.hour == 2) &(train_a['pv_measurement'] >0)]\n",
    "#train_a = set_nighttime_to_zero(train_a,23,3,0)\n",
    "#train_b = set_nighttime_to_zero(train_b,23,3,0)\n",
    "#train_c = set_nighttime_to_zero(train_c,23,3,0)\n",
    "#train_a[(train_a['time'].dt.hour == 2) &(train_a['pv_measurement'] >0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fdf4a-51e1-41af-9e08-bd12e50f903a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove rows with high rad values and zero PV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ec7f2-9c3f-409b-b024-0f075b1ba9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rad_null(merged_df):\n",
    "    merged_data = merged_df.copy()\n",
    "    merged_data['clear_sky_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['clear_sky_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['direct_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['direct_rad_1h:J'].fillna(0, inplace=True)\n",
    "    return merged_data\n",
    "\"\"\"\n",
    "m_a = remove_rad_null(merged_a)\n",
    "m_b = remove_rad_null(merged_b)\n",
    "m_c = remove_rad_null(merged_c)\n",
    "\"\"\"\n",
    "\n",
    "def get_percentiles_df(merged_df):\n",
    "    merged_data = merged_df.copy()\n",
    "    merged_data['clear_sky_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['clear_sky_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['direct_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['direct_rad_1h:J'].fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate and display percentiles\n",
    "    percentiles = [50,60,70,80,85,90,95]\n",
    "    percentile_values_direct_rad= np.percentile(merged_data['direct_rad:W'], percentiles)\n",
    "    percentile_values_direct_rad_1h = np.percentile(merged_data['direct_rad_1h:J'], percentiles)\n",
    "    percentile_values_clear_sky_rad = np.percentile(merged_data['clear_sky_rad:W'], percentiles)\n",
    "    percentile_values_clear_sky_energy = np.percentile(merged_data['clear_sky_energy_1h:J'], percentiles)\n",
    "    percentile_values_df = pd.DataFrame({\n",
    "        'Percentile': percentiles,\n",
    "        'direct_rad:W':percentile_values_direct_rad,\n",
    "        'direct_rad_1h:J': percentile_values_direct_rad_1h,\n",
    "        'clear_sky_rad:W': percentile_values_clear_sky_rad,\n",
    "        'clear_sky_energy_1h:J': percentile_values_clear_sky_energy\n",
    "        })\n",
    "    \n",
    "    return percentile_values_df\n",
    "\n",
    "def get_anomals(merged_data,feature,percentile): \n",
    "    #identify the rows where the \"direct_rad:W\" column in x_train_a is high\n",
    "    #but the PV measurement in train_a is zero -> Indicates wrong\n",
    "    \n",
    "    percentile_df = get_percentiles_df(merged_data)\n",
    "    \n",
    "    # Define a threshold for high solar radiation\n",
    "    threshold = percentile_df[percentile_df['Percentile']==percentile][feature].values[0],\n",
    "\n",
    "    # Find rows where 'direct_rad:W' is high but PV measurement is zero\n",
    "    anomalous_rows = merged_data[(merged_data[feature] > threshold) & (merged_data['pv_measurement'] == 0)]\n",
    "    \n",
    "    \n",
    "    # Display the anomalous rows\n",
    "    return anomalous_rows\n",
    "\"\"\"\n",
    "merged_a1 = merged_a.copy().drop(get_anomals(merged_a,'clear_sky_rad:W',90).index)\n",
    "merged_b1 = merged_b.copy().drop(get_anomals(merged_b,'direct_rad:W',90).index)\n",
    "merged_c1 = merged_c.copy().drop(get_anomals(merged_c,'direct_rad_1h:J',90).index)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5e046-dee0-480b-93eb-1e77befff30a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Add avg pv at this time over the past week or month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ac698-54ab-4b43-8c88-42e9620dc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_df = merged_a.resample('7D', on='date_forecast',).mean()\n",
    "resampled_df['pv_measurement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33dc20-ae6c-4e3a-8be9-4298fb65f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_same_time_average(merged_df, period='7D'):\n",
    "    \n",
    "    df = merged_df.copy()\n",
    "    # Resample the data at the desired frequency\n",
    "    resampled_df = df.resample(period, on='date_forecast',).mean()\n",
    "    \n",
    "    \n",
    "    # Reindex the resampled data to match the original index, filling missing values by interpolation\n",
    "    return resampled_df\n",
    "\n",
    "#merged_a['weekly_avg_pv_hourly'] = calculate_rolling_same_time_average(merged_a, '7D')\n",
    "\n",
    "# Calculate the rolling average at the same time over the past month\n",
    "#merged_a['monthly_avg_same_time'] = calculate_rolling_same_time_average(merged_a, '30D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7d537-27fa-4d76-b9f5-87a487493050",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Add direct_rad * sun_elevation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5b1dd-5ce5-4af4-8628-820e7bb1bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did not improve kaggle score\n",
    "def add_rad_x_sun(merged_data):\n",
    "    df = merged_data.copy()\n",
    "    df['rad_x_sun_elevation'] = df['direct_rad:W']*df['sun_elevation:d']\n",
    "    return df\n",
    "\"\"\"\n",
    "mod_a = add_rad_x_sun(merged_a)\n",
    "mod_b = add_rad_x_sun(merged_b)\n",
    "mod_c = add_rad_x_sun(merged_c)\n",
    "\n",
    "x_test_a_mod = add_rad_x_sun(x_test_a)\n",
    "x_test_b_mod = add_rad_x_sun(x_test_b)\n",
    "x_test_c_mod = add_rad_x_sun(x_test_c)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbaa3a-6a11-4d6d-8aca-2490ead53649",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Categorical Feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9317f8d-1845-497d-a5ea-def502c3f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_columns_to_cat(merged_data, cat_features):\n",
    "    df = merged_data.copy()\n",
    "    for col in cat_features:\n",
    "        df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "cat_features=['estimated','dew_or_rime:idx','is_day:idx','is_in_shadow:idx','precip_type_5min:idx','snow_drift:idx']\n",
    "cat_features1 = ['estimated']\n",
    "cat_features2= ['estimated', 'is_in_shadow:idx', 'precip_type_5min:idx']\n",
    "\n",
    "merged_a_cat = convert_columns_to_cat(merged_a,cat_features1)\n",
    "merged_b_cat = convert_columns_to_cat(merged_b,cat_features1)\n",
    "merged_c_cat = convert_columns_to_cat(merged_c,cat_features1)\n",
    "\n",
    "x_test_a_cat = convert_columns_to_cat(x_test_a,cat_features1)\n",
    "x_test_b_cat = convert_columns_to_cat(x_test_b,cat_features1)\n",
    "x_test_c_cat = convert_columns_to_cat(x_test_c,cat_features1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd791c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Catboost model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a775a1b-ddaf-4ed2-988e-53856f4c9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_catboost(merged_df,own_split = False):    \n",
    "    if own_split:\n",
    "        train_data, val_data = split_dataset(merged_df)\n",
    "        train_data.drop(columns=['date_forecast','time'],inplace = True)\n",
    "        val_data.drop(columns=['date_forecast','time'],inplace = True)\n",
    "        \n",
    "        X_train = train_data.drop(columns=['pv_measurement'])\n",
    "        X_validation = val_data.drop(columns=['pv_measurement'])\n",
    "        \n",
    "        y_train = train_data['pv_measurement']\n",
    "        y_validation = val_data['pv_measurement']\n",
    "        \n",
    "    else:\n",
    "        merged_df = merged_df.drop(columns=['date_forecast','time'])\n",
    "            \n",
    "        X = merged_df.drop(columns=['pv_measurement'])\n",
    "        y = merged_df['pv_measurement']\n",
    "    \n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "  \n",
    "    catboost_model = CatBoostRegressor(\n",
    "        cat_features=['estimated'],\n",
    "        iterations=4000,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=42,\n",
    "        verbose=200\n",
    "    )\n",
    "    \n",
    "    catboost_model.fit(X_train, y_train, eval_set=(X_validation, y_validation), use_best_model=True, early_stopping_rounds=200)\n",
    "    return catboost_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c8e06-55a8-484c-a314-58cd6bd077fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = build_catboost(merged_a_cat,False)\n",
    "model_b = build_catboost(merged_b_cat,False)\n",
    "model_c = build_catboost(merged_c_cat,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279bf74-3a02-454e-9178-0718af6dc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = build_catboost(merged_a)\n",
    "model_b = build_catboost(merged_b)\n",
    "model_c = build_catboost(merged_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e1369-9bdd-4f0e-ae5b-119ecd3fcce7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build multiple catboost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "683d5454-0d10-4398-a121-e8fe022a38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_catboost_multiple_seed(merged_df,x_test):\n",
    "    merged_df = merged_df.drop(columns=['date_forecast', 'time'])\n",
    "    X = merged_df.drop(columns=['pv_measurement'])\n",
    "    y = merged_df['pv_measurement']\n",
    "    \n",
    "    predictions = []\n",
    "    models = []\n",
    "    scores = []\n",
    "    seeds = range(30)  # Random seeds from 0 to 9\n",
    "    \n",
    "    for seed in seeds:\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "            X, y, train_size=0.8, random_state=seed)\n",
    "        \n",
    "        catboost_model = CatBoostRegressor(\n",
    "            cat_features=['estimated'],\n",
    "            iterations=10000,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE',\n",
    "            random_seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        catboost_model.fit(X_train, y_train, eval_set=(X_validation, y_validation),\n",
    "                           use_best_model=True, early_stopping_rounds=200)\n",
    "        \n",
    "        score = catboost_model.get_best_score()['validation']['MAE']\n",
    "        scores.append(score)\n",
    "        # Print the best validation MAE for the current seed\n",
    "        print(f\"Best validation MAE for seed {seed}: {score}\")\n",
    "        \n",
    "        \n",
    "        # Predict using the current model\n",
    "        preds = catboost_model.predict(x_test)\n",
    "        predictions.append(preds)\n",
    "        models.append(catboost_model)\n",
    "    \n",
    "    # Average the predictions from all models\n",
    "    averaged_predictions = np.mean(predictions, axis=0)\n",
    "    average_score = np.mean(scores, axis = 0)\n",
    "    \n",
    "    return averaged_predictions,models, average_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0415b3cb-d879-4d9a-9eda-0d6cce18db50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation MAE for seed 0: 171.37944334964084\n",
      "Best validation MAE for seed 1: 176.08212201103308\n",
      "Best validation MAE for seed 2: 171.05006893630627\n",
      "Best validation MAE for seed 3: 166.07339279503202\n",
      "Best validation MAE for seed 4: 167.988943808119\n",
      "Best validation MAE for seed 5: 170.8150645617657\n",
      "Best validation MAE for seed 6: 171.1427035297064\n",
      "Best validation MAE for seed 7: 173.08250777616368\n",
      "Best validation MAE for seed 8: 167.35751429136945\n",
      "Best validation MAE for seed 9: 167.9763808397839\n",
      "Best validation MAE for seed 10: 172.39094422009907\n",
      "Best validation MAE for seed 11: 174.70258695632134\n",
      "Best validation MAE for seed 12: 170.12782753995657\n",
      "Best validation MAE for seed 13: 173.95599485733425\n",
      "Best validation MAE for seed 14: 168.68232671480627\n",
      "Best validation MAE for seed 15: 167.56956660591075\n",
      "Best validation MAE for seed 16: 167.1253103779937\n",
      "Best validation MAE for seed 17: 166.70762278748137\n",
      "Best validation MAE for seed 18: 174.7442509294883\n",
      "Best validation MAE for seed 19: 175.5771915928492\n",
      "Best validation MAE for seed 20: 177.85063174174178\n",
      "Best validation MAE for seed 21: 167.89684324774677\n",
      "Best validation MAE for seed 22: 171.2608080007077\n",
      "Best validation MAE for seed 23: 179.10621678277857\n",
      "Best validation MAE for seed 24: 172.46634786851456\n",
      "Best validation MAE for seed 25: 174.85215699217707\n",
      "Best validation MAE for seed 26: 168.4668831039136\n",
      "Best validation MAE for seed 27: 193.04309869146886\n",
      "Best validation MAE for seed 28: 172.39197819737268\n",
      "Best validation MAE for seed 29: 175.98486932781097\n",
      "Best validation MAE for seed 0: 23.912040384049238\n",
      "Best validation MAE for seed 1: 23.724411192965697\n",
      "Best validation MAE for seed 2: 23.998923403591043\n",
      "Best validation MAE for seed 3: 24.880892433983547\n",
      "Best validation MAE for seed 4: 24.49095960948933\n",
      "Best validation MAE for seed 5: 24.552579996009708\n",
      "Best validation MAE for seed 6: 23.71292905023691\n",
      "Best validation MAE for seed 7: 24.471049478852848\n",
      "Best validation MAE for seed 8: 23.985216319602742\n",
      "Best validation MAE for seed 9: 24.622083929969666\n",
      "Best validation MAE for seed 10: 24.685616944105274\n",
      "Best validation MAE for seed 11: 26.10224545313183\n",
      "Best validation MAE for seed 12: 26.04439039391924\n",
      "Best validation MAE for seed 13: 24.234472717451112\n",
      "Best validation MAE for seed 14: 24.15305861319575\n",
      "Best validation MAE for seed 15: 23.548028758358704\n",
      "Best validation MAE for seed 16: 24.55067884168616\n",
      "Best validation MAE for seed 17: 24.558728516472023\n",
      "Best validation MAE for seed 18: 25.04638110727234\n",
      "Best validation MAE for seed 19: 23.819100503191766\n",
      "Best validation MAE for seed 20: 24.30479906601962\n",
      "Best validation MAE for seed 21: 24.53753315444841\n",
      "Best validation MAE for seed 22: 24.44479643650539\n",
      "Best validation MAE for seed 23: 24.533699980177527\n",
      "Best validation MAE for seed 24: 24.277298693606692\n",
      "Best validation MAE for seed 25: 25.31510890225382\n",
      "Best validation MAE for seed 26: 25.409661932220814\n",
      "Best validation MAE for seed 27: 24.25848830689712\n",
      "Best validation MAE for seed 28: 25.54631536444146\n",
      "Best validation MAE for seed 29: 23.781452945479742\n",
      "Best validation MAE for seed 0: 22.061546621993212\n",
      "Best validation MAE for seed 1: 21.4235858148463\n",
      "Best validation MAE for seed 2: 21.18863401909545\n",
      "Best validation MAE for seed 3: 21.579079751260856\n",
      "Best validation MAE for seed 4: 21.8909984411219\n",
      "Best validation MAE for seed 5: 20.669749128839005\n",
      "Best validation MAE for seed 6: 20.88032832039881\n",
      "Best validation MAE for seed 7: 21.27073801259501\n",
      "Best validation MAE for seed 8: 22.311612078520806\n",
      "Best validation MAE for seed 9: 22.831494353403787\n",
      "Best validation MAE for seed 10: 22.575724179664412\n",
      "Best validation MAE for seed 11: 21.69138893162055\n",
      "Best validation MAE for seed 12: 21.45815800853017\n",
      "Best validation MAE for seed 13: 22.16736854824976\n",
      "Best validation MAE for seed 14: 22.97668060427853\n",
      "Best validation MAE for seed 15: 22.64197168823213\n",
      "Best validation MAE for seed 16: 22.206637861004637\n",
      "Best validation MAE for seed 17: 20.904779690641675\n",
      "Best validation MAE for seed 18: 21.288143758443564\n",
      "Best validation MAE for seed 19: 21.78890880560254\n",
      "Best validation MAE for seed 20: 21.888110786269323\n",
      "Best validation MAE for seed 21: 21.43665304182978\n",
      "Best validation MAE for seed 22: 21.8130748130728\n",
      "Best validation MAE for seed 23: 22.132269347675503\n",
      "Best validation MAE for seed 24: 20.937534733206277\n",
      "Best validation MAE for seed 25: 21.826615134401703\n",
      "Best validation MAE for seed 26: 21.88253225058559\n",
      "Best validation MAE for seed 27: 22.72243084251829\n",
      "Best validation MAE for seed 28: 21.4290142780506\n",
      "Best validation MAE for seed 29: 21.26982900406185\n"
     ]
    }
   ],
   "source": [
    "pred_a, models_a, avg_a = build_catboost_multiple_seed(merged_a_cat,x_test_a_cat)\n",
    "pred_b, models_b, avg_b = build_catboost_multiple_seed(merged_b_cat,x_test_b_cat)\n",
    "pred_c, models_c, avg_c= build_catboost_multiple_seed(merged_c_cat,x_test_c_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64a43dbc-c364-4c42-a2c8-3e3e3113b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.26171994784642 24.516764747652854 21.771519761667157\n"
     ]
    }
   ],
   "source": [
    "print(avg_a, avg_b, avg_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9609ca-e2a7-45c8-b3d5-8818baed82b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Predict Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415dee63-822a-42ff-b91c-acce4ef6fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_with_lag(model, test_data, initial_lag_value, lag_hours=1, column_name='pv_measurement'):\n",
    "    \"\"\"\n",
    "    Predict using a model that requires a lag feature, updating the test set iteratively.\n",
    "\n",
    "    Parameters:\n",
    "    model (model object): The trained model used for prediction.\n",
    "    test_data (pd.DataFrame): The test dataset without the target column.\n",
    "    initial_lag_value (float): The last known value of the target variable from the training set.\n",
    "    lag_hours (int): The number of hours to lag.\n",
    "    column_name (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A series of predictions for the test dataset.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    lag_feature_name = f\"{column_name}_lag_{lag_hours}h\"\n",
    "    current_lag_value = initial_lag_value\n",
    "    \n",
    "    for index, row in test_data.iterrows():\n",
    "        # Set the current lag value\n",
    "        row[lag_feature_name] = current_lag_value\n",
    "        \n",
    "        # Make a prediction\n",
    "        prediction = model.predict(row.to_frame().transpose())[0]\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        # Update the lag value with the current prediction\n",
    "        current_lag_value = prediction\n",
    "    \n",
    "    return pd.Series(predictions, index=test_data.index)\n",
    "\n",
    "initial_lag_val_a = merged_a.tail(24).iloc[0,52]\n",
    "initial_lag_val_b = merged_b.tail(24).iloc[0,52]\n",
    "initial_lag_val_c = merged_c.tail(24).iloc[0,52]\n",
    "\n",
    "# Then, use the function to make predictions:\n",
    "laged_pred_a = np.array(predict_with_lag(model=laged_model_a, test_data=x_test_a_laged, \n",
    "                               initial_lag_value=initial_lag_val_a, lag_hours=24))\n",
    "laged_pred_b = np.array(predict_with_lag(model=laged_model_b, test_data=x_test_b_laged, \n",
    "                               initial_lag_value=initial_lag_val_a, lag_hours=24))\n",
    "laged_pred_c = np.array(predict_with_lag(model=laged_model_c, test_data=x_test_c_laged, \n",
    "                               initial_lag_value=initial_lag_val_a, lag_hours=24))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67684c52-a72c-492b-9428-ac0a64049be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict and Submit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b63a33-669f-4e43-beb5-ee8a5e5df1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a = model_a.predict(x_test_a)\n",
    "pred_b = model_b.predict(x_test_b)\n",
    "pred_c = model_c.predict(x_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9604ef-d4e2-4b91-8159-be5b62cd37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a = model_a.predict(x_test_a_cat)\n",
    "pred_b = model_b.predict(x_test_b_cat)\n",
    "pred_c = model_c.predict(x_test_c_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "98989635-e755-4cb3-808e-fcaffbce7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub(pred_a,pred_b,pred_c):\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "    submission['prediction'] = np.concatenate([pred_a,pred_b,pred_c])\n",
    "    submission.loc[submission['prediction'] < 0, 'prediction'] = 0\n",
    "    return submission\n",
    "\n",
    "sub = create_sub(pred_a,pred_b,pred_c)\n",
    "#sub = create_sub(laged_pred_a,laged_pred_b,laged_pred_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf0e33ff-9039-4dd4-a2f8-2f2855dd00f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.392815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.271528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.269724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>54.984417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>346.915775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2155</td>\n",
       "      <td>78.973194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2156</td>\n",
       "      <td>47.315443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2157</td>\n",
       "      <td>18.056388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2158</td>\n",
       "      <td>4.216985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2159</td>\n",
       "      <td>2.788673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0        0    0.392815\n",
       "1        1    0.271528\n",
       "2        2    0.269724\n",
       "3        3   54.984417\n",
       "4        4  346.915775\n",
       "...    ...         ...\n",
       "2155  2155   78.973194\n",
       "2156  2156   47.315443\n",
       "2157  2157   18.056388\n",
       "2158  2158    4.216985\n",
       "2159  2159    2.788673\n",
       "\n",
       "[2160 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d456bf31-9668-4eb2-98a9-7097c3679b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f'Submissions/30modelCatboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24076d9-b911-498d-87ea-23c55b6691e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name,location):\n",
    "    save_directory = 'Saved_models/'+ location.upper()\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    # Define the path to save the model\n",
    "    model_file_path = os.path.join(save_directory, f'{model_name}.cbm')\n",
    "\n",
    "    # Save the model\n",
    "    model.save_model(model_file_path)\n",
    "\n",
    "    print(f\"Model successfully saved at {model_file_path}\")\n",
    "    \n",
    "save_model(model_a,'cyclical_catBoost','A')\n",
    "save_model(model_b,'cyclical_catBoost','B')\n",
    "save_model(model_c,'cyclical_catBoost','C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754cbdf-96c1-464c-8234-6b2e4ff37ab0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea45b7-a8fc-4c20-b887-b3ad124e0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_importance(model):\n",
    "    feats = {'feature':merged_a.drop(columns =['date_forecast','time','pv_measurement']).columns,\n",
    "         'importance':model.get_feature_importance()}\n",
    "    df = pd.DataFrame(feats).sort_values('importance',ascending = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231c4d3-d6ce-4ad3-8b7c-a7b1985c10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feat_importance(model_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf40fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_preds(pred1,pred2):\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.scatter(pred1['prediction'], pred2['prediction'], alpha=0.5)\n",
    "\n",
    "    # Line of equality (for reference)\n",
    "    plt.plot([pred1['prediction'].min(), pred1['prediction'].max()],\n",
    "             [pred2['prediction'].min(), pred2['prediction'].max()],\n",
    "             color='red', linestyle='--')\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('Predictions from First Model')\n",
    "    plt.ylabel('Predictions from New model')\n",
    "    plt.title('Comparison of Predictions from Two Models')\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471709c6-c455-48b0-b3b9-ff1d0eba5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_two_preds(sub,sub_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(preds):\n",
    "    test = pd.read_csv('test.csv')\n",
    "    predictions= preds['predict'].as_data_frame()\n",
    "    predictions['time'] = test['time'].unique()\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Prediction', color='tab:blue')\n",
    "    ax1.plot(predictions['time'], predictions['predict'], color='tab:blue', label='Solar Power Production')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Time Series Plot of prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8937d-fd7d-478c-9dae-77c249bf96d6",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1282dc2-ab41-4e86-a742-869ce7a9d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_average2.csv')\n",
    "df.loc[df['prediction'] < 8, 'prediction'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eba965-56c9-46bd-8310-64a3c19d8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'Submissions/merged_models3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34416f8d-98d7-475e-b7f7-4d9b27b49dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "maks = max([train_a['pv_measurement'].max(),train_b['pv_measurement'].max(),train_c['pv_measurement'].max()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78e54a-fbf8-4d98-9517-73614d584109",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd01df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"\"\"# Plot the distribution of \"direct_rad:W\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(merged_data['direct_rad:W'], bins=50, kde=True)\n",
    "    plt.title('Distribution of \"direct_rad:W\"')\n",
    "    plt.xlabel('Direct Radiation (W)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(merged_data['clear_sky_rad:W'], bins=50, kde=True)\n",
    "    plt.title('Distribution of \"clear_sky_rad:W\"')\n",
    "    plt.xlabel('Direct Radiation (W)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(merged_data['direct_rad_1h:J'], bins=50, kde=True)\n",
    "    plt.title('Distribution of \"direct_rad_1h:J\"')\n",
    "    plt.xlabel('Radiation 1h(J)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(merged_data['clear_sky_energy_1h:J'], bins=50, kde=True)\n",
    "    plt.title('Distribution of \"clear_sky_energy_1h:J\"')\n",
    "    plt.xlabel('Radiation 1h(J)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3b9cf-df7d-46de-a185-2a062e092aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_week_feat(df):\n",
    "\n",
    "    df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "    \n",
    "    # Extract week number\n",
    "    df['week'] = df['date_forecast'].dt.isocalendar().week\n",
    "\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "x_train_a = add_week_feat(x_train_a)\n",
    "x_train_b = add_week_feat(x_train_b)\n",
    "x_train_c = add_week_feat(x_train_c)\n",
    "\n",
    "x_test_a = add_week_feat(x_test_a)\n",
    "x_test_b = add_week_feat(x_test_b)\n",
    "x_test_c = add_week_feat(x_test_c)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "x_train_a = pd.read_csv('cleaned_data_Henning/A/x_train_a.csv')\n",
    "x_train_b = pd.read_csv('cleaned_data_Henning/B/x_train_b.csv')\n",
    "x_train_c = pd.read_csv('cleaned_data_Henning/C/x_train_c.csv')\n",
    "\n",
    "x_test_a = pd.read_csv('cleaned_data_Henning/A/x_test_a.csv')\n",
    "x_test_b = pd.read_csv('cleaned_data_Henning/B/x_test_b.csv')\n",
    "x_test_c = pd.read_csv('cleaned_data_Henning/C/x_test_c.csv')\n",
    "\n",
    "train_a = pd.read_csv('cleaned_data_Henning/A/train_a.csv')\n",
    "train_b = pd.read_csv('cleaned_data_Henning/B/train_b.csv')\n",
    "train_c = pd.read_csv('cleaned_data_Henning/C/train_c.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec1c03-ba78-4719-84be-a0214374b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_data, val_size=0.1, val = False, estimated_column = 'estimated'):\n",
    "    if val: \n",
    "        estimated_one = train_data[train_data[estimated_column] == 1]\n",
    "\n",
    "        #Split the filtered dataset into two\n",
    "        half_index = len(estimated_one) // 2\n",
    "        validation_set = estimated_one[half_index:]\n",
    "\n",
    "        # Combine the first half of observed_zero with the rest of the data where observed != 0\n",
    "        training_set = pd.concat([train_data[train_data[estimated_column] == 0], estimated_one[:half_index]])\n",
    "    else:\n",
    "        split_index = int(train_data.shape[0] * (1 - val_size))\n",
    "        training_set = train_data.iloc[:split_index]\n",
    "        validation_set = train_data.iloc[split_index:]\n",
    "    return training_set, validation_set\n",
    "\n",
    "\"\"\"\n",
    "  else:\n",
    "        training_set, validation_set = split_dataset(merged_df, val_size, True)\n",
    "        X_train = training_set.drop(columns=['pv_measurement'])\n",
    "        y_train = training_set['pv_measurement']\n",
    "        X_validation = validation_set.drop(columns=['pv_measurement'])\n",
    "        y_validation = validation_set['pv_measurement']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6a625-664a-4c2a-9477-67d05a66f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def build_catboost_models_multiple_seed_select_best(merged_df,x_test, n_seeds, select_k):\n",
    "    merged_df = merged_df.drop(columns=['date_forecast', 'time'])\n",
    "    X = merged_df.drop(columns=['pv_measurement'])\n",
    "    y = merged_df['pv_measurement']\n",
    "    \n",
    "    seed_mae_scores = []\n",
    "    models = []\n",
    "    seeds = range(n_seeds)  \n",
    "    \n",
    "    # Train 20 models and track their MAE scores\n",
    "    for seed in seeds:\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "            X, y, train_size=0.8, random_state=seed, shuffle=True)\n",
    "        \n",
    "        catboost_model = CatBoostRegressor(\n",
    "            #cat_features=['estimated', 'is_in_shadow:idx', 'precip_type_5min:idx'],\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE',\n",
    "            random_seed=seed,\n",
    "            verbose=200\n",
    "        )\n",
    "        \n",
    "        catboost_model.fit(X_train, y_train, eval_set=(X_validation, y_validation),\n",
    "                           use_best_model=True, early_stopping_rounds=200)\n",
    "        \n",
    "        # Get the best validation MAE for the current seed\n",
    "        mae_score = catboost_model.get_best_score()['validation']['MAE']\n",
    "        seed_mae_scores.append((seed, mae_score, catboost_model))\n",
    "        \n",
    "        # Print the best validation MAE for the current seed\n",
    "        print(f\"Best validation MAE for seed {seed}: {mae_score}\")\n",
    "    \n",
    "    # Sort models based on MAE scores and select the k best models\n",
    "    seed_mae_scores.sort(key=lambda x: x[1])\n",
    "    best_models = seed_mae_scores[:select_k]\n",
    "    \n",
    "    # Predict using the 10 best models\n",
    "    predictions = []\n",
    "    for seed, mae_score, model in best_models:\n",
    "        preds = model.predict(x_test)\n",
    "        predictions.append(preds)\n",
    "        models.append(model)\n",
    "    \n",
    "    average_best_mae = np.mean([mae_score for seed, mae_score, model in best_models])\n",
    "    # Average the predictions from the k best models\n",
    "    averaged_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return averaged_predictions,models,average_best_mae\n",
    "\n",
    "def split_dataset(train_data, date_column='month', estimated_column='estimated'):\n",
    "    \"\"\"\n",
    "    Splits the dataset into a training set and a validation set.\n",
    "    The validation set includes approximately 50% of the estimated data, evenly distributed across months.\n",
    "    Additionally, it includes about half of the observed data for May, June, and July, if present.\n",
    "    The training set includes all months, excluding the observed data that is included in the validation set.\n",
    "    \n",
    "    :param train_data: The original training dataset as a pandas DataFrame.\n",
    "    :param date_column: The name of the column that contains the month information.\n",
    "    :param observed_column: The name of the column that indicates if the data is observed.\n",
    "    :return: A tuple (training_set, validation_set)\n",
    "    \"\"\"\n",
    "    # Work with a copy to avoid modifying the original DataFrame\n",
    "    train_data = train_data.copy()\n",
    "    train_data.sort_values(by='date_forecast', inplace=True)\n",
    "    \n",
    "    # Separate observed and estimated data\n",
    "    estimated_data = train_data[train_data[estimated_column] == '1']\n",
    "    observed_data = train_data[train_data[estimated_column] == '0']\n",
    "    # Split the estimated data into training and validation sets\n",
    "    estimated_train, estimated_val = train_test_split(\n",
    "        estimated_data, test_size=0.5, random_state=42, stratify=estimated_data[date_column]\n",
    "    )\n",
    "    \n",
    "    # Check if there are any observed data for May, June, and July\n",
    "    if not observed_data[observed_data[date_column].isin([5, 6, 7])].empty:\n",
    "        observed_may_june_july = observed_data[observed_data[date_column].isin([5, 6, 7])]\n",
    "        observed_train_mjj, observed_val_mjj = train_test_split(\n",
    "            observed_may_june_july, test_size=0.5, random_state=42, stratify=observed_may_june_july[date_column]\n",
    "        )\n",
    "    else:\n",
    "        \n",
    "        observed_train_mjj = pd.DataFrame()\n",
    "        observed_val_mjj = pd.DataFrame()\n",
    "    \n",
    "    # Combine the estimated and observed May, June, July data for the validation set\n",
    "    validation_set = pd.concat([estimated_val, observed_val_mjj])\n",
    "    validation_set.sort_values(by='date_forecast', inplace=True)\n",
    "    \n",
    "    # The rest of the observed data (excluding May, June, July) will be added to the training set\n",
    "    observed_rest = observed_data[~observed_data[date_column].isin([5, 6, 7])]\n",
    "    \n",
    "    # Combine all training parts for the final training set\n",
    "    training_set = pd.concat([estimated_train, observed_train_mjj, observed_rest])\n",
    "    training_set.sort_values(by='date_forecast', inplace=True)\n",
    "    \n",
    "    return training_set, validation_set\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
