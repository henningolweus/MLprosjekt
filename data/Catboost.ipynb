{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c788fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6686ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a = pd.read_csv('cleaned_data/A/x_train_a.csv')\n",
    "x_train_b = pd.read_csv('cleaned_data/B/x_train_b.csv')\n",
    "x_train_c = pd.read_csv('cleaned_data/C/x_train_c.csv')\n",
    "\n",
    "x_test_a = pd.read_csv('cleaned_data/A/x_test_a.csv')\n",
    "x_test_b = pd.read_csv('cleaned_data/B/x_test_b.csv')\n",
    "x_test_c = pd.read_csv('cleaned_data/C/x_test_c.csv')\n",
    "\n",
    "train_a = pd.read_csv('cleaned_data/A/train_a.csv')\n",
    "train_b = pd.read_csv('cleaned_data/B/train_b.csv')\n",
    "train_c = pd.read_csv('cleaned_data/C/train_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78d43bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a['time'] = pd.to_datetime(train_a['time'])\n",
    "train_b['time'] = pd.to_datetime(train_b['time'])\n",
    "train_c['time'] = pd.to_datetime(train_c['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfd9be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_a = x_test_a.drop(columns = ['date_forecast'])\n",
    "x_test_b = x_test_b.drop(columns = ['date_forecast'])\n",
    "x_test_c = x_test_c.drop(columns = ['date_forecast'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e17787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in X_train that has timestamp that does not exist in train_loc, and visa_verca\n",
    "#e.g missing solar power measurements from 2022-10-21 01:00 - 2022-10-28 21:00\n",
    "def align_X_y(x_train,y_train, x_date_column='date_forecast', y_date_column='time'):\n",
    "    \"\"\"\n",
    "    Aligns two dataframes based on the 'date_forecast' column of X and the 'time' column of y,\n",
    "    ensuring that only rows with matching time values are retained.\n",
    "\n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): The first dataframe with time in the 'date_forecast'\n",
    "    - y (pd.DataFrame): The second dataframe with time in the 'time' column.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the aligned dataframes.\n",
    "    \"\"\"\n",
    "    # Convert date columns to datetime format for easier comparison\n",
    "    x_train[x_date_column] = pd.to_datetime(x_train[x_date_column])\n",
    "    y_train[y_date_column] = pd.to_datetime(y_train[y_date_column])\n",
    "    \n",
    "    # Find common dates\n",
    "    common_dates = x_train[x_date_column][x_train[x_date_column].isin(y_train[y_date_column])]\n",
    "    \n",
    "    # Filter both datasets based on common dates\n",
    "    x_train_synced = x_train[x_train[x_date_column].isin(common_dates)]\n",
    "    y_train_synced = y_train[y_train[y_date_column].isin(common_dates)]\n",
    "    \n",
    "    return x_train_synced, y_train_synced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1a15d",
   "metadata": {},
   "source": [
    "# Analysis of Target variable  - Looking at PV_measurement\n",
    "1. Handle constant measurments over longer periods of time. Likely caused by sensor malfunction, data logging issues, or other external factors.\n",
    "    - Handeled by removing all constant values lasting more than 24 hours \n",
    "2. Add cyclical features \n",
    "2. Handle longer periods of missing data:\n",
    "    - Remove (currently tested)\n",
    "    - Interpolate \n",
    "    - Copy from previous year\n",
    "    - Copy solar production at missing time from another location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89446f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Handle constant PV measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5db23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series plot of PV_measurement \n",
    "\n",
    "def solar_prod_plot(y_train, resolution='year', chunks=5):\n",
    "    df = y_train.copy()\n",
    "    \n",
    "    # Determine the plotting resolution based on the 'resolution' argument\n",
    "    # Chunks = number of year/months/days in each plot\n",
    "    if resolution == 'year':\n",
    "        unique_values = df['time'].dt.year.unique()\n",
    "        label = 'Year'\n",
    "    elif resolution == 'month':\n",
    "        df['year_month'] = df['time'].dt.to_period('M')\n",
    "        unique_values = df['year_month'].unique()\n",
    "        label = 'Month'\n",
    "    elif resolution == 'week':\n",
    "        df['year_week'] = df['time'].dt.to_period('W')\n",
    "        unique_values = df['year_week'].unique()\n",
    "        label = 'Week'\n",
    "    elif resolution == 'day':\n",
    "        df['date'] = df['time'].dt.date\n",
    "        unique_values = df['date'].unique()\n",
    "        label = 'Day'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid resolution. Choose from 'year', 'month', 'week', or 'day'.\")\n",
    "    \n",
    "    # Loop over the unique values in chunks\n",
    "    for i in range(0, len(unique_values), chunks):\n",
    "        subset_values = unique_values[i:i+chunks]\n",
    "        \n",
    "        if resolution == 'year':\n",
    "            subset_df = df[df['time'].dt.year.isin(subset_values)]\n",
    "        elif resolution == 'month':\n",
    "            subset_df = df[df['year_month'].isin(subset_values)]\n",
    "        elif resolution == 'week':\n",
    "            subset_df = df[df['year_week'].isin(subset_values)]\n",
    "        elif resolution == 'day':\n",
    "            subset_df = df[df['date'].isin(subset_values)]\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(subset_df['time'], subset_df['pv_measurement'])\n",
    "\n",
    "        title = f\"Solar Power Production for {label}: {subset_values[0]}\"\n",
    "        if len(subset_values) > 1:\n",
    "            title += f\" to {subset_values[-1]}\"\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"PV Measurement\")\n",
    "        plt.show()\n",
    "\n",
    "def remove_constant_intervals(y_train, low_thresh, upp_thresh):\n",
    "    \"\"\"\n",
    "    Identify and remove intervals of constant PV readings that exceed a specified duration. \n",
    "    Constant readings may indicate sensor malfunctions or data logging issues.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    y_train : pd.DataFrame\n",
    "        Dataframe containing the time-series data of solar power production.\n",
    "    threshold : int\n",
    "        The minimum duration required for an interval to be considered for removal.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The input dataframe with intervals of constant readings (exceeding the duration threshold) removed.\n",
    "    \"\"\"\n",
    "    df = y_train.copy()\n",
    "    \n",
    "    # Calculate the difference in production values\n",
    "    df['diff'] = df['pv_measurement'].diff()\n",
    "\n",
    "    # Identify where the difference is zero\n",
    "    df['zero_diff'] = df['diff'].abs() < 1e-5\n",
    "\n",
    "    # Identify groups of consecutive zero differences\n",
    "    df['group'] = (df['zero_diff'] != df['zero_diff'].shift()).cumsum()\n",
    "\n",
    "    # Filter out only the groups with consecutive zero differences\n",
    "    constant_intervals = df[df['zero_diff']].groupby('group').agg(start=('time', 'min'), \n",
    "                                                                  end=('time', 'max'),\n",
    "                                                                  duration=('time', 'size'))\n",
    "    \n",
    "    # Filter intervals based on the threshold\n",
    "    interval_df_thresh = constant_intervals[(constant_intervals['duration'] > low_thresh) & (constant_intervals['duration'] <upp_thresh)]\n",
    "    \n",
    "    # Remove rows from the main dataframe that fall within these intervals\n",
    "    for _, row in interval_df_thresh.iterrows():\n",
    "        start_time, end_time = row['start'], row['end']\n",
    "        df = df[(df['time'] < start_time) | (df['time'] > end_time)]\n",
    "    \n",
    "    # Drop the added columns used for calculations\n",
    "    df.drop(columns=['diff', 'zero_diff', 'group'], inplace=True)\n",
    "    \n",
    "    return df, constant_intervals\n",
    "\n",
    "\n",
    "def get_time_interval(df, start_time = '2020-08-01 00:00:00', end_time = '2021-01-01 00:00:00'):\n",
    "    # Filter rows based on the time period\n",
    "    filtered_df = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e1af31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removed all constant values with duration > 24 hours\n",
    "\n",
    "train_a, const_interval_a = remove_constant_intervals(train_a,24,10**6)\n",
    "\n",
    "#update X_train_a by removing coresponding rows that have been filtered here\n",
    "x_train_a, train_a = align_X_y(x_train_a, train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77a90e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows removed 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>2020-01-04 15:00:00</td>\n",
       "      <td>2020-01-06 08:00:00</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    start                 end  duration\n",
       "group                                                  \n",
       "434   2020-01-04 15:00:00 2020-01-06 08:00:00        42"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_removed_a = np.sum(const_interval_a[const_interval_a['duration']>24]['duration'])\n",
    "print(f'total number of rows removed {rows_removed_a}')\n",
    "const_interval_a[const_interval_a['duration']>24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d4bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in groups of constant values, where duration of constant measurements is > 1 day (24 hours)\n",
    "train_b, const_interval_b = remove_constant_intervals(train_b,24,10**6)\n",
    "\n",
    "#update X_train_a by removing coresponding rows that have been filtered here\n",
    "x_train_b, train_b = align_X_y(x_train_b, train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd0cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows removed 6865\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019-01-14 15:00:00</td>\n",
       "      <td>2019-01-18 11:00:00</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019-01-19 13:00:00</td>\n",
       "      <td>2019-01-26 08:00:00</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2019-01-27 11:00:00</td>\n",
       "      <td>2019-01-28 13:00:00</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2019-02-10 16:00:00</td>\n",
       "      <td>2019-02-13 07:00:00</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2019-03-23 18:00:00</td>\n",
       "      <td>2019-03-26 06:00:00</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>2019-05-31 08:00:00</td>\n",
       "      <td>2019-06-03 12:00:00</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2019-10-28 14:00:00</td>\n",
       "      <td>2019-10-30 22:00:00</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>2019-12-01 13:00:00</td>\n",
       "      <td>2019-12-04 08:00:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>2019-12-07 14:00:00</td>\n",
       "      <td>2019-12-11 08:00:00</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2019-12-18 14:00:00</td>\n",
       "      <td>2019-12-20 09:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>2019-12-25 14:00:00</td>\n",
       "      <td>2019-12-30 09:00:00</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>2020-01-02 14:00:00</td>\n",
       "      <td>2020-01-04 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>2020-01-04 14:00:00</td>\n",
       "      <td>2020-01-06 10:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>2020-01-24 12:00:00</td>\n",
       "      <td>2020-01-26 08:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>2020-02-05 14:00:00</td>\n",
       "      <td>2020-02-07 09:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>2020-02-23 17:00:00</td>\n",
       "      <td>2020-02-25 09:00:00</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>2020-03-26 14:00:00</td>\n",
       "      <td>2020-03-27 21:00:00</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>2020-04-02 02:00:00</td>\n",
       "      <td>2020-04-16 06:00:00</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2020-07-12 21:00:00</td>\n",
       "      <td>2020-08-25 21:00:00</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>2020-09-24 13:00:00</td>\n",
       "      <td>2020-09-25 21:00:00</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>2020-12-16 14:00:00</td>\n",
       "      <td>2020-12-18 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>2020-12-26 14:00:00</td>\n",
       "      <td>2020-12-28 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>2021-01-09 14:00:00</td>\n",
       "      <td>2021-01-13 09:00:00</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>2021-01-19 13:00:00</td>\n",
       "      <td>2021-01-21 09:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>2021-01-22 16:00:00</td>\n",
       "      <td>2021-01-24 08:00:00</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>2021-01-28 16:00:00</td>\n",
       "      <td>2021-01-30 08:00:00</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>2021-01-30 14:00:00</td>\n",
       "      <td>2021-02-01 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>2021-02-01 11:00:00</td>\n",
       "      <td>2021-02-03 08:00:00</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2021-02-18 00:00:00</td>\n",
       "      <td>2021-03-08 14:00:00</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2021-03-08 16:00:00</td>\n",
       "      <td>2021-04-19 11:00:00</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>2021-04-28 23:00:00</td>\n",
       "      <td>2021-05-01 21:00:00</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>2021-06-05 02:00:00</td>\n",
       "      <td>2021-06-07 07:00:00</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2021-06-13 02:00:00</td>\n",
       "      <td>2021-06-14 09:00:00</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>2021-06-22 02:00:00</td>\n",
       "      <td>2021-06-24 08:00:00</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>2021-07-03 13:00:00</td>\n",
       "      <td>2021-07-06 05:00:00</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>2021-08-25 23:00:00</td>\n",
       "      <td>2021-09-03 21:00:00</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>2021-09-08 13:00:00</td>\n",
       "      <td>2021-09-14 13:00:00</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>2021-09-19 00:00:00</td>\n",
       "      <td>2021-09-27 08:00:00</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>2021-11-22 15:00:00</td>\n",
       "      <td>2021-11-24 08:00:00</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>2021-11-26 12:00:00</td>\n",
       "      <td>2021-12-04 08:00:00</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>2021-12-16 14:00:00</td>\n",
       "      <td>2021-12-18 09:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>2021-12-21 14:00:00</td>\n",
       "      <td>2021-12-24 09:00:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>2021-12-24 12:00:00</td>\n",
       "      <td>2022-01-03 09:00:00</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>2022-01-03 13:00:00</td>\n",
       "      <td>2022-01-11 09:00:00</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>2022-01-12 14:00:00</td>\n",
       "      <td>2022-01-14 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>2022-01-30 16:00:00</td>\n",
       "      <td>2022-02-04 09:00:00</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2022-02-10 15:00:00</td>\n",
       "      <td>2022-02-12 11:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>2022-02-14 16:00:00</td>\n",
       "      <td>2022-02-16 09:00:00</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>2022-02-16 14:00:00</td>\n",
       "      <td>2022-02-18 10:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>2022-02-19 10:00:00</td>\n",
       "      <td>2022-02-24 06:00:00</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2022-03-06 11:00:00</td>\n",
       "      <td>2022-03-07 11:00:00</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2022-03-19 14:00:00</td>\n",
       "      <td>2022-03-28 07:00:00</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>2022-03-28 12:00:00</td>\n",
       "      <td>2022-04-05 06:00:00</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>2023-01-15 15:00:00</td>\n",
       "      <td>2023-01-17 09:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    start                 end  duration\n",
       "group                                                  \n",
       "32    2019-01-14 15:00:00 2019-01-18 11:00:00        93\n",
       "36    2019-01-19 13:00:00 2019-01-26 08:00:00       164\n",
       "40    2019-01-27 11:00:00 2019-01-28 13:00:00        27\n",
       "74    2019-02-10 16:00:00 2019-02-13 07:00:00        64\n",
       "160   2019-03-23 18:00:00 2019-03-26 06:00:00        61\n",
       "302   2019-05-31 08:00:00 2019-06-03 12:00:00        77\n",
       "606   2019-10-28 14:00:00 2019-10-30 22:00:00        57\n",
       "674   2019-12-01 13:00:00 2019-12-04 08:00:00        68\n",
       "682   2019-12-07 14:00:00 2019-12-11 08:00:00        91\n",
       "700   2019-12-18 14:00:00 2019-12-20 09:00:00        44\n",
       "712   2019-12-25 14:00:00 2019-12-30 09:00:00       116\n",
       "724   2020-01-02 14:00:00 2020-01-04 08:00:00        43\n",
       "726   2020-01-04 14:00:00 2020-01-06 10:00:00        45\n",
       "768   2020-01-24 12:00:00 2020-01-26 08:00:00        45\n",
       "790   2020-02-05 14:00:00 2020-02-07 09:00:00        44\n",
       "824   2020-02-23 17:00:00 2020-02-25 09:00:00        41\n",
       "890   2020-03-26 14:00:00 2020-03-27 21:00:00        32\n",
       "906   2020-04-02 02:00:00 2020-04-16 06:00:00       341\n",
       "1090  2020-07-12 21:00:00 2020-08-25 21:00:00      1057\n",
       "1154  2020-09-24 13:00:00 2020-09-25 21:00:00        33\n",
       "1332  2020-12-16 14:00:00 2020-12-18 08:00:00        43\n",
       "1352  2020-12-26 14:00:00 2020-12-28 08:00:00        43\n",
       "1380  2021-01-09 14:00:00 2021-01-13 09:00:00        92\n",
       "1396  2021-01-19 13:00:00 2021-01-21 09:00:00        45\n",
       "1400  2021-01-22 16:00:00 2021-01-24 08:00:00        41\n",
       "1410  2021-01-28 16:00:00 2021-01-30 08:00:00        41\n",
       "1414  2021-01-30 14:00:00 2021-02-01 08:00:00        43\n",
       "1416  2021-02-01 11:00:00 2021-02-03 08:00:00        46\n",
       "1454  2021-02-18 00:00:00 2021-03-08 14:00:00       447\n",
       "1456  2021-03-08 16:00:00 2021-04-19 11:00:00      1003\n",
       "1478  2021-04-28 23:00:00 2021-05-01 21:00:00        71\n",
       "1550  2021-06-05 02:00:00 2021-06-07 07:00:00        54\n",
       "1564  2021-06-13 02:00:00 2021-06-14 09:00:00        32\n",
       "1582  2021-06-22 02:00:00 2021-06-24 08:00:00        55\n",
       "1602  2021-07-03 13:00:00 2021-07-06 05:00:00        65\n",
       "1710  2021-08-25 23:00:00 2021-09-03 21:00:00       215\n",
       "1722  2021-09-08 13:00:00 2021-09-14 13:00:00       145\n",
       "1734  2021-09-19 00:00:00 2021-09-27 08:00:00       201\n",
       "1858  2021-11-22 15:00:00 2021-11-24 08:00:00        42\n",
       "1864  2021-11-26 12:00:00 2021-12-04 08:00:00       189\n",
       "1894  2021-12-16 14:00:00 2021-12-18 09:00:00        44\n",
       "1902  2021-12-21 14:00:00 2021-12-24 09:00:00        68\n",
       "1904  2021-12-24 12:00:00 2022-01-03 09:00:00       238\n",
       "1906  2022-01-03 13:00:00 2022-01-11 09:00:00       189\n",
       "1910  2022-01-12 14:00:00 2022-01-14 08:00:00        43\n",
       "1948  2022-01-30 16:00:00 2022-02-04 09:00:00       114\n",
       "1966  2022-02-10 15:00:00 2022-02-12 11:00:00        45\n",
       "1972  2022-02-14 16:00:00 2022-02-16 09:00:00        42\n",
       "1974  2022-02-16 14:00:00 2022-02-18 10:00:00        45\n",
       "1978  2022-02-19 10:00:00 2022-02-24 06:00:00       117\n",
       "2004  2022-03-06 11:00:00 2022-03-07 11:00:00        25\n",
       "2032  2022-03-19 14:00:00 2022-03-28 07:00:00       209\n",
       "2034  2022-03-28 12:00:00 2022-04-05 06:00:00       187\n",
       "2196  2023-01-15 15:00:00 2023-01-17 09:00:00        43"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_removed = np.sum(const_interval_b[const_interval_b['duration']>24]['duration'])\n",
    "print(f'total number of rows removed {rows_removed}')\n",
    "const_interval_b[const_interval_b['duration']>24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d09483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in groups of constant values, where duration of constant measurements is > 1 day (24 hours)\n",
    "train_c, const_interval_c = remove_constant_intervals(train_c,24,10**6)\n",
    "\n",
    "#update X_train_a by removing coresponding rows that have been filtered here\n",
    "x_train_c, train_c = align_X_y(x_train_c, train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c619c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows removed 4926\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-09-04 10:00:00</td>\n",
       "      <td>2019-09-05 12:00:00</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2019-11-11 12:00:00</td>\n",
       "      <td>2019-11-13 08:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2019-11-28 15:00:00</td>\n",
       "      <td>2019-12-05 09:00:00</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2019-12-07 14:00:00</td>\n",
       "      <td>2019-12-13 09:00:00</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2019-12-16 14:00:00</td>\n",
       "      <td>2019-12-21 09:00:00</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2019-12-25 13:00:00</td>\n",
       "      <td>2019-12-30 09:00:00</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2020-01-02 14:00:00</td>\n",
       "      <td>2020-01-07 09:00:00</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2020-01-23 15:00:00</td>\n",
       "      <td>2020-01-26 08:00:00</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>2020-02-05 14:00:00</td>\n",
       "      <td>2020-02-10 07:00:00</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>2020-02-23 17:00:00</td>\n",
       "      <td>2020-03-08 08:00:00</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2020-03-28 18:00:00</td>\n",
       "      <td>2020-03-31 09:00:00</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>2020-11-18 13:00:00</td>\n",
       "      <td>2020-11-22 08:00:00</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>2020-12-16 14:00:00</td>\n",
       "      <td>2020-12-18 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>2020-12-21 14:00:00</td>\n",
       "      <td>2020-12-23 09:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>2020-12-25 14:00:00</td>\n",
       "      <td>2020-12-28 09:00:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>2021-01-09 14:00:00</td>\n",
       "      <td>2021-01-22 10:00:00</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>2021-01-22 15:00:00</td>\n",
       "      <td>2021-01-24 10:00:00</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>2021-01-24 13:00:00</td>\n",
       "      <td>2021-02-19 10:00:00</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>2021-03-03 17:00:00</td>\n",
       "      <td>2021-03-06 07:00:00</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>2021-03-08 14:00:00</td>\n",
       "      <td>2021-03-10 08:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>2021-03-20 18:00:00</td>\n",
       "      <td>2021-03-22 05:00:00</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2021-04-09 19:00:00</td>\n",
       "      <td>2021-04-11 08:00:00</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>2021-11-24 14:00:00</td>\n",
       "      <td>2021-12-14 09:00:00</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>2021-12-21 14:00:00</td>\n",
       "      <td>2022-01-16 10:00:00</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>2022-01-16 13:00:00</td>\n",
       "      <td>2022-01-18 10:00:00</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2022-01-19 14:00:00</td>\n",
       "      <td>2022-01-22 09:00:00</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>2022-01-24 16:00:00</td>\n",
       "      <td>2022-01-26 10:00:00</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>2022-01-27 16:00:00</td>\n",
       "      <td>2022-01-30 08:00:00</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>2022-01-30 15:00:00</td>\n",
       "      <td>2022-02-07 11:00:00</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>2022-02-08 14:00:00</td>\n",
       "      <td>2022-03-02 09:00:00</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>2022-04-02 18:00:00</td>\n",
       "      <td>2022-04-04 09:00:00</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>2022-04-05 13:00:00</td>\n",
       "      <td>2022-04-08 09:00:00</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>2023-02-19 14:00:00</td>\n",
       "      <td>2023-02-21 10:00:00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>2023-02-21 16:00:00</td>\n",
       "      <td>2023-02-23 09:00:00</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    start                 end  duration\n",
       "group                                                  \n",
       "2     2019-09-04 10:00:00 2019-09-05 12:00:00        27\n",
       "180   2019-11-11 12:00:00 2019-11-13 08:00:00        45\n",
       "230   2019-11-28 15:00:00 2019-12-05 09:00:00       163\n",
       "240   2019-12-07 14:00:00 2019-12-13 09:00:00       140\n",
       "256   2019-12-16 14:00:00 2019-12-21 09:00:00       116\n",
       "276   2019-12-25 13:00:00 2019-12-30 09:00:00       117\n",
       "290   2020-01-02 14:00:00 2020-01-07 09:00:00       116\n",
       "340   2020-01-23 15:00:00 2020-01-26 08:00:00        66\n",
       "376   2020-02-05 14:00:00 2020-02-10 07:00:00       114\n",
       "414   2020-02-23 17:00:00 2020-03-08 08:00:00       328\n",
       "484   2020-03-28 18:00:00 2020-03-31 09:00:00        64\n",
       "1150  2020-11-18 13:00:00 2020-11-22 08:00:00        92\n",
       "1238  2020-12-16 14:00:00 2020-12-18 08:00:00        43\n",
       "1252  2020-12-21 14:00:00 2020-12-23 09:00:00        44\n",
       "1264  2020-12-25 14:00:00 2020-12-28 09:00:00        68\n",
       "1312  2021-01-09 14:00:00 2021-01-22 10:00:00       309\n",
       "1316  2021-01-22 15:00:00 2021-01-24 10:00:00        44\n",
       "1318  2021-01-24 13:00:00 2021-02-19 10:00:00       622\n",
       "1358  2021-03-03 17:00:00 2021-03-06 07:00:00        63\n",
       "1374  2021-03-08 14:00:00 2021-03-10 08:00:00        43\n",
       "1408  2021-03-20 18:00:00 2021-03-22 05:00:00        36\n",
       "1458  2021-04-09 19:00:00 2021-04-11 08:00:00        38\n",
       "2122  2021-11-24 14:00:00 2021-12-14 09:00:00       476\n",
       "2146  2021-12-21 14:00:00 2022-01-16 10:00:00       621\n",
       "2148  2022-01-16 13:00:00 2022-01-18 10:00:00        46\n",
       "2152  2022-01-19 14:00:00 2022-01-22 09:00:00        68\n",
       "2164  2022-01-24 16:00:00 2022-01-26 10:00:00        43\n",
       "2168  2022-01-27 16:00:00 2022-01-30 08:00:00        65\n",
       "2172  2022-01-30 15:00:00 2022-02-07 11:00:00       189\n",
       "2178  2022-02-08 14:00:00 2022-03-02 09:00:00       524\n",
       "2284  2022-04-02 18:00:00 2022-04-04 09:00:00        40\n",
       "2290  2022-04-05 13:00:00 2022-04-08 09:00:00        69\n",
       "2486  2023-02-19 14:00:00 2023-02-21 10:00:00        45\n",
       "2490  2023-02-21 16:00:00 2023-02-23 09:00:00        42"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_removed = np.sum(const_interval_c[const_interval_c['duration']>24]['duration'])\n",
    "print(f'total number of rows removed {rows_removed}')\n",
    "const_interval_c[const_interval_c['duration']>24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295598a-c357-4143-898f-f57bc361c20f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merge x_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7084bebc-37bb-4a70-afd6-5d22049f2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_a = pd.merge(x_train_a, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_b = pd.merge(x_train_b, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "merged_c = pd.merge(x_train_c, train_c, left_on='date_forecast', right_on='time', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcdb7f-cabb-4695-90ca-ab8a5a283e42",
   "metadata": {},
   "source": [
    "### Add lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0aae5e-3b81-426d-a165-06f346b3e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_feature(data, lag_hours, column_name='pv_measurement'):\n",
    "    \"\"\"\n",
    "    Add lag features to the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The original dataset.\n",
    "    lag_hours (int): The number of hours to lag.\n",
    "    column_name (str): The name of the column to create the lag feature for.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataset with the new lag feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the lag feature\n",
    "    df = data.copy()\n",
    "    lag_feature_name = f\"{column_name}_lag_{lag_hours}h\"\n",
    "    df[lag_feature_name] = df[column_name].shift(lag_hours)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6563274-2059-4198-a061-2875448f9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "laged_a = add_lag_feature(merged_a,1)\n",
    "laged_b = add_lag_feature(merged_b,1)\n",
    "laged_c = add_lag_feature(merged_c,1)\n",
    "\n",
    "x_test_a_laged = x_test_a.copy()\n",
    "x_test_b_laged = x_test_b.copy()\n",
    "x_test_c_laged = x_test_c.copy()\n",
    "\n",
    "\n",
    "# You can add an empty column for the lag feature in your test set:\n",
    "x_test_a_laged[f'pv_measurement_lag_{1}h'] = None\n",
    "x_test_b_laged[f'pv_measurement_lag_{1}h'] = None\n",
    "x_test_c_laged[f'pv_measurement_lag_{1}h'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b9338-8fa4-41ed-bd50-584aafeb6421",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Handle NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9057e-2844-4c9d-bd82-71042e28865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are plotting on the modified dataset\n",
    "def time_series_plot(feature,merged_data):\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Solar Power Production', color='tab:blue')\n",
    "    ax1.plot(merged_data['time'], merged_data['pv_measurement'], color='tab:blue', label='Solar Power Production')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    ax2.set_ylabel(feature, color='tab:red')  \n",
    "    ax2.plot(merged_data['date_forecast'], merged_data[feature], color='tab:red', label=feature)\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Time Series Plot of Solar Power Production and {feature}')\n",
    "    plt.show()\n",
    "\n",
    "time_series_plot('ceiling_height_agl:m', merged_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae1f7c67-65da-468d-8059-d6c066e3de4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "snow_density:kgm3                 32903\n",
       "ceiling_height_agl:m               6151\n",
       "cloud_base_agl:m                   2387\n",
       "sfc_pressure:hPa                     24\n",
       "wind_speed_u_10m:ms                  24\n",
       "precip_type_5min:idx                 24\n",
       "pressure_100m:hPa                    24\n",
       "absolute_humidity_2m:gm3             24\n",
       "prob_rime:p                          24\n",
       "wind_speed_10m:ms                    24\n",
       "relative_humidity_1000hPa:p          24\n",
       "pressure_50m:hPa                     24\n",
       "is_in_shadow:idx                     24\n",
       "snow_depth:cm                        24\n",
       "snow_drift:idx                       24\n",
       "visibility:m                         24\n",
       "total_cloud_cover:p                  24\n",
       "sun_azimuth:d                        24\n",
       "sun_elevation:d                      24\n",
       "msl_pressure:hPa                     24\n",
       "wind_speed_v_10m:ms                  24\n",
       "is_day:idx                           24\n",
       "t_1000hPa:K                          24\n",
       "air_density_2m:kgm3                  24\n",
       "clear_sky_rad:W                      24\n",
       "dew_or_rime:idx                      24\n",
       "dew_point_2m:K                       24\n",
       "diffuse_rad:W                        24\n",
       "direct_rad:W                         24\n",
       "effective_cloud_cover:p              24\n",
       "elevation:m                          24\n",
       "wind_speed_w_1000hPa:ms              24\n",
       "super_cooled_liquid_water:kgm2       24\n",
       "hour                                  0\n",
       "estimated                             0\n",
       "day                                   0\n",
       "month                                 0\n",
       "year                                  0\n",
       "time                                  0\n",
       "date_forecast                         0\n",
       "snow_water:kgm2                       0\n",
       "snow_melt_10min:mm                    0\n",
       "rain_water:kgm2                       0\n",
       "precip_5min:mm                        0\n",
       "fresh_snow_6h:cm                      0\n",
       "fresh_snow_3h:cm                      0\n",
       "fresh_snow_24h:cm                     0\n",
       "fresh_snow_1h:cm                      0\n",
       "fresh_snow_12h:cm                     0\n",
       "direct_rad_1h:J                       0\n",
       "diffuse_rad_1h:J                      0\n",
       "clear_sky_energy_1h:J                 0\n",
       "pv_measurement                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_a.isnull().sum().sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f652a7-cec9-4c63-83ff-cf45ad74db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_a['ceiling_height_agl:m'] = merged_a['ceiling_height_agl:m'].fillna(value = 0)\n",
    "merged_b['ceiling_height_agl:m'] = merged_b['ceiling_height_agl:m'].fillna(value = 0)\n",
    "merged_c['ceiling_height_agl:m'] = merged_c['ceiling_height_agl:m'].fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0463e5b-49e1-480a-8713-53a29ac86c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_a.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d8647",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Add Cyclical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cyclical features for hour of the day\n",
    "def cyclic_hourly(x):\n",
    "    train_data = x.copy()\n",
    "    train_data['hour_sin'] = np.sin(2 * np.pi * train_data['hour'] / 24)\n",
    "    train_data['hour_cos'] = np.cos(2 * np.pi * train_data['hour'] / 24)\n",
    "    return train_data\n",
    "\n",
    "\n",
    "# Creating cyclical features for month of the year\n",
    "def cyclic_monthly(x):\n",
    "    train_data = x.copy()\n",
    "    train_data['month_sin'] = np.sin(2 * np.pi * train_data['month'] / 12)\n",
    "    train_data['month_cos'] = np.cos(2 * np.pi * train_data['month'] / 12)\n",
    "    return train_data\n",
    "\n",
    "\"\"\"\n",
    "x_train_a = cyclic_hourly(x_train_a)\n",
    "x_train_a = cyclic_monthly(x_train_a)\n",
    "\n",
    "x_test_a = cyclic_hourly(x_test_a)\n",
    "x_test_a = cyclic_monthly(x_test_a)\n",
    "\n",
    "x_train_b = cyclic_hourly(x_train_b)\n",
    "x_train_b = cyclic_monthly(x_train_b)\n",
    "\n",
    "x_test_b = cyclic_hourly(x_test_b)\n",
    "x_test_b = cyclic_monthly(x_test_b)\n",
    "\n",
    "x_train_c = cyclic_hourly(x_train_c)\n",
    "x_train_c = cyclic_monthly(x_train_c)\n",
    "\n",
    "x_test_c = cyclic_hourly(x_test_c)\n",
    "x_test_c = cyclic_monthly(x_test_c)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab20ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove outliers during night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_avg(y_train):\n",
    "    # Grouping by hour and calculating the average PV measurement for each hour\n",
    "    train_data = y_train.copy()\n",
    "    train_data['hour'] = y_train['time'].dt.hour\n",
    "    hourly_avg = train_data.groupby('hour')['pv_measurement'].mean()\n",
    "\n",
    "    # Plotting the average PV production for each hour\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    hourly_avg.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Average PV Production by Hour')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Average PV Production')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_dist_hour(y_train, hour):\n",
    "    train_data = y_train.copy()\n",
    "    train_data['hour'] = y_train['time'].dt.hour\n",
    "    \n",
    "    # Filtering the data for the given hour\n",
    "    hour_data = train_data[train_data['hour'] == hour]\n",
    "    \n",
    "    # Plotting the distribution of PV measurements for 1 am\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(hour_data['pv_measurement'], bins=50, color='teal', alpha=0.7)\n",
    "    plt.title(f'Distribution of PV Measurements at {hour}')\n",
    "    plt.xlabel('PV Measurement')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(hour_data['pv_measurement'].value_counts())\n",
    "#train_c[(train_c['time'].dt.hour == 2) &(train_c['pv_measurement'] == 9.8)]\n",
    "\n",
    "def get_nighttime_stats(y_train,night_start,night_end):\n",
    "    train_data = y_train.copy()\n",
    "    train_data['hour'] = y_train['time'].dt.hour\n",
    "\n",
    "    # Filtering the data for nighttime hours (8 pm to 4 am)\n",
    "    nighttime_data = train_data[(train_data['hour'] >= night_start) | (train_data['hour'] <= night_end)]\n",
    "\n",
    "    # Descriptive statistics for nighttime PV measurements\n",
    "    nighttime_stats = nighttime_data['pv_measurement'].describe()\n",
    "\n",
    "    # Plotting the distribution of nighttime PV measurements\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(nighttime_data['pv_measurement'], bins=50, color='purple', alpha=0.7)\n",
    "    plt.axvline(nighttime_stats['75%'], color='red', linestyle='dashed', label='75th Percentile')\n",
    "    plt.axvline(nighttime_stats['max'], color='green', linestyle='dashed', label='Max Value')\n",
    "    plt.title('Distribution of Nighttime PV Measurements')\n",
    "    plt.xlabel('PV Measurement')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(nighttime_stats)\n",
    "    \n",
    "def set_nighttime_to_zero(y_train, night_start,night_end, thresh):\n",
    "    df = y_train.copy()\n",
    "    df['hour'] = y_train['time'].dt.hour\n",
    "    mask = (df['hour'] >= 23) | (df['hour'] <= 3) & (df['pv_measurement'] > thresh)\n",
    "    df.loc[mask, 'pv_measurement'] = 0\n",
    "    df = df.drop(columns = ['hour'])\n",
    "    return df\n",
    "\n",
    "#train_a[(train_a['time'].dt.hour == 2) &(train_a['pv_measurement'] >0)]\n",
    "#train_a = set_nighttime_to_zero(train_a,23,3,0)\n",
    "#train_b = set_nighttime_to_zero(train_b,23,3,0)\n",
    "#train_c = set_nighttime_to_zero(train_c,23,3,0)\n",
    "#train_a[(train_a['time'].dt.hour == 2) &(train_a['pv_measurement'] >0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fdf4a-51e1-41af-9e08-bd12e50f903a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove rows with high rad values and zero PV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ec7f2-9c3f-409b-b024-0f075b1ba9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rad_null(merged_df):\n",
    "    merged_data = merged_df.copy()\n",
    "    merged_data['clear_sky_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['clear_sky_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['direct_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['direct_rad_1h:J'].fillna(0, inplace=True)\n",
    "    return merged_data\n",
    "\"\"\"\n",
    "m_a = remove_rad_null(merged_a)\n",
    "m_b = remove_rad_null(merged_b)\n",
    "m_c = remove_rad_null(merged_c)\n",
    "\"\"\"\n",
    "\n",
    "def get_percentiles_df(merged_df):\n",
    "    merged_data = merged_df.copy()\n",
    "    merged_data['clear_sky_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['clear_sky_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['direct_rad:W'].fillna(0, inplace=True)\n",
    "    merged_data['direct_rad_1h:J'].fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate and display percentiles\n",
    "    percentiles = [50,60,70,80,85,90,95]\n",
    "    percentile_values_direct_rad= np.percentile(merged_data['direct_rad:W'], percentiles)\n",
    "    percentile_values_direct_rad_1h = np.percentile(merged_data['direct_rad_1h:J'], percentiles)\n",
    "    percentile_values_clear_sky_rad = np.percentile(merged_data['clear_sky_rad:W'], percentiles)\n",
    "    percentile_values_clear_sky_energy = np.percentile(merged_data['clear_sky_energy_1h:J'], percentiles)\n",
    "    percentile_values_df = pd.DataFrame({\n",
    "        'Percentile': percentiles,\n",
    "        'direct_rad:W':percentile_values_direct_rad,\n",
    "        'direct_rad_1h:J': percentile_values_direct_rad_1h,\n",
    "        'clear_sky_rad:W': percentile_values_clear_sky_rad,\n",
    "        'clear_sky_energy_1h:J': percentile_values_clear_sky_energy\n",
    "        })\n",
    "    \n",
    "    return percentile_values_df\n",
    "\n",
    "def get_anomals(merged_data,feature,percentile): \n",
    "    #identify the rows where the \"direct_rad:W\" column in x_train_a is high\n",
    "    #but the PV measurement in train_a is zero -> Indicates wrong\n",
    "    \n",
    "    percentile_df = get_percentiles_df(merged_data)\n",
    "    \n",
    "    # Define a threshold for high solar radiation\n",
    "    threshold = percentile_df[percentile_df['Percentile']==percentile][feature].values[0],\n",
    "\n",
    "    # Find rows where 'direct_rad:W' is high but PV measurement is zero\n",
    "    anomalous_rows = merged_data[(merged_data[feature] > threshold) & (merged_data['pv_measurement'] == 0)]\n",
    "    \n",
    "    \n",
    "    # Display the anomalous rows\n",
    "    return anomalous_rows\n",
    "\"\"\"\n",
    "merged_a1 = merged_a.copy().drop(get_anomals(merged_a,'clear_sky_rad:W',90).index)\n",
    "merged_b1 = merged_b.copy().drop(get_anomals(merged_b,'direct_rad:W',90).index)\n",
    "merged_c1 = merged_c.copy().drop(get_anomals(merged_c,'direct_rad_1h:J',90).index)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5415d0f-67fe-4978-9c6e-58b11f04c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_data, estimated_column='estimated',val_size = 0.1,val = False):\n",
    "    \"\"\"\n",
    "    Splits the dataset into a training set and a validation set.\n",
    "    The validation set contains the last half of the rows where observed = 0,\n",
    "    and the training set contains the rest.\n",
    "\n",
    "    :param train_data: The original training dataset as a pandas DataFrame.\n",
    "    :param observed_column: The name of the column that indicates if the row is observed.\n",
    "    :return: A tuple (training_set, validation_set)\n",
    "    \"\"\"\n",
    "    \n",
    "    if val: \n",
    "        estimated_one = train_data[train_data[estimated_column] == 1]\n",
    "\n",
    "        #Split the filtered dataset into two\n",
    "        half_index = len(estimated_one) // 2\n",
    "        validation_set = estimated_one[half_index:]\n",
    "\n",
    "        # Combine the first half of observed_zero with the rest of the data where observed != 0\n",
    "        training_set = pd.concat([train_data[train_data[estimated_column] == 0], estimated_one[:half_index]])\n",
    "    else:\n",
    "        split_index = int(train_data.shape[0] * (1-val_size))\n",
    "\n",
    "        # Split the data\n",
    "        training_set = train_data.iloc[:split_index]\n",
    "        validation_set = train_data.iloc[split_index:]\n",
    "\n",
    "    # Filter rows where observed = 0\n",
    "   \n",
    "    return training_set, validation_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7d537-27fa-4d76-b9f5-87a487493050",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Add direct_rad + sun_elevation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef5b1dd-5ce5-4af4-8628-820e7bb1bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did not improve kaggle score\n",
    "def add_rad_x_sun(merged_data):\n",
    "    df = merged_data.copy()\n",
    "    df['rad_x_sun_elevation'] = df['direct_rad:W']*df['sun_elevation:d']\n",
    "    return df\n",
    "\"\"\"\n",
    "mod_a = add_rad_x_sun(merged_a)\n",
    "mod_b = add_rad_x_sun(merged_b)\n",
    "mod_c = add_rad_x_sun(merged_c)\n",
    "\n",
    "x_test_a_mod = add_rad_x_sun(x_test_a)\n",
    "x_test_b_mod = add_rad_x_sun(x_test_b)\n",
    "x_test_c_mod = add_rad_x_sun(x_test_c)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f54bf-c4c8-4177-8596-09a6821b99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_plot('rad_x_sun_elevation',df.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd791c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Catboost model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a775a1b-ddaf-4ed2-988e-53856f4c9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_data, val_size=0.1, val = False, estimated_column = 'estimated'):\n",
    "    if val: \n",
    "        estimated_one = train_data[train_data[estimated_column] == 1]\n",
    "\n",
    "        #Split the filtered dataset into two\n",
    "        half_index = len(estimated_one) // 2\n",
    "        validation_set = estimated_one[half_index:]\n",
    "\n",
    "        # Combine the first half of observed_zero with the rest of the data where observed != 0\n",
    "        training_set = pd.concat([train_data[train_data[estimated_column] == 0], estimated_one[:half_index]])\n",
    "    else:\n",
    "        split_index = int(train_data.shape[0] * (1 - val_size))\n",
    "        training_set = train_data.iloc[:split_index]\n",
    "        validation_set = train_data.iloc[split_index:]\n",
    "    return training_set, validation_set\n",
    "\n",
    "def build_catboost(merged_df, val_size=0.1, randomized=False):\n",
    "    merged_df = merged_df.drop(columns=['date_forecast','time'])\n",
    "    if randomized:\n",
    "        X = merged_df.drop(columns=['pv_measurement'])\n",
    "        y = merged_df['pv_measurement']\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "    else:\n",
    "        training_set, validation_set = split_dataset(merged_df, val_size, True)\n",
    "        X_train = training_set.drop(columns=['pv_measurement'])\n",
    "        y_train = training_set['pv_measurement']\n",
    "        X_validation = validation_set.drop(columns=['pv_measurement'])\n",
    "        y_validation = validation_set['pv_measurement']\n",
    "    \n",
    "    catboost_model = CatBoostRegressor(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=42,\n",
    "        verbose=200\n",
    "    )\n",
    "    \n",
    "    catboost_model.fit(X_train, y_train, eval_set=(X_validation, y_validation), use_best_model=True)\n",
    "    return catboost_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8599a46-e0fd-44c5-80b7-269315fa8eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 600.2686128\ttest: 598.9900368\tbest: 598.9900368 (0)\ttotal: 69.6ms\tremaining: 1m 9s\n",
      "200:\tlearn: 186.8224590\ttest: 184.8368845\tbest: 184.8368845 (200)\ttotal: 2.05s\tremaining: 8.15s\n",
      "400:\tlearn: 172.5852193\ttest: 177.0184811\tbest: 177.0184811 (400)\ttotal: 4.02s\tremaining: 6s\n",
      "600:\tlearn: 165.4562878\ttest: 173.9216187\tbest: 173.9216187 (600)\ttotal: 5.93s\tremaining: 3.94s\n",
      "800:\tlearn: 159.9711581\ttest: 171.9586921\tbest: 171.9478331 (782)\ttotal: 7.8s\tremaining: 1.94s\n",
      "999:\tlearn: 155.5396263\ttest: 171.0395115\tbest: 170.9733018 (967)\ttotal: 9.66s\tremaining: 0us\n",
      "\n",
      "bestTest = 170.9733018\n",
      "bestIteration = 967\n",
      "\n",
      "Shrink model to first 968 iterations.\n",
      "0:\tlearn: 98.2556381\ttest: 97.5971320\tbest: 97.5971320 (0)\ttotal: 12ms\tremaining: 12s\n",
      "200:\tlearn: 24.6167625\ttest: 27.1157637\tbest: 27.1157637 (200)\ttotal: 1.79s\tremaining: 7.13s\n",
      "400:\tlearn: 22.8155224\ttest: 26.3407276\tbest: 26.3407276 (400)\ttotal: 3.56s\tremaining: 5.31s\n",
      "600:\tlearn: 21.7527308\ttest: 25.9275926\tbest: 25.9268793 (598)\ttotal: 5.26s\tremaining: 3.5s\n",
      "800:\tlearn: 20.8929991\ttest: 25.7105246\tbest: 25.7105246 (800)\ttotal: 6.92s\tremaining: 1.72s\n",
      "999:\tlearn: 20.3249002\ttest: 25.5346900\tbest: 25.5343290 (997)\ttotal: 8.56s\tremaining: 0us\n",
      "\n",
      "bestTest = 25.53432904\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "0:\tlearn: 89.8955069\ttest: 89.5916507\tbest: 89.5916507 (0)\ttotal: 11.4ms\tremaining: 11.4s\n",
      "200:\tlearn: 22.4650308\ttest: 24.3733605\tbest: 24.3721150 (198)\ttotal: 1.62s\tremaining: 6.43s\n",
      "400:\tlearn: 20.7305466\ttest: 23.5838245\tbest: 23.5800958 (384)\ttotal: 3.58s\tremaining: 5.34s\n",
      "600:\tlearn: 19.0977490\ttest: 22.9743806\tbest: 22.9733555 (598)\ttotal: 5.11s\tremaining: 3.4s\n",
      "800:\tlearn: 18.1173813\ttest: 22.6765839\tbest: 22.6761843 (797)\ttotal: 6.63s\tremaining: 1.65s\n",
      "999:\tlearn: 17.5669484\ttest: 22.5127912\tbest: 22.5024092 (978)\ttotal: 8.15s\tremaining: 0us\n",
      "\n",
      "bestTest = 22.50240917\n",
      "bestIteration = 978\n",
      "\n",
      "Shrink model to first 979 iterations.\n"
     ]
    }
   ],
   "source": [
    "model_a = build_catboost(merged_a,0.2, True)\n",
    "model_b = build_catboost(merged_b,0.2, True)\n",
    "model_c = build_catboost(merged_c,0.125, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e318564e-878f-41b2-93a1-1c14ca270856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 586.9978423\ttest: 585.7427113\tbest: 585.7427113 (0)\ttotal: 17.5ms\tremaining: 17.5s\n",
      "200:\tlearn: 135.4348033\ttest: 132.5729848\tbest: 132.5729848 (200)\ttotal: 2.11s\tremaining: 8.38s\n",
      "400:\tlearn: 126.7251863\ttest: 127.9034601\tbest: 127.8959837 (394)\ttotal: 4.14s\tremaining: 6.18s\n",
      "600:\tlearn: 123.6682405\ttest: 126.8611030\tbest: 126.8611030 (600)\ttotal: 6.11s\tremaining: 4.05s\n",
      "800:\tlearn: 120.8061786\ttest: 126.4408790\tbest: 126.3593272 (795)\ttotal: 8.04s\tremaining: 2s\n",
      "999:\tlearn: 117.7813444\ttest: 126.1648452\tbest: 126.1565724 (997)\ttotal: 9.94s\tremaining: 0us\n",
      "\n",
      "bestTest = 126.1565724\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "0:\tlearn: 100.5853318\ttest: 99.9754460\tbest: 99.9754460 (0)\ttotal: 11.4ms\tremaining: 11.4s\n",
      "200:\tlearn: 18.7861492\ttest: 19.8754087\tbest: 19.8754087 (200)\ttotal: 1.85s\tremaining: 7.36s\n",
      "400:\tlearn: 18.2069378\ttest: 19.7060787\tbest: 19.7011318 (379)\ttotal: 3.64s\tremaining: 5.43s\n",
      "600:\tlearn: 17.4356649\ttest: 19.5957723\tbest: 19.5852728 (572)\ttotal: 5.37s\tremaining: 3.56s\n",
      "800:\tlearn: 16.8933570\ttest: 19.5216625\tbest: 19.5201356 (790)\ttotal: 7.05s\tremaining: 1.75s\n",
      "999:\tlearn: 16.3739126\ttest: 19.5794075\tbest: 19.5198737 (808)\ttotal: 8.74s\tremaining: 0us\n",
      "\n",
      "bestTest = 19.5198737\n",
      "bestIteration = 808\n",
      "\n",
      "Shrink model to first 809 iterations.\n",
      "0:\tlearn: 90.6649935\ttest: 90.3875478\tbest: 90.3875478 (0)\ttotal: 10ms\tremaining: 10s\n",
      "200:\tlearn: 16.2513233\ttest: 18.1155483\tbest: 18.1155483 (200)\ttotal: 1.7s\tremaining: 6.77s\n",
      "400:\tlearn: 15.4448796\ttest: 17.9325593\tbest: 17.9325593 (400)\ttotal: 3.36s\tremaining: 5.02s\n",
      "600:\tlearn: 14.8836589\ttest: 17.8928679\tbest: 17.8775200 (568)\ttotal: 5.03s\tremaining: 3.34s\n",
      "800:\tlearn: 14.3659030\ttest: 17.8760139\tbest: 17.8503600 (776)\ttotal: 6.86s\tremaining: 1.7s\n",
      "999:\tlearn: 13.9149748\ttest: 17.9185004\tbest: 17.8503600 (776)\ttotal: 8.79s\tremaining: 0us\n",
      "\n",
      "bestTest = 17.85035996\n",
      "bestIteration = 776\n",
      "\n",
      "Shrink model to first 777 iterations.\n"
     ]
    }
   ],
   "source": [
    "laged_model_a = build_catboost(laged_a,0.2, True)\n",
    "laged_model_b = build_catboost(laged_b,0.2, True)\n",
    "laged_model_c = build_catboost(laged_c,0.125, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67684c52-a72c-492b-9428-ac0a64049be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict and Submit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e422c76-2808-4e58-87ca-8c9dfe8b8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_with_lag(model, test_data, initial_lag_value, lag_hours=1, column_name='pv_measurement'):\n",
    "    \"\"\"\n",
    "    Predict using a model that requires a lag feature, updating the test set iteratively.\n",
    "\n",
    "    Parameters:\n",
    "    model (model object): The trained model used for prediction.\n",
    "    test_data (pd.DataFrame): The test dataset without the target column.\n",
    "    initial_lag_value (float): The last known value of the target variable from the training set.\n",
    "    lag_hours (int): The number of hours to lag.\n",
    "    column_name (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A series of predictions for the test dataset.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    lag_feature_name = f\"{column_name}_lag_{lag_hours}h\"\n",
    "    current_lag_value = initial_lag_value\n",
    "    \n",
    "    for index, row in test_data.iterrows():\n",
    "        # Set the current lag value\n",
    "        row[lag_feature_name] = current_lag_value\n",
    "        \n",
    "        # Make a prediction\n",
    "        prediction = model.predict(row.to_frame().transpose())[0]\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        # Update the lag value with the current prediction\n",
    "        current_lag_value = prediction\n",
    "    \n",
    "    return pd.Series(predictions, index=test_data.index)\n",
    "\n",
    "initial_lag_val_a = merged_a['pv_measurement'].tail(1).values[0]\n",
    "initial_lag_val_b = merged_b['pv_measurement'].tail(1).values[0]\n",
    "initial_lag_val_c = merged_c['pv_measurement'].tail(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0561422-66f6-4d01-8564-cd6986f4febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Then, use the function to make predictions:\n",
    "laged_pred_a = predict_with_lag(model=laged_model_a, test_data=x_test_a_laged, \n",
    "                               initial_lag_value=initial_lag_val_a, lag_hours=1)\n",
    "laged_pred_b = predict_with_lag(model=laged_model_b, test_data=x_test_b_laged, \n",
    "                               initial_lag_value=initial_lag_val_a, lag_hours=1)\n",
    "laged_pred_c = predict_with_lag(model=laged_model_c, test_data=x_test_c_laged, \n",
    "                               initial_lag_value=initial_lag_val_a, lag_hours=1)\n",
    "laged_pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5b63a33-669f-4e43-beb5-ee8a5e5df1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a = model_a.predict(x_test_a)\n",
    "pred_b = model_b.predict(x_test_b)\n",
    "pred_c = model_c.predict(x_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98989635-e755-4cb3-808e-fcaffbce7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub(pred_a,pred_b,pred_c):\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "    submission['prediction'] = np.concatenate([pred_a,pred_b,pred_c])\n",
    "    submission.loc[submission['prediction'] < 0, 'prediction'] = 0\n",
    "    return submission\n",
    "\n",
    "sub1 = create_sub(pred_a,pred_b,pred_c)\n",
    "sub_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456bf31-9668-4eb2-98a9-7097c3679b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f'Submissions/rad_solarfeatCatboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24076d9-b911-498d-87ea-23c55b6691e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name,location):\n",
    "    save_directory = 'Saved_models/'+ location.upper()\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    # Define the path to save the model\n",
    "    model_file_path = os.path.join(save_directory, f'{model_name}.cbm')\n",
    "\n",
    "    # Save the model\n",
    "    model.save_model(model_file_path)\n",
    "\n",
    "    print(f\"Model successfully saved at {model_file_path}\")\n",
    "    \n",
    "save_model(model_a,'base_catBoost','A')\n",
    "save_model(model_b,'base_catBoost','B')\n",
    "save_model(model_c,'base_catBoost','C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754cbdf-96c1-464c-8234-6b2e4ff37ab0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea45b7-a8fc-4c20-b887-b3ad124e0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_importance(model):\n",
    "    feats = {'feature':merged_a.drop(columns =['date_forecast','time','pv_measurement']).columns,\n",
    "         'importance':model.get_feature_importance()}\n",
    "    df = pd.DataFrame(feats).sort_values('importance',ascending = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231c4d3-d6ce-4ad3-8b7c-a7b1985c10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feat_importance(model_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf40fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_preds(pred1,pred2):\n",
    "    pred1 = preds_a_original.as_data_frame()\n",
    "    pred2 = preds_a2.as_data_frame()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.scatter(y_pred1['predict'], y_pred2['predict'], alpha=0.5)\n",
    "\n",
    "    # Line of equality (for reference)\n",
    "    plt.plot([y_pred1['predict'].min(), y_pred1['predict'].max()],\n",
    "             [y_pred2['predict'].min(), y_pred2['predict'].max()],\n",
    "             color='red', linestyle='--')\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('Predictions from First Model')\n",
    "    plt.ylabel('Predictions from New model')\n",
    "    plt.title('Comparison of Predictions from Two Models')\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(preds):\n",
    "    test = pd.read_csv('test.csv')\n",
    "    predictions= preds['predict'].as_data_frame()\n",
    "    predictions['time'] = test['time'].unique()\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Prediction', color='tab:blue')\n",
    "    ax1.plot(predictions['time'], predictions['predict'], color='tab:blue', label='Solar Power Production')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Time Series Plot of prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd01df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"\"\"# Plot the distribution of \"direct_rad:W\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(merged_data['direct_rad:W'], bins=50, kde=True)\n",
    "    plt.title('Distribution of \"direct_rad:W\"')\n",
    "    plt.xlabel('Direct Radiation (W)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(merged_data['clear_sky_rad:W'], bins=50, kde=True)\n",
    "    plt.title('Distribution of \"clear_sky_rad:W\"')\n",
    "    plt.xlabel('Direct Radiation (W)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(merged_data['direct_rad_1h:J'], bins=50, kde=True)\n",
    "    plt.title('Distribution of \"direct_rad_1h:J\"')\n",
    "    plt.xlabel('Radiation 1h(J)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(merged_data['clear_sky_energy_1h:J'], bins=50, kde=True)\n",
    "    plt.title('Distribution of \"clear_sky_energy_1h:J\"')\n",
    "    plt.xlabel('Radiation 1h(J)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
