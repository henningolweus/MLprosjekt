{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Exclude boolean columns (location_A, location_B, location_C)\n",
    "# columns_to_scale = x_train.columns.difference(['location_A', 'location_B', 'location_C'])\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# x_train[columns_to_scale] = scaler.fit_transform(x_train[columns_to_scale])\n",
    "# x_test[columns_to_scale] = scaler.transform(x_test[columns_to_scale])\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('cleaned_and_combined_data/x_train_combined.csv')\n",
    "y_train = pd.read_csv('cleaned_and_combined_data/y_train_combined.csv')\n",
    "x_test = pd.read_csv('cleaned_and_combined_data/x_test_combined.csv')\n",
    "\n",
    "# Determine the split index\n",
    "split_index = int(0.875 * len(x_train))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train_df = x_train[:split_index]\n",
    "y_train_df = y_train[:split_index]\n",
    "x_valid_df = x_train[split_index:]\n",
    "y_valid_df = y_train[split_index:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Exclude boolean columns (location_A, location_B, location_C)\n",
    "# columns_to_scale = x_train.columns.difference(['location_A', 'location_B', 'location_C'])\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# x_train[columns_to_scale] = scaler.fit_transform(x_train[columns_to_scale])\n",
    "# x_test[columns_to_scale] = scaler.transform(x_test[columns_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train_df,label = y_train_df['pv_measurement'])\n",
    "dval = xgb.DMatrix(x_valid_df,label = y_valid_df['pv_measurement'])\n",
    "test = xgb.DMatrix(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    'booster': 'gbtree',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'alpha': 0.1,\n",
    "    'lambda': 1,\n",
    "\n",
    "}\n",
    "#  Train the model\n",
    "num_boost_round = 1000  # Maximum number of boosting rounds\n",
    "early_stopping_rounds = 50  # Stop if validation score doesn't improve for 50 rounds\n",
    "\n",
    "evals = [(dtrain, 'train'), (dval, 'eval')]\n",
    "progress = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:452.58473\teval-mae:378.20850\n",
      "[1]\ttrain-mae:431.71483\teval-mae:361.54254\n",
      "[2]\ttrain-mae:412.10626\teval-mae:345.76168\n",
      "[3]\ttrain-mae:393.72026\teval-mae:330.92285\n",
      "[4]\ttrain-mae:376.27797\teval-mae:316.90233\n",
      "[5]\ttrain-mae:359.88827\teval-mae:303.74271\n",
      "[6]\ttrain-mae:344.43929\teval-mae:291.23351\n",
      "[7]\ttrain-mae:330.03668\teval-mae:279.54061\n",
      "[8]\ttrain-mae:316.31920\teval-mae:268.41997\n",
      "[9]\ttrain-mae:303.54260\teval-mae:258.03149\n",
      "[10]\ttrain-mae:291.54325\teval-mae:248.16702\n",
      "[11]\ttrain-mae:280.21693\teval-mae:239.05823\n",
      "[12]\ttrain-mae:269.66630\teval-mae:230.08814\n",
      "[13]\ttrain-mae:259.68628\teval-mae:221.61960\n",
      "[14]\ttrain-mae:250.30667\teval-mae:213.73687\n",
      "[15]\ttrain-mae:241.43505\teval-mae:206.17293\n",
      "[16]\ttrain-mae:233.18643\teval-mae:199.05066\n",
      "[17]\ttrain-mae:225.39908\teval-mae:192.29870\n",
      "[18]\ttrain-mae:218.17008\teval-mae:185.98613\n",
      "[19]\ttrain-mae:211.29225\teval-mae:179.81635\n",
      "[20]\ttrain-mae:204.82265\teval-mae:174.02157\n",
      "[21]\ttrain-mae:198.77002\teval-mae:168.68400\n",
      "[22]\ttrain-mae:193.05248\teval-mae:163.58407\n",
      "[23]\ttrain-mae:187.61972\teval-mae:158.59145\n",
      "[24]\ttrain-mae:182.61357\teval-mae:153.89060\n",
      "[25]\ttrain-mae:177.80701\teval-mae:149.39923\n",
      "[26]\ttrain-mae:173.32840\teval-mae:145.19995\n",
      "[27]\ttrain-mae:169.03056\teval-mae:141.18708\n",
      "[28]\ttrain-mae:164.98827\teval-mae:137.35816\n",
      "[29]\ttrain-mae:161.20692\teval-mae:133.86369\n",
      "[30]\ttrain-mae:157.73988\teval-mae:130.66459\n",
      "[31]\ttrain-mae:154.32327\teval-mae:127.42369\n",
      "[32]\ttrain-mae:151.13505\teval-mae:124.34081\n",
      "[33]\ttrain-mae:148.08013\teval-mae:121.45665\n",
      "[34]\ttrain-mae:145.19408\teval-mae:118.72095\n",
      "[35]\ttrain-mae:142.40997\teval-mae:115.90687\n",
      "[36]\ttrain-mae:139.94079\teval-mae:113.44688\n",
      "[37]\ttrain-mae:137.52838\teval-mae:111.18583\n",
      "[38]\ttrain-mae:135.20848\teval-mae:108.94486\n",
      "[39]\ttrain-mae:133.00869\teval-mae:106.78807\n",
      "[40]\ttrain-mae:130.92139\teval-mae:104.80367\n",
      "[41]\ttrain-mae:128.91141\teval-mae:102.69690\n",
      "[42]\ttrain-mae:127.03146\teval-mae:100.85832\n",
      "[43]\ttrain-mae:125.24211\teval-mae:99.24943\n",
      "[44]\ttrain-mae:123.55686\teval-mae:97.56699\n",
      "[45]\ttrain-mae:121.98353\teval-mae:95.96807\n",
      "[46]\ttrain-mae:120.44564\teval-mae:94.39693\n",
      "[47]\ttrain-mae:119.01941\teval-mae:93.13684\n",
      "[48]\ttrain-mae:117.68241\teval-mae:91.84355\n",
      "[49]\ttrain-mae:116.38671\teval-mae:90.59940\n",
      "[50]\ttrain-mae:115.15748\teval-mae:89.40962\n",
      "[51]\ttrain-mae:113.98675\teval-mae:88.22430\n",
      "[52]\ttrain-mae:112.84675\teval-mae:87.10810\n",
      "[53]\ttrain-mae:111.80041\teval-mae:86.15250\n",
      "[54]\ttrain-mae:110.75755\teval-mae:85.05114\n",
      "[55]\ttrain-mae:109.78946\teval-mae:84.01069\n",
      "[56]\ttrain-mae:108.86636\teval-mae:82.99220\n",
      "[57]\ttrain-mae:108.00565\teval-mae:82.19129\n",
      "[58]\ttrain-mae:107.15981\teval-mae:81.40758\n",
      "[59]\ttrain-mae:106.37351\teval-mae:80.62626\n",
      "[60]\ttrain-mae:105.65953\teval-mae:79.93324\n",
      "[61]\ttrain-mae:104.94398\teval-mae:79.09991\n",
      "[62]\ttrain-mae:104.20024\teval-mae:78.40261\n",
      "[63]\ttrain-mae:103.53339\teval-mae:77.80025\n",
      "[64]\ttrain-mae:102.83072\teval-mae:77.20580\n",
      "[65]\ttrain-mae:102.26578\teval-mae:76.75089\n",
      "[66]\ttrain-mae:101.71064\teval-mae:76.13066\n",
      "[67]\ttrain-mae:101.17438\teval-mae:75.58124\n",
      "[68]\ttrain-mae:100.68807\teval-mae:75.16158\n",
      "[69]\ttrain-mae:100.22055\teval-mae:74.66515\n",
      "[70]\ttrain-mae:99.76054\teval-mae:74.14691\n",
      "[71]\ttrain-mae:99.30218\teval-mae:73.69321\n",
      "[72]\ttrain-mae:98.87489\teval-mae:73.32319\n",
      "[73]\ttrain-mae:98.42401\teval-mae:72.92706\n",
      "[74]\ttrain-mae:97.98901\teval-mae:72.58231\n",
      "[75]\ttrain-mae:97.60688\teval-mae:71.75892\n",
      "[76]\ttrain-mae:97.24107\teval-mae:71.49175\n",
      "[77]\ttrain-mae:96.86064\teval-mae:71.25280\n",
      "[78]\ttrain-mae:96.52085\teval-mae:70.96833\n",
      "[79]\ttrain-mae:96.24671\teval-mae:70.87542\n",
      "[80]\ttrain-mae:95.88902\teval-mae:70.42979\n",
      "[81]\ttrain-mae:95.59469\teval-mae:70.14904\n",
      "[82]\ttrain-mae:95.26652\teval-mae:69.85324\n",
      "[83]\ttrain-mae:95.07442\teval-mae:69.78271\n",
      "[84]\ttrain-mae:94.77725\teval-mae:69.47591\n",
      "[85]\ttrain-mae:94.50949\teval-mae:69.32888\n",
      "[86]\ttrain-mae:94.25976\teval-mae:69.25196\n",
      "[87]\ttrain-mae:94.03520\teval-mae:68.94340\n",
      "[88]\ttrain-mae:93.76590\teval-mae:68.69601\n",
      "[89]\ttrain-mae:93.55904\teval-mae:68.54623\n",
      "[90]\ttrain-mae:93.32426\teval-mae:68.38604\n",
      "[91]\ttrain-mae:93.12616\teval-mae:68.23697\n",
      "[92]\ttrain-mae:92.89588\teval-mae:68.18433\n",
      "[93]\ttrain-mae:92.77273\teval-mae:68.21935\n",
      "[94]\ttrain-mae:92.57950\teval-mae:68.12048\n",
      "[95]\ttrain-mae:92.45219\teval-mae:68.08145\n",
      "[96]\ttrain-mae:92.30170\teval-mae:68.03738\n",
      "[97]\ttrain-mae:92.04074\teval-mae:67.80939\n",
      "[98]\ttrain-mae:91.89370\teval-mae:67.66783\n",
      "[99]\ttrain-mae:91.75646\teval-mae:67.50764\n",
      "[100]\ttrain-mae:91.57389\teval-mae:67.46248\n",
      "[101]\ttrain-mae:91.45644\teval-mae:67.44008\n",
      "[102]\ttrain-mae:91.33684\teval-mae:67.44334\n",
      "[103]\ttrain-mae:91.09713\teval-mae:67.55476\n",
      "[104]\ttrain-mae:90.90280\teval-mae:67.41229\n",
      "[105]\ttrain-mae:90.75061\teval-mae:67.31747\n",
      "[106]\ttrain-mae:90.60853\teval-mae:67.31385\n",
      "[107]\ttrain-mae:90.47438\teval-mae:67.29502\n",
      "[108]\ttrain-mae:90.34433\teval-mae:67.26709\n",
      "[109]\ttrain-mae:90.23745\teval-mae:67.19005\n",
      "[110]\ttrain-mae:90.11862\teval-mae:67.48175\n",
      "[111]\ttrain-mae:89.93844\teval-mae:67.34804\n",
      "[112]\ttrain-mae:89.83442\teval-mae:67.24934\n",
      "[113]\ttrain-mae:89.73801\teval-mae:67.24623\n",
      "[114]\ttrain-mae:89.54131\teval-mae:67.10880\n",
      "[115]\ttrain-mae:89.41072\teval-mae:67.19007\n",
      "[116]\ttrain-mae:89.33851\teval-mae:67.43825\n",
      "[117]\ttrain-mae:89.19369\teval-mae:67.24696\n",
      "[118]\ttrain-mae:89.03645\teval-mae:67.54386\n",
      "[119]\ttrain-mae:88.93916\teval-mae:67.52570\n",
      "[120]\ttrain-mae:88.88917\teval-mae:67.51155\n",
      "[121]\ttrain-mae:88.75225\teval-mae:67.45801\n",
      "[122]\ttrain-mae:88.68577\teval-mae:67.36164\n",
      "[123]\ttrain-mae:88.56822\teval-mae:67.29358\n",
      "[124]\ttrain-mae:88.45623\teval-mae:67.19124\n",
      "[125]\ttrain-mae:88.36424\teval-mae:67.15210\n",
      "[126]\ttrain-mae:88.27755\teval-mae:67.46901\n",
      "[127]\ttrain-mae:88.14265\teval-mae:67.39230\n",
      "[128]\ttrain-mae:88.04680\teval-mae:67.32234\n",
      "[129]\ttrain-mae:87.97845\teval-mae:67.40779\n",
      "[130]\ttrain-mae:87.88248\teval-mae:67.41365\n",
      "[131]\ttrain-mae:87.76394\teval-mae:67.33887\n",
      "[132]\ttrain-mae:87.67823\teval-mae:67.31812\n",
      "[133]\ttrain-mae:87.57782\teval-mae:67.40576\n",
      "[134]\ttrain-mae:87.50870\teval-mae:67.46285\n",
      "[135]\ttrain-mae:87.44692\teval-mae:67.43002\n",
      "[136]\ttrain-mae:87.38708\teval-mae:67.43489\n",
      "[137]\ttrain-mae:87.29684\teval-mae:67.32097\n",
      "[138]\ttrain-mae:87.18144\teval-mae:67.39214\n",
      "[139]\ttrain-mae:87.06843\teval-mae:67.19652\n",
      "[140]\ttrain-mae:86.98891\teval-mae:67.23138\n",
      "[141]\ttrain-mae:86.89287\teval-mae:67.29732\n",
      "[142]\ttrain-mae:86.80018\teval-mae:67.25371\n",
      "[143]\ttrain-mae:86.69935\teval-mae:67.18413\n",
      "[144]\ttrain-mae:86.60270\teval-mae:67.52489\n",
      "[145]\ttrain-mae:86.51554\teval-mae:67.54225\n",
      "[146]\ttrain-mae:86.48725\teval-mae:67.53593\n",
      "[147]\ttrain-mae:86.38409\teval-mae:67.54287\n",
      "[148]\ttrain-mae:86.31206\teval-mae:67.40626\n",
      "[149]\ttrain-mae:86.26083\teval-mae:67.41676\n",
      "[150]\ttrain-mae:86.21482\teval-mae:67.43341\n",
      "[151]\ttrain-mae:86.11923\teval-mae:67.38140\n",
      "[152]\ttrain-mae:86.07521\teval-mae:67.32072\n",
      "[153]\ttrain-mae:86.01098\teval-mae:67.30243\n",
      "[154]\ttrain-mae:85.97152\teval-mae:67.31120\n",
      "[155]\ttrain-mae:85.88302\teval-mae:67.25848\n",
      "[156]\ttrain-mae:85.79151\teval-mae:67.22255\n",
      "[157]\ttrain-mae:85.71022\teval-mae:67.16867\n",
      "[158]\ttrain-mae:85.66371\teval-mae:67.19342\n",
      "[159]\ttrain-mae:85.61764\teval-mae:67.15285\n",
      "[160]\ttrain-mae:85.53694\teval-mae:67.31178\n",
      "[161]\ttrain-mae:85.47295\teval-mae:67.32567\n",
      "[162]\ttrain-mae:85.39174\teval-mae:67.31673\n",
      "[163]\ttrain-mae:85.30152\teval-mae:67.30336\n",
      "Best MAE: 67.11 with 115 rounds\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    evals_result=progress\n",
    ")\n",
    "\n",
    "print(\"Best MAE: {:.2f} with {} rounds\".format(\n",
    "         bst.best_score,\n",
    "         bst.best_iteration+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.7156568,  2.7156568,  2.6367888, ..., -2.3124921, -3.5591257,\n",
       "       -4.442778 ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = bst.predict(test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Convert the numpy array to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['prediction'])\n",
    "\n",
    "# Join the 'id' column from sample_submission with the predictions\n",
    "sample_submission['prediction'] = predictions_df['prediction']\n",
    "\n",
    "# Save to CSV\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
