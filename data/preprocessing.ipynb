{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow\n",
    "# !pip install fastparquet\n",
    "# !pip install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method of preprocessing\n",
    "1. Import the datasets\n",
    "2. Transform the datasets into hourly format\n",
    "3. Handle missing values and rows\n",
    "4. Final feature engineering and storing\n",
    "5. Create additional combined super-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_estimated_a = pd.read_parquet('A/x_train_estimated.parquet')\n",
    "x_train_estimated_b = pd.read_parquet('B/x_train_estimated.parquet')\n",
    "x_train_estimated_c = pd.read_parquet('C/x_train_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_observed_a = pd.read_parquet('A/x_train_observed.parquet')\n",
    "x_train_observed_b = pd.read_parquet('B/x_train_observed.parquet')\n",
    "x_train_observed_c = pd.read_parquet('C/x_train_observed.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_estimated_a = pd.read_parquet('A/x_test_estimated.parquet')\n",
    "x_test_estimated_b = pd.read_parquet('B/x_test_estimated.parquet')\n",
    "x_test_estimated_c = pd.read_parquet('C/x_test_estimated.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_merged_a = pd.concat([x_train_observed_a,x_train_estimated_a])\n",
    "x_train_merged_b = pd.concat([x_train_observed_b,x_train_estimated_b])\n",
    "x_train_merged_c = pd.concat([x_train_observed_c,x_train_estimated_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transform into hourly\n",
    "\n",
    "- Observed and estimated measurements are taken every 15 minutes, while energy is measured every hour\n",
    "- We need to transform the measurements into hourly aggregations to match the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating from 15-minute intervals to hourly intervals based on different aggregation methods\n",
    "def resample_to_hourly(df, aggregation_methods):\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df_hourly = df.resample('H').agg(aggregation_methods)\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "    \n",
    "    return df_hourly\n",
    "\n",
    "# Aggregation methods based on features' names\n",
    "aggregation_methods = {\n",
    "    'date_calc' : 'max',\n",
    "    'absolute_humidity_2m:gm3': 'mean',\n",
    "    'air_density_2m:kgm3': 'mean',\n",
    "    'ceiling_height_agl:m': 'mean',\n",
    "    'clear_sky_energy_1h:J': 'sum',\n",
    "    'clear_sky_rad:W': 'mean',\n",
    "    'cloud_base_agl:m': 'mean',\n",
    "    'dew_or_rime:idx': 'max',\n",
    "    'dew_point_2m:K': 'mean',\n",
    "    'diffuse_rad:W': 'mean',\n",
    "    'diffuse_rad_1h:J': 'sum',\n",
    "    'direct_rad:W': 'mean',\n",
    "    'direct_rad_1h:J': 'sum',\n",
    "    'effective_cloud_cover:p': 'mean',\n",
    "    'elevation:m': 'mean',\n",
    "    'fresh_snow_12h:cm': 'sum',\n",
    "    'fresh_snow_1h:cm': 'sum',\n",
    "    'fresh_snow_24h:cm': 'sum',\n",
    "    'fresh_snow_3h:cm': 'sum',\n",
    "    'fresh_snow_6h:cm': 'sum',\n",
    "    'is_day:idx': 'max',\n",
    "    'is_in_shadow:idx': 'max',\n",
    "    'msl_pressure:hPa': 'mean',\n",
    "    'precip_5min:mm': 'sum',\n",
    "    'precip_type_5min:idx': 'max',\n",
    "    'pressure_100m:hPa': 'mean',\n",
    "    'pressure_50m:hPa': 'mean',\n",
    "    'prob_rime:p': 'mean',\n",
    "    'rain_water:kgm2': 'sum',\n",
    "    'relative_humidity_1000hPa:p': 'mean',\n",
    "    'sfc_pressure:hPa': 'mean',\n",
    "    'snow_density:kgm3': 'mean',\n",
    "    'snow_depth:cm': 'mean',\n",
    "    'snow_drift:idx': 'max',\n",
    "    'snow_melt_10min:mm': 'sum',\n",
    "    'snow_water:kgm2': 'sum',\n",
    "    'sun_azimuth:d': 'mean',\n",
    "    'sun_elevation:d': 'mean',\n",
    "    'super_cooled_liquid_water:kgm2': 'mean',\n",
    "    't_1000hPa:K': 'mean',\n",
    "    'total_cloud_cover:p': 'mean',\n",
    "    'visibility:m': 'mean',\n",
    "    'wind_speed_10m:ms': 'mean',\n",
    "    'wind_speed_u_10m:ms': 'mean',\n",
    "    'wind_speed_v_10m:ms': 'mean',\n",
    "    'wind_speed_w_1000hPa:ms': 'mean'\n",
    "}\n",
    "\n",
    "# Apply the function to each dataset\n",
    "x_train_a_hourly = resample_to_hourly(x_train_merged_a, aggregation_methods)\n",
    "x_train_b_hourly = resample_to_hourly(x_train_merged_b, aggregation_methods)\n",
    "x_train_c_hourly = resample_to_hourly(x_train_merged_c, aggregation_methods)\n",
    "\n",
    "x_test_a_hourly = resample_to_hourly(x_test_estimated_a, aggregation_methods)\n",
    "x_test_b_hourly = resample_to_hourly(x_test_estimated_b, aggregation_methods)\n",
    "x_test_c_hourly = resample_to_hourly(x_test_estimated_c, aggregation_methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Handle missing values and rows\n",
    "- Remove NaN pv measurement values from y\n",
    "- Remove rows that are not present in both x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indices of the rows with NaN values in the 'pv_measurement' column\n",
    "nan_indices_a = train_a[train_a['pv_measurement'].isna()].index\n",
    "nan_indices_b = train_b[train_b['pv_measurement'].isna()].index\n",
    "nan_indices_c = train_c[train_c['pv_measurement'].isna()].index\n",
    "\n",
    "# Drop these indices from y_train\n",
    "train_a = train_a.drop(nan_indices_a)\n",
    "train_b = train_b.drop(nan_indices_b)\n",
    "train_c = train_c.drop(nan_indices_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with date-time values that are not present in both x and y in order to synchronize x and its labels. \n",
    "def remove_non_synchronous_rows(x_train, y_train, x_date_column='date_forecast', y_date_column='time'):\n",
    "    # Convert date columns to datetime format for easier comparison\n",
    "    x_train[x_date_column] = pd.to_datetime(x_train[x_date_column])\n",
    "    y_train[y_date_column] = pd.to_datetime(y_train[y_date_column])\n",
    "    \n",
    "    # Find common dates\n",
    "    common_dates = x_train[x_date_column][x_train[x_date_column].isin(y_train[y_date_column])]\n",
    "    \n",
    "    # Filter both datasets based on common dates\n",
    "    x_train_synced = x_train[x_train[x_date_column].isin(common_dates)]\n",
    "    y_train_synced = y_train[y_train[y_date_column].isin(common_dates)]\n",
    "    \n",
    "    return x_train_synced, y_train_synced\n",
    "\n",
    "# Remove the rows with date and time that only shows up in one of the sets\n",
    "x_train_a_hourly, train_a = remove_non_synchronous_rows(x_train_a_hourly, train_a)\n",
    "x_train_b_hourly, train_b = remove_non_synchronous_rows(x_train_b_hourly, train_b)\n",
    "x_train_c_hourly, train_c = remove_non_synchronous_rows(x_train_c_hourly, train_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final feature engineering and storing\n",
    "- Extract year, month, day and hour features from each datetime column\n",
    "- Store the cleaned data for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts year, month, day, and hour features from a given datetime column\n",
    "def extract_date_features(df, date_column, prefix):\n",
    "    # Convert to datetime\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "    # Extract features\n",
    "    df[f'{prefix}_year'] = df[date_column].dt.year\n",
    "    df[f'{prefix}_month'] = df[date_column].dt.month\n",
    "    df[f'{prefix}_day'] = df[date_column].dt.day\n",
    "    df[f'{prefix}_hour'] = df[date_column].dt.hour\n",
    "    \n",
    "    # Drop the original date column\n",
    "    df.drop(columns=[date_column], inplace=True)\n",
    "\n",
    "# List of datasets\n",
    "datasets = [x_train_a_hourly, x_train_b_hourly, x_train_c_hourly, x_test_a_hourly, x_test_b_hourly, x_test_c_hourly, train_a, train_b, train_c]\n",
    "\n",
    "# Loop through datasets and extract date-time features for both date_forecast, date_calc and 'time'\n",
    "for dataset in datasets:\n",
    "    if 'date_forecast' in dataset.columns:\n",
    "        extract_date_features(dataset, 'date_forecast', 'forecast')\n",
    "    if 'date_calc' in dataset.columns:\n",
    "        extract_date_features(dataset, 'date_calc', 'calc')\n",
    "    if 'time' in dataset.columns:\n",
    "        extract_date_features(dataset, 'time', 'time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cleaned datasets\n",
    "output_dir = 'cleaned_data'\n",
    "# Ensure directory exists, if not create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Directories for each location\n",
    "dir_a = os.path.join(output_dir, 'A')\n",
    "dir_b = os.path.join(output_dir, 'B')\n",
    "dir_c = os.path.join(output_dir, 'C')\n",
    "\n",
    "# Ensure subdirectories exist\n",
    "for dir_path in [dir_a, dir_b, dir_c]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "# Saving datasets for location A\n",
    "x_train_a_hourly.to_csv(os.path.join(dir_a, 'x_train_a.csv'), index=False)\n",
    "x_test_a_hourly.to_csv(os.path.join(dir_a, 'x_test_a.csv'), index=False)\n",
    "train_a.to_csv(os.path.join(dir_a, 'train_a.csv'), index=False)\n",
    "\n",
    "# Saving datasets for location B\n",
    "x_train_b_hourly.to_csv(os.path.join(dir_b, 'x_train_b.csv'), index=False)\n",
    "x_test_b_hourly.to_csv(os.path.join(dir_b, 'x_test_b.csv'), index=False)\n",
    "train_b.to_csv(os.path.join(dir_b, 'train_b.csv'), index=False)\n",
    "\n",
    "# Saving datasets for location C\n",
    "x_train_c_hourly.to_csv(os.path.join(dir_c, 'x_train_c.csv'), index=False)\n",
    "x_test_c_hourly.to_csv(os.path.join(dir_c, 'x_test_c.csv'), index=False)\n",
    "train_c.to_csv(os.path.join(dir_c, 'train_c.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create super-sets for x_train, y_train and x_test\n",
    "- For experimentational purposes we create three supersets, eachcontaining encoded rows telling if their from A, B and C.\n",
    "- This is used to see if training a single model is better than training three separate models\n",
    "- Sorted by forecast_date for x_train, time_date for y_train, and location and forecast_date for x_test (in order to match the submission format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the location for the dataset, encodes it into three columns and removes the original location column\n",
    "def add_and_encode_location(dfs, locations):\n",
    "    for df, loc in zip(dfs, locations):\n",
    "        df['location'] = loc\n",
    "        for unique_loc in locations:\n",
    "            df[f'location_{unique_loc}'] = (df['location'] == unique_loc).astype(int)\n",
    "        df.drop('location', axis=1, inplace=True)\n",
    "\n",
    "datasets = [x_train_a_hourly, x_train_b_hourly, x_train_c_hourly, x_test_a_hourly, x_test_b_hourly, x_test_c_hourly, train_a, train_b, train_c]\n",
    "locations = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "\n",
    "add_and_encode_location(datasets, locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all x_train, x_test and y_train datasets creating a superset containing data for all three locations \n",
    "x_train_combined = pd.concat([x_train_a_hourly, x_train_b_hourly, x_train_c_hourly], ignore_index=True)\n",
    "x_test_combined = pd.concat([x_test_a_hourly, x_test_b_hourly, x_test_c_hourly], ignore_index=True)\n",
    "y_train_combined = pd.concat([train_a, train_b, train_c], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting x_train\n",
    "x_train = x_train_combined.sort_values(by=['forecast_year', 'forecast_month', 'forecast_day', 'forecast_hour','location_A', 'location_B', 'location_C'])\n",
    "# Sorting y_train\n",
    "y_train = y_train_combined.sort_values(by=[ 'time_year', 'time_month', 'time_day', 'time_hour','location_A', 'location_B', 'location_C'])\n",
    "# Sorting x_test to match the sorting method used in test.csv\n",
    "x_test = x_test_combined.sort_values(by=['location_A', 'location_B', 'location_C', 'forecast_year', 'forecast_month', 'forecast_day', 'forecast_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data as csv-files in a folder called cleaned_and_combined_data\n",
    "output_dir = 'cleaned_and_combined_data'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the dataframes into the folder\n",
    "x_train.to_csv(os.path.join(output_dir, 'x_train_combined.csv'), index=False)\n",
    "x_test_combined.to_csv(os.path.join(output_dir, 'x_test_combined.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(output_dir, 'y_train_combined.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
